{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San-Franscisco Crime Predition Challenge - Kaggle\n",
    "### Team Member : Shanti Greene, Jing Xu, Abhishek Kumar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description\n",
    "\n",
    "This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. \n",
    "\n",
    "##### train.csv / test.csv\n",
    "\n",
    "Data fields \n",
    "\n",
    " - Dates - timestamp of the crime incident \n",
    " - Category - category of the crime incident (only in train.csv). This is the target variable you are going to predict. \n",
    " - Descript - detailed description of the crime incident (only in train.csv) \n",
    " - DayOfWeek - the day of the week \n",
    " - PdDistrict - name of the Police Department District \n",
    " - Resolution - how the crime incident was resolved (only in train.csv) \n",
    " - Address - the approximate street address of the crime incident  \n",
    " - X - Longitude \n",
    " - Y - Latitude\n",
    " \n",
    "##### Submission data ( sampleSubmission.csv)\n",
    "\n",
    "You must submit a csv file with the incident id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:\n",
    "\n",
    "\n",
    "##### evaluation criteria\n",
    "\n",
    "Submissions are evaluated using the multi-class logarithmic loss. Each incident has been labeled with one true class. For each incident, you must submit a set of predicted probabilities (one for every class). The formula is then,\n",
    "\n",
    "logloss=−1/N∑i=1 to N ∑ j=1 to M yijlog(pij),\n",
    "\n",
    "where N is the number of images in the test set, M is the number of class labels, log is the natural logarithm, yij is 1 if observation i is in class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j.\n",
    "\n",
    "The submitted probabilities for a given incident are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with max(min(p,1−10−15),10−15).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import gzip\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#from nolearn.dbn import DBN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read train and test data files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#submission_df = pd.read_csv('sampleSubmission.csv')\n",
    "#street_df = pd.read_csv('Street_Names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>PARK</td>\n",
       "      <td>NONE</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dates        Category                      Descript  \\\n",
       "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
       "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "\n",
       "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
       "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
       "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
       "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.425892  37.774599  \n",
       "1 -122.425892  37.774599  \n",
       "2 -122.424363  37.800414  \n",
       "3 -122.426995  37.800873  \n",
       "4 -122.438738  37.771541  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show head of train_df\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "train_df['loc'] = train_df['X'].map(lambda x: str(round(x,2))) +   train_df['Y'].map(lambda y: ' {0}'.format(str(round(y,2))))\n",
    "test_df['loc'] = test_df['X'].map(lambda x: str(round(x,2))) +   test_df['Y'].map(lambda y: ' {0}'.format(str(round(y,2))))\n",
    "unique_cordinates =train_df['loc'].unique().tolist()\n",
    "# check how to handle one extra unique cordinates\n",
    "#unique_cordinates = list(set(train_df['loc'].unique().tolist() + test_df['loc'].unique().tolist()))\n",
    "\n",
    "print len(unique_cordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 10 columns):\n",
      "Dates         878049 non-null object\n",
      "Category      878049 non-null object\n",
      "Descript      878049 non-null object\n",
      "DayOfWeek     878049 non-null object\n",
      "PdDistrict    878049 non-null object\n",
      "Resolution    878049 non-null object\n",
      "Address       878049 non-null object\n",
      "X             878049 non-null float64\n",
      "Y             878049 non-null float64\n",
      "loc           878049 non-null object\n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 73.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Dates</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-05-10 23:59:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>2000 Block of THOMAS AV</td>\n",
       "      <td>-122.399588</td>\n",
       "      <td>37.735051</td>\n",
       "      <td>-122.4 37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-10 23:51:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>3RD ST / REVERE AV</td>\n",
       "      <td>-122.391523</td>\n",
       "      <td>37.732432</td>\n",
       "      <td>-122.39 37.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-05-10 23:50:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2000 Block of GOUGH ST</td>\n",
       "      <td>-122.426002</td>\n",
       "      <td>37.792212</td>\n",
       "      <td>-122.43 37.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "      <td>-122.44 37.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "      <td>-122.44 37.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                Dates DayOfWeek PdDistrict                   Address  \\\n",
       "0   0  2015-05-10 23:59:00    Sunday    BAYVIEW   2000 Block of THOMAS AV   \n",
       "1   1  2015-05-10 23:51:00    Sunday    BAYVIEW        3RD ST / REVERE AV   \n",
       "2   2  2015-05-10 23:50:00    Sunday   NORTHERN    2000 Block of GOUGH ST   \n",
       "3   3  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "4   4  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "\n",
       "            X          Y            loc  \n",
       "0 -122.399588  37.735051   -122.4 37.74  \n",
       "1 -122.391523  37.732432  -122.39 37.73  \n",
       "2 -122.426002  37.792212  -122.43 37.79  \n",
       "3 -122.437394  37.721412  -122.44 37.72  \n",
       "4 -122.437394  37.721412  -122.44 37.72  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show head of test_df\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 8 columns):\n",
      "Id            884262 non-null int64\n",
      "Dates         884262 non-null object\n",
      "DayOfWeek     884262 non-null object\n",
      "PdDistrict    884262 non-null object\n",
      "Address       884262 non-null object\n",
      "X             884262 non-null float64\n",
      "Y             884262 non-null float64\n",
      "loc           884262 non-null object\n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 60.7+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show info for submission_df\n",
    "#print submission_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WARRANTS' 'OTHER OFFENSES' 'LARCENY/THEFT' 'VEHICLE THEFT' 'VANDALISM'\n",
      " 'NON-CRIMINAL' 'ROBBERY' 'ASSAULT' 'WEAPON LAWS' 'BURGLARY'\n",
      " 'SUSPICIOUS OCC' 'DRUNKENNESS' 'FORGERY/COUNTERFEITING' 'DRUG/NARCOTIC'\n",
      " 'STOLEN PROPERTY' 'SECONDARY CODES' 'TRESPASS' 'MISSING PERSON' 'FRAUD'\n",
      " 'KIDNAPPING' 'RUNAWAY' 'DRIVING UNDER THE INFLUENCE'\n",
      " 'SEX OFFENSES FORCIBLE' 'PROSTITUTION' 'DISORDERLY CONDUCT' 'ARSON'\n",
      " 'FAMILY OFFENSES' 'LIQUOR LAWS' 'BRIBERY' 'EMBEZZLEMENT' 'SUICIDE'\n",
      " 'LOITERING' 'SEX OFFENSES NON FORCIBLE' 'EXTORTION' 'GAMBLING'\n",
      " 'BAD CHECKS' 'TREA' 'RECOVERED VEHICLE' 'PORNOGRAPHY/OBSCENE MAT']\n",
      "['PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_INGLESIDE', 'PdDistrict_BAYVIEW', 'PdDistrict_RICHMOND', 'PdDistrict_CENTRAL', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'PdDistrict_MISSION', 'PdDistrict_SOUTHERN']\n",
      "['Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12']\n",
      "['year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015']\n",
      "['season_winter', 'season_spring', 'season_summer']\n",
      "['cord_-122.43 37.77', 'cord_-122.42 37.8', 'cord_-122.43 37.8', 'cord_-122.44 37.77', 'cord_-122.4 37.71', 'cord_-122.42 37.73', 'cord_-122.37 37.73', 'cord_-122.51 37.78', 'cord_-122.42 37.81', 'cord_-122.49 37.74', 'cord_-122.41 37.78', 'cord_-122.43 37.78', 'cord_-122.4 37.73', 'cord_-122.38 37.74', 'cord_-122.42 37.74', 'cord_-122.39 37.74', 'cord_-122.45 37.74', 'cord_-122.42 37.75', 'cord_-122.44 37.8', 'cord_-122.41 37.79', 'cord_-122.43 37.79', 'cord_-122.48 37.74', 'cord_-122.5 37.75', 'cord_-122.44 37.76', 'cord_-122.39 37.78', 'cord_-122.47 37.72', 'cord_-122.43 37.71', 'cord_-122.42 37.77', 'cord_-122.42 37.79', 'cord_-122.47 37.77', 'cord_-122.45 37.77', 'cord_-122.4 37.75', 'cord_-122.4 37.79', 'cord_-122.4 37.78', 'cord_-122.41 37.77', 'cord_-122.47 37.73', 'cord_-122.42 37.78', 'cord_-122.4 37.72', 'cord_-122.41 37.76', 'cord_-122.4 37.8', 'cord_-122.41 37.75', 'cord_-122.44 37.78', 'cord_-122.47 37.74', 'cord_-122.39 37.76', 'cord_-122.45 37.78', 'cord_-122.41 37.8', 'cord_-122.38 37.73', 'cord_-122.42 37.76', 'cord_-122.41 37.74', 'cord_-122.5 37.78', 'cord_-122.43 37.76', 'cord_-122.44 37.72', 'cord_-122.43 37.72', 'cord_-122.4 37.74', 'cord_-122.45 37.76', 'cord_-122.51 37.77', 'cord_-122.4 37.76', 'cord_-122.39 37.73', 'cord_-122.46 37.73', 'cord_-122.46 37.72', 'cord_-122.45 37.8', 'cord_-122.39 37.77', 'cord_-122.45 37.73', 'cord_-122.41 37.81', 'cord_-122.48 37.75', 'cord_-122.44 37.73', 'cord_-122.49 37.78', 'cord_-122.48 37.76', 'cord_-122.48 37.77', 'cord_-122.4 37.77', 'cord_-122.47 37.79', 'cord_-122.42 37.71', 'cord_-122.44 37.74', 'cord_-122.49 37.75', 'cord_-122.48 37.78', 'cord_-122.41 37.73', 'cord_-122.45 37.71', 'cord_-122.47 37.76', 'cord_-122.39 37.79', 'cord_-122.46 37.78', 'cord_-122.46 37.79', 'cord_-122.43 37.73', 'cord_-122.45 37.72', 'cord_-122.51 37.75', 'cord_-122.47 37.78', 'cord_-122.46 37.71', 'cord_-122.44 37.79', 'cord_-122.44 37.71', 'cord_-122.47 37.71', 'cord_-122.5 37.74', 'cord_-122.39 37.75', 'cord_-122.44 37.75', 'cord_-122.45 37.75', 'cord_-122.48 37.73', 'cord_-122.43 37.74', 'cord_-122.43 37.75', 'cord_-122.51 37.74', 'cord_-122.49 37.71', 'cord_-122.46 37.77', 'cord_-122.49 37.73', 'cord_-122.45 37.79', 'cord_-122.5 37.72', 'cord_-122.48 37.72', 'cord_-122.4 37.81', 'cord_-122.46 37.74', 'cord_-122.49 37.76', 'cord_-122.41 37.71', 'cord_-122.46 37.76', 'cord_-122.42 37.72', 'cord_-122.5 37.77', 'cord_-122.39 37.72', 'cord_-122.46 37.75', 'cord_-122.47 37.75', 'cord_-122.43 37.81', 'cord_-122.41 37.72', 'cord_-122.37 37.81', 'cord_-122.5 37.73', 'cord_-122.39 37.8', 'cord_-122.5 37.76', 'cord_-122.49 37.79', 'cord_-122.38 37.72', 'cord_-122.49 37.77', 'cord_-122.39 37.71', 'cord_-122.38 37.76', 'cord_-122.51 37.76', 'cord_-122.49 37.72', 'cord_-122.48 37.79', 'cord_-122.44 37.81', 'cord_-122.5 37.79', 'cord_-122.38 37.75', 'cord_-122.5 37.71', 'cord_-122.37 37.82', 'cord_-122.48 37.71', 'cord_-122.45 37.81', 'cord_-122.51 37.73', 'cord_-122.38 37.71', 'cord_-122.47 37.81', 'cord_-120.5 90.0', 'cord_-122.36 37.81']\n",
      "['ARSON' 'ASSAULT' 'BAD CHECKS' 'BRIBERY' 'BURGLARY' 'DISORDERLY CONDUCT'\n",
      " 'DRIVING UNDER THE INFLUENCE' 'DRUG/NARCOTIC' 'DRUNKENNESS' 'EMBEZZLEMENT'\n",
      " 'EXTORTION' 'FAMILY OFFENSES' 'FORGERY/COUNTERFEITING' 'FRAUD' 'GAMBLING'\n",
      " 'KIDNAPPING' 'LARCENY/THEFT' 'LIQUOR LAWS' 'LOITERING' 'MISSING PERSON'\n",
      " 'NON-CRIMINAL' 'OTHER OFFENSES' 'PORNOGRAPHY/OBSCENE MAT' 'PROSTITUTION'\n",
      " 'RECOVERED VEHICLE' 'ROBBERY' 'RUNAWAY' 'SECONDARY CODES'\n",
      " 'SEX OFFENSES FORCIBLE' 'SEX OFFENSES NON FORCIBLE' 'STOLEN PROPERTY'\n",
      " 'SUICIDE' 'SUSPICIOUS OCC' 'TREA' 'TRESPASS' 'VANDALISM' 'VEHICLE THEFT'\n",
      " 'WARRANTS' 'WEAPON LAWS']\n",
      "['address_0', 'address_1', 'address_2', 'address_3', 'address_4', 'address_5', 'address_6', 'address_7', 'address_8', 'address_9', 'address_10', 'address_11', 'address_12', 'address_13', 'address_14', 'address_15', 'address_16', 'address_17', 'address_18', 'address_19', 'address_20', 'address_21', 'address_22', 'address_23', 'address_24', 'address_25', 'address_26', 'address_27', 'address_28', 'address_29', 'address_30', 'address_31', 'address_32', 'address_33', 'address_34', 'address_35', 'address_36', 'address_37', 'address_38', 'address_39', 'address_40', 'address_41', 'address_42', 'address_43', 'address_44', 'address_45', 'address_46', 'address_47', 'address_48', 'address_49', 'address_50', 'address_51', 'address_52', 'address_53', 'address_54', 'address_55', 'address_56', 'address_57', 'address_58', 'address_59', 'address_60', 'address_61', 'address_62', 'address_63', 'address_64', 'address_65', 'address_66', 'address_67', 'address_68', 'address_69', 'address_70', 'address_71', 'address_72', 'address_73', 'address_74', 'address_75', 'address_76', 'address_77', 'address_78', 'address_79', 'address_80', 'address_81', 'address_82', 'address_83', 'address_84', 'address_85', 'address_86', 'address_87', 'address_88', 'address_89', 'address_90', 'address_91', 'address_92', 'address_93', 'address_94', 'address_95', 'address_96', 'address_97', 'address_98', 'address_99']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = train_df['Category'].unique()\n",
    "print unique_categories\n",
    "\n",
    "# creating features for district and dayofWeek\n",
    "# for discrict\n",
    "Unique_PdDistricts = train_df['PdDistrict'].unique()\n",
    "PdDistrict_features = ['PdDistrict_{0}'.format(district) for district in Unique_PdDistricts]\n",
    "print PdDistrict_features\n",
    "# for days of week\n",
    "Unique_DayOfWeek = train_df['DayOfWeek'].unique()\n",
    "DayOfWeek_features = ['DayOfWeek_{0}'.format(day) for day in Unique_DayOfWeek]\n",
    "# for month\n",
    "month_features = ['Month_{0}'.format(month) for month in range(1,13)]\n",
    "print month_features\n",
    "# for year\n",
    "year_features = ['year_{0}'.format(year) for year in range(2003,2016)]\n",
    "print year_features\n",
    "# for season \n",
    "season_features = ['season_winter','season_spring','season_summer']\n",
    "print season_features\n",
    "#for cordinates\n",
    "cordinate_features = ['cord_{0}'.format(cord) for cord in unique_cordinates]\n",
    "print cordinate_features\n",
    "\n",
    "# for category\n",
    "category_encoder = LabelEncoder()\n",
    "category_encoder.fit(unique_categories) \n",
    "print category_encoder.classes_\n",
    "\n",
    "num_address_features = 100\n",
    "address_vectorizer = HashingVectorizer(decode_error='ignore', n_features=num_address_features,non_negative=True)\n",
    "address_features = ['address_{0}'.format(i) for i in range(num_address_features)]\n",
    "print address_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130\n"
     ]
    }
   ],
   "source": [
    "# Processing Address Feature\n",
    "def removePunctuation(text):\n",
    "    '''\n",
    "    function to remove punctuations\n",
    "    '''\n",
    "    # create a regex for punctuations\n",
    "    punct = re.compile(r'([^A-Za-z0-9 ])')\n",
    "    # replace the punctuation with empty space\n",
    "    return punct.sub(\" \", text)\n",
    "\n",
    "# removing punctuation from address values\n",
    "train_df['Address'] = train_df['Address'].map(lambda x : removePunctuation(x))\n",
    "test_df['Address'] = test_df['Address'].map(lambda x : removePunctuation(x))\n",
    "\n",
    "\n",
    "# count vectorizer for address\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_address_features = vectorizer.fit_transform(train_df['Address'].values)\n",
    "test_adddress_features = vectorizer.transform(test_df['Address'].values)\n",
    "print len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProcessData(df, datatype):\n",
    "    # set the correct type for dates column\n",
    "    df['Dates'] = df['Dates'].astype('datetime64[ns]')   \n",
    "    # adding Features\n",
    "    df = addFeatures(df)\n",
    "       \n",
    "    # drop columns not needed now\n",
    "    if datatype == 'train':\n",
    "        df = df.drop(['Descript','Resolution','Address', 'Dates','PdDistrict','DayOfWeek'], axis=1)\n",
    "    if datatype == 'test':\n",
    "        df = df.drop(['Address','Dates','PdDistrict','DayOfWeek'], axis=1)\n",
    "    return df\n",
    "    \n",
    "def addFeatures(df):   \n",
    "    #df  = processPdDiscrictCrimePropertion(df)   # not working well.\n",
    "    df = processPdDiscrict(df)  \n",
    "    df = processDayOfWeek(df)\n",
    "    df = processCordinates(df)\n",
    "    #df = processAddress(df) # not working well. use some some other technique to extract features\n",
    "    df = addHourOfCrime(df)\n",
    "    df = addMonthOfCrime(df)\n",
    "    df = addYearOfCrime(df)\n",
    "    df = addSeasonOfCrime(df)\n",
    "    return df\n",
    "    \n",
    "def processPdDiscrictCrimePropertion(df):    \n",
    "    df  = pd.merge(df, df_prop_crime_per_pdDisrict, on='PdDistrict',how='left') \n",
    "    return df\n",
    "\n",
    "\n",
    "def processPdDiscrict(df):   \n",
    "    new_PdDistrict_df = pd.get_dummies(df['PdDistrict'], prefix='PdDistrict')   \n",
    "    new_PdDistrict_df = new_PdDistrict_df[PdDistrict_features]\n",
    "    df  = pd.concat([df, new_PdDistrict_df], axis=1)   \n",
    "    return df\n",
    "\n",
    "\n",
    "def processCordinates(df):   \n",
    "    new_cord_df = pd.get_dummies(df['loc'], prefix='cord')   \n",
    "    new_cord_df = new_cord_df[cordinate_features]\n",
    "    df  = pd.concat([df, new_cord_df], axis=1)   \n",
    "    return df\n",
    "\n",
    "def processDayOfWeek(df):   \n",
    "    new_DayOfWeek_df = pd.get_dummies(df['DayOfWeek'], prefix='DayOfWeek')   \n",
    "    new_DayOfWeek_df = new_DayOfWeek_df[DayOfWeek_features]\n",
    "    df  = pd.concat([df, new_DayOfWeek_df], axis=1)    \n",
    "    return df\n",
    "    \n",
    "def addHourOfCrime(df):      \n",
    "    df['HourOfCrime'] = df['Dates'].map(lambda d: d.hour + d.minute / 60.)  \n",
    "    return df\n",
    "\n",
    "def addMonthOfCrime(df):      \n",
    "    #df['MonthOfCrime'] = df['Dates'].map(lambda d: d.month)  \n",
    "    new_month_df = pd.get_dummies(df['Dates'].map(lambda d: d.month) , prefix='Month')   \n",
    "    new_month_df = new_month_df[month_features]\n",
    "    df  = pd.concat([df, new_month_df], axis=1)    \n",
    "    return df\n",
    "\n",
    "def addYearOfCrime(df):          \n",
    "    new_year_df = pd.get_dummies(df['Dates'].map(lambda d: d.year) , prefix='year')   \n",
    "    new_year_df = new_year_df[year_features]\n",
    "    df  = pd.concat([df, new_year_df], axis=1)    \n",
    "    return df\n",
    "\n",
    "# March - June \"Spring\", July - October \"Summer\", November - February \"Winter\"\n",
    "def addSeasonOfCrime(df):        \n",
    "    new_month_df = pd.get_dummies(df['Dates'].map(lambda d: GetSeason(d.month)) , prefix='season')   \n",
    "    new_month_df = new_month_df[season_features]   \n",
    "    df  = pd.concat([df, new_month_df], axis=1)    \n",
    "    return df\n",
    "def GetSeason(month):\n",
    "    if month == 3 or month == 4 or month == 5 or month == 6:\n",
    "        return 'spring'\n",
    "    if month == 7 or month == 8 or month == 9 or month == 10:\n",
    "        return 'summer'\n",
    "    if month == 11 or month == 12 or month == 1 or month == 2:\n",
    "        return 'winter'\n",
    "\n",
    "def processAddress(df):   \n",
    "    address_hashed = address_vectorizer.fit_transform(df['Address'])\n",
    "    new_address_df = pd.SparseDataFrame([ pd.SparseSeries(address_hashed[i].toarray().ravel()) \n",
    "                                   for i in np.arange(address_hashed.shape[0]) ])\n",
    "    new_address_df.columns = address_features\n",
    "    df  = pd.concat([df, new_address_df], axis=1)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_train_df = ProcessData(train_df, 'train')\n",
    "final_train_df['Category'] = category_encoder.transform(final_train_df['Category'])  \n",
    "final_test_df = ProcessData(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Columns: 189 entries, Category to season_summer\n",
      "dtypes: float64(187), int64(1), object(1)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Columns: 189 entries, Id to season_summer\n",
      "dtypes: float64(187), int64(1), object(1)\n",
      "memory usage: 1.3+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print final_train_df.info()\n",
    "print final_test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Final Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input features\n",
    "to_be_scaled_features = []\n",
    "#to_be_scaled_features +=  list('dist_' + unique_categories)\n",
    "to_be_scaled_features +=  ['HourOfCrime']\n",
    "no_scale_features = []\n",
    "no_scale_features += PdDistrict_features \n",
    "no_scale_features += DayOfWeek_features \n",
    "no_scale_features += month_features \n",
    "no_scale_features += year_features \n",
    "no_scale_features += cordinate_features\n",
    "no_scale_features += season_features\n",
    "inputFeatures = []\n",
    "inputFeatures = to_be_scaled_features + no_scale_features\n",
    "#output features\n",
    "ouptutFeatures = ['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = final_train_df[inputFeatures].values\n",
    "Y = final_train_df[ouptutFeatures].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1556"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del final_train_df\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L, 2315L)\n"
     ]
    }
   ],
   "source": [
    "# append count vectorizer features for address\n",
    "X = np.concatenate((X,  train_address_features.toarray()), axis=1)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  scaling data\n",
    "#scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "#X[:,:len(to_be_scaled_features)] = scaler.fit_transform(X[:,:len(to_be_scaled_features)])\n",
    "#X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del train_address_features\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(790240L, 2315L) (790240L, 1L)\n",
      "(87809L, 2315L) (87809L, 1L)\n",
      "39 39\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(Y, test_size=0.1, random_state=1234)\n",
    "for train_index, dev_index in sss:\n",
    "    break\n",
    "train_data, train_labels = X[train_index], Y[train_index]\n",
    "dev_data, dev_labels = X[dev_index], Y[dev_index]\n",
    "print train_data.shape, train_labels.shape\n",
    "print dev_data.shape, dev_labels.shape\n",
    "print len(np.unique(train_labels)), len(np.unique(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del X\n",
    "del Y\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-1b1ca91bd76e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-1b1ca91bd76e>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for comp in np.arange(30 100, 10:\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "components = []\n",
    "explained_ratios = []\n",
    "for comp in np.arange(30 100, 10):\n",
    "    print 'trying comp size : {0}'.format(comp)\n",
    "    pca = IncrementalPCA(n_components=comp, batch_size=10000)\n",
    "    pca.fit(train_data)\n",
    "    components.append(comp)\n",
    "    explained_ratios.append(np.sum(pca.explained_variance_ratio_))\n",
    "plt.plot(components, explained_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = 100\n",
    "#pca = PCA(n_components=components)\n",
    "pca = IncrementalPCA(n_components=components, batch_size=10000)\n",
    "train_data_pca = pca.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.42627339e-01   1.18047915e-02   8.34601375e-03   7.98179114e-03\n",
      "   5.29984866e-03   3.86414566e-03   3.30574706e-03   3.01521592e-03\n",
      "   2.92441048e-03   2.86589255e-03   2.80843957e-03   2.78513063e-03\n",
      "   2.72858837e-03   2.63179886e-03   2.49305265e-03   2.14708310e-03\n",
      "   1.93103110e-03   1.77059677e-03   1.74034975e-03   1.72590181e-03\n",
      "   1.71983951e-03   1.68585271e-03   1.66742113e-03   1.65798816e-03\n",
      "   1.64849376e-03   1.63561835e-03   1.62488574e-03   1.60702396e-03\n",
      "   1.59886162e-03   1.59247584e-03   1.57405273e-03   1.56450404e-03\n",
      "   1.55625601e-03   1.53846056e-03   1.52698548e-03   1.50560765e-03\n",
      "   1.50343076e-03   1.47823736e-03   1.45087280e-03   1.38703333e-03\n",
      "   1.25429521e-03   1.15032512e-03   1.08640197e-03   9.98748435e-04\n",
      "   9.50303664e-04   8.85554634e-04   8.51514635e-04   8.23859778e-04\n",
      "   8.11904694e-04   7.65114661e-04   7.30254000e-04   6.79943461e-04\n",
      "   6.25586081e-04   5.98456905e-04   5.73737628e-04   5.67204310e-04\n",
      "   5.24750847e-04   4.97112682e-04   4.84600165e-04   4.72890410e-04\n",
      "   4.52353347e-04   4.42893643e-04   4.31890120e-04   4.16953235e-04\n",
      "   4.05510662e-04   3.91747295e-04   3.84035096e-04   3.70337476e-04\n",
      "   3.54951503e-04   3.45968640e-04   3.36720721e-04   3.33739313e-04\n",
      "   3.31406842e-04   3.24238711e-04   3.19578946e-04   3.15393030e-04\n",
      "   3.07817388e-04   3.00671451e-04   2.97491055e-04   2.87502145e-04\n",
      "   2.79308632e-04   2.77729923e-04   2.75014780e-04   2.71420076e-04\n",
      "   2.69835174e-04   2.66465022e-04   2.64094332e-04   2.53259565e-04\n",
      "   2.52088716e-04   2.50294197e-04   2.48407365e-04   2.45640866e-04\n",
      "   2.41109946e-04   2.34043345e-04   2.30086143e-04   2.28439313e-04\n",
      "   2.24066911e-04   2.17825284e-04   2.13826724e-04   2.06172574e-04]\n",
      "total variance explained : 0.9737539588\n"
     ]
    }
   ],
   "source": [
    "print pca.explained_variance_ratio_\n",
    "print 'total variance explained : {0}'.format(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_data_pca = pca.transform(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(790240L, 2315L) (790240L, 100L)\n",
      "(87809L, 2315L) (87809L, 100L)\n"
     ]
    }
   ],
   "source": [
    "print train_data.shape, train_data_pca.shape\n",
    "print dev_data.shape, dev_data_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of  50 | elapsed:   27.4s remaining: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  44 | elapsed:    0.1s remaining:    8.6s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc  LogLoss 2.37112734818\n"
     ]
    }
   ],
   "source": [
    "# trying Random Forest Network\n",
    " # model 4 - Random Forest\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=1337, n_jobs=-1, verbose=1)\n",
    "rfc.fit(train_data_pca, train_labels)\n",
    "print 'rfc  LogLoss {0}'.format(log_loss(dev_labels, rfc.predict_proba(dev_data_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DBN] fitting X.shape=(790240L, 60L)\n",
      "[DBN] layers [60L, 300, 39]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DBN] Fine-tune...\n",
      "Epoch 1:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 7645049.8974\n",
      "  err  0.804093352636\n",
      "  (0:00:55)\n",
      "Epoch 2:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.688498898\n",
      "  err  0.802472260468\n",
      "  (0:00:54)\n",
      "Epoch 3:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68930282237\n",
      "  err  0.802965801409\n",
      "  (0:00:55)\n",
      "Epoch 4:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68784684317\n",
      "  err  0.802564641208\n",
      "  (0:00:55)\n",
      "Epoch 5:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68635195572\n",
      "  err  0.801383939418\n",
      "  (0:00:55)\n",
      "Epoch 6:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68664861494\n",
      "  err  0.801957206204\n",
      "  (0:00:55)\n",
      "Epoch 7:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68577188873\n",
      "  err  0.802114126913\n",
      "  (0:00:55)\n",
      "Epoch 8:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68567634986\n",
      "  err  0.801400390783\n",
      "  (0:00:56)\n",
      "Epoch 9:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  loss 2.68414372489\n",
      "  err  0.801675002025\n",
      "  (0:00:57)\n",
      "Epoch 10:\n",
      "  loss 2.68425798099\n",
      "  err  0.800958734915\n",
      "  (0:00:55)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       151\n",
      "          1       0.00      0.00      0.00      7688\n",
      "          2       0.00      0.00      0.00        41\n",
      "          3       0.00      0.00      0.00        29\n",
      "          4       0.00      0.00      0.00      3676\n",
      "          5       0.00      0.00      0.00       432\n",
      "          6       0.00      0.00      0.00       227\n",
      "          7       0.00      0.00      0.00      5397\n",
      "          8       0.00      0.00      0.00       428\n",
      "          9       0.00      0.00      0.00       117\n",
      "         10       0.00      0.00      0.00        26\n",
      "         11       0.00      0.00      0.00        49\n",
      "         12       0.00      0.00      0.00      1061\n",
      "         13       0.00      0.00      0.00      1668\n",
      "         14       0.00      0.00      0.00        15\n",
      "         15       0.00      0.00      0.00       234\n",
      "         16       0.20      1.00      0.33     17490\n",
      "         17       0.00      0.00      0.00       190\n",
      "         18       0.00      0.00      0.00       123\n",
      "         19       0.00      0.00      0.00      2599\n",
      "         20       0.00      0.00      0.00      9230\n",
      "         21       0.00      0.00      0.00     12618\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       0.00      0.00      0.00       748\n",
      "         24       0.00      0.00      0.00       314\n",
      "         25       0.00      0.00      0.00      2300\n",
      "         26       0.00      0.00      0.00       195\n",
      "         27       0.00      0.00      0.00       999\n",
      "         28       0.00      0.00      0.00       439\n",
      "         29       0.00      0.00      0.00        15\n",
      "         30       0.00      0.00      0.00       454\n",
      "         31       0.00      0.00      0.00        51\n",
      "         32       0.00      0.00      0.00      3141\n",
      "         33       0.00      0.00      0.00         1\n",
      "         34       0.00      0.00      0.00       733\n",
      "         35       0.00      0.00      0.00      4473\n",
      "         36       0.00      0.00      0.00      5378\n",
      "         37       0.00      0.00      0.00      4221\n",
      "         38       0.00      0.00      0.00       856\n",
      "\n",
      "avg / total       0.04      0.20      0.07     87809\n",
      "\n",
      "2.68206541735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# trying deep belief networks\n",
    "clf = DBN(\n",
    "    [train_data_pca.shape[1], 300, len(category_encoder.classes_)],\n",
    "    learn_rates=0.3,\n",
    "    learn_rate_decays=0.9,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "clf.fit(train_data_pca, train_labels.ravel())\n",
    "preds = clf.predict(dev_data_pca)\n",
    "print classification_report(dev_labels, preds)\n",
    "print log_loss(dev_labels, clf.predict_proba(dev_data_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trying support vector machine models\n",
    "from sklearn.svm import SVC\n",
    "C = 1.0\n",
    "svc = SVC(kernel='linear', C=C, probability =True).fit(train_data_pca, train_labels.ravel())\n",
    "print 'linear  svc : log loss = {0}'.format(log_loss(dev_labels, svc.predict_proba(dev_data_pca)))\n",
    "\n",
    "rbf_svc = SVC(kernel='rbf', C=C, probability =True).fit(train_data_pca, train_labels.ravel())\n",
    "print 'linear  svc : log loss = {0}'.format(log_loss(dev_labels, rbf_svc.predict_proba(dev_data_pca)))\n",
    "\n",
    "poly_svc = SVC(kernel='poly', C=C, probability =True).fit(train_data_pca, train_labels.ravel())\n",
    "print 'linear  svc : log loss = {0}'.format(log_loss(dev_labels, poly_svc.predict_proba(dev_data_pca)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "predictions = []\n",
    "\n",
    "# batch training of random forest\n",
    "def getRows(rows, data, labels):\n",
    "    return data[rows], labels[rows].ravel()\n",
    "\n",
    "def iter_minibatches(chunksize, data, labels):\n",
    "    numtrainingpoints = len(data)     \n",
    "    chunkstartmarker = 0\n",
    "    while chunkstartmarker < numtrainingpoints:\n",
    "        start = chunkstartmarker\n",
    "        if start + chunksize < numtrainingpoints:\n",
    "            end = chunkstartmarker + chunksize\n",
    "        else:\n",
    "            end = numtrainingpoints\n",
    "        chunkrows = range(start,end)       \n",
    "        X_chunk, y_chunk = getRows(chunkrows, data, labels)        \n",
    "        yield X_chunk, y_chunk\n",
    "        chunkstartmarker += chunksize       \n",
    "        \n",
    "def train_model(train_full_data, train_full_labels, train_pca_data, dev_full_data, dev_full_labels, dev_pca_data):\n",
    "    #model = batch_SGDClassifier(data, labels)\n",
    "    #model = MultinomialNBClassifier(data, labels)\n",
    "    #model = batch_RandomForest(data, labels)\n",
    "    #return model\n",
    "    models, weights = ensemble_training(train_full_data, train_full_labels, train_pca_data, dev_full_data, dev_full_labels, dev_pca_data)\n",
    "    return models, weights\n",
    "\n",
    "def ensemble_training(train_full_data, train_full_labels, train_pca_data, dev_full_data, dev_full_labels, dev_pca_data):\n",
    "    \n",
    "    model_modes = []\n",
    "    # batch mode models - using partial_fit - use all features\n",
    "    # model 1 - batch SGDClassifier\n",
    "    #batch_sgd_1 = batch_SGDClassifier(train_full_data, train_full_labels,dev_full_data, dev_full_labels, 3213)\n",
    "    #print 'model 1 : Batch SGD -1 LogLoss {0}'.format(log_loss(dev_full_labels, batch_sgd_1.predict_proba(dev_full_data)))\n",
    "    #clfs.append(batch_sgd_1)\n",
    "    #model_modes.append('Full')\n",
    "    \n",
    "    # model 2 - batch SGDClassifier\n",
    "    #batch_sgd_2 = batch_SGDClassifier(train_full_data, train_full_labels,dev_full_data, dev_full_labels, 4321)\n",
    "    #print 'model 2 : Batch SGD -2 LogLoss {0}'.format(log_loss(dev_full_labels, batch_sgd_2.predict_proba(dev_full_data)))\n",
    "    #clfs.append(batch_sgd_2)\n",
    "    #model_modes.append('Full')\n",
    "    \n",
    "    # model 3 - Random Forest\n",
    "    rfc_1 = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=4141, n_jobs=-1, verbose=1)\n",
    "    rfc_1.fit(train_pca_data, train_full_labels)\n",
    "    print 'model 3 : RFC -1 LogLoss {0}'.format(log_loss(dev_full_labels, rfc_1.predict_proba(dev_pca_data)))\n",
    "    clfs.append(rfc_1)\n",
    "    model_modes.append('PCA')\n",
    "    \n",
    "    # model 4 - Random Forest\n",
    "    rfc_2 = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=1337, n_jobs=-1, verbose=1)\n",
    "    rfc_2.fit(train_pca_data, train_full_labels)\n",
    "    print 'model 4 : RFC - 2  LogLoss {0}'.format(log_loss(dev_full_labels, rfc_2.predict_proba(dev_pca_data)))\n",
    "    clfs.append(rfc_2) \n",
    "    model_modes.append('PCA')\n",
    "    \n",
    "    # model 3 - Random Forest\n",
    "    #rfc_3 = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=8765, n_jobs=-1)\n",
    "    #rfc_3.fit(train_pca_data, train_full_labels)\n",
    "    #print 'model 3 : RFC -3 LogLoss {0}'.format(log_loss(dev_full_labels, rfc_3.predict_proba(dev_pca_data)))\n",
    "    #clfs.append(rfc_3)\n",
    "    #model_modes.append('PCA')\n",
    "    \n",
    "    # model 4 - Random Forest\n",
    "    #rfc_4 = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=5469, n_jobs=-1, verbose=1)\n",
    "    #rfc_4.fit(train_pca_data, train_full_labels)\n",
    "    #print 'model 4 : RFC - 4  LogLoss {0}'.format(log_loss(dev_full_labels, rfc_4.predict_proba(dev_pca_data)))\n",
    "    #clfs.append(rfc_4) \n",
    "    #model_modes.append('PCA')\n",
    "    \n",
    "    \n",
    "     # model 5 - Gradient Boost\n",
    "    #gbc_1 = GBC(n_estimators=50, max_depth=15, random_state=3421, verbose=1)\n",
    "    #gbc_1.fit(train_pca_data, train_full_labels)\n",
    "    #print 'GBC -1 LogLoss {0}'.format(log_loss(dev_labels, gbc_1.predict_proba(dev_pca_data)))\n",
    "    #clfs.append(gbc_1)\n",
    "    #model_modes.append('PCA')\n",
    "    \n",
    "     # model 6 - Gradient Boost\n",
    "    #gbc_2 = GBC(n_estimators=50, max_depth=15, random_state=7651, verbose=1)\n",
    "    #gbc_2.fit(train_pca_data, train_full_labels)\n",
    "    #print 'GBC -2 LogLoss {0}'.format(log_loss(dev_labels, gbc_2.predict_proba(dev_pca_data)))\n",
    "    #clfs.append(gbc_2)\n",
    "    #model_modes.append('PCA')\n",
    "    \n",
    "    \n",
    "    # model 5 - Logistic Regression\n",
    "    #lr_1 = LogisticRegression(C=100)\n",
    "    #lr_1.fit(train_pca_data, train_full_labels)\n",
    "    #print 'LR -1 LogLoss {0}'.format(log_loss(dev_labels, lr_1.predict_proba(dev_pca_data)))\n",
    "    #clfs.append(lr_1)\n",
    "    #model_modes.append('PCA')\n",
    "    \n",
    "    ### finding the optimum weights    \n",
    "    for index in range(len(clfs)):\n",
    "        clf = clfs[index]\n",
    "        model_mode = model_modes[index]\n",
    "        if model_mode == 'Full':\n",
    "            predictions.append(clf.predict_proba(dev_full_data))\n",
    "        if model_mode == 'PCA':\n",
    "            predictions.append(clf.predict_proba(dev_pca_data))\n",
    "    \n",
    "    #the algorithms need a starting value, right not we chose 0.5 for all weights\n",
    "    #its better to choose many random starting points and run minimize a few times\n",
    "    starting_values = [0.5]*len(predictions)\n",
    "\n",
    "    #adding constraints  and a different solver as suggested by user 16universe\n",
    "    #https://kaggle2.blob.core.windows.net/forum-message-attachments/75655/2393/otto%20model%20weights.pdf?sv=2012-02-12&se=2015-05-03T21%3A22%3A17Z&sr=b&sp=r&sig=rkeA7EJC%2BiQ%2FJ%2BcMpcA4lYQLFh6ubNqs2XAkGtFsAv0%3D\n",
    "    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n",
    "    #our weights are bound between 0 and 1\n",
    "    bounds = [(0,1)]*len(predictions)\n",
    "\n",
    "    res = minimize(log_loss_func, starting_values, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "    print 'Ensamble Score: {0}'.format(res['fun'])\n",
    "    print 'Best Weights: {0}'.format(res['x'])\n",
    "    return clfs, res['x']\n",
    "    \n",
    "def log_loss_func(weights):\n",
    "    ''' scipy minimize will pass the weights as a numpy array '''\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, predictions):\n",
    "            final_prediction += weight*prediction\n",
    "\n",
    "    return log_loss(dev_labels, final_prediction)\n",
    "\n",
    "def batch_RandomForest(data, labels):\n",
    "    model = RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth=5)\n",
    "    model.fit(data, labels)\n",
    "    return model  \n",
    "    \n",
    "def MultinomialNBClassifier(data, labels):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(data,labels)\n",
    "    return model\n",
    "\n",
    "def batch_SGDClassifier(data , labels, test_data, test_labels,random_state):\n",
    "    chunk_size = 1000\n",
    "    batcherator = iter_minibatches(chunk_size, data, labels)\n",
    "    model = SGDClassifier(n_jobs=-1,alpha=0.00005,n_iter = 50, loss='log',random_state=random_state)\n",
    "    # Train model on each chunk\n",
    "    chunk_count = 1\n",
    "    for X_chunk, y_chunk in batcherator:               \n",
    "        model.partial_fit(X_chunk, y_chunk, classes=np.array(range(len(category_encoder.classes_))))\n",
    "        score = model.score(test_data, test_labels)\n",
    "        probs = model.predict_proba(test_data)\n",
    "        log_loss_value = log_loss(test_labels, probs)\n",
    "        print 'training using chunk : {0} validation score : {1:.4f} log-loss : {2:.5f}'.format(chunk_count,score,log_loss_value) \n",
    "        chunk_count += 1\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of  50 | elapsed:   20.3s remaining: 16.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  46 | elapsed:    0.1s remaining:   10.5s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of  50 | elapsed:   19.4s remaining: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=16)]: Done   1 out of  50 | elapsed:    0.1s remaining:    9.8s\n",
      "[Parallel(n_jobs=16)]: Done  50 out of  50 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 3 : RFC -1 LogLoss 2.38681878209\n",
      "model 4 : RFC - 2  LogLoss 2.3871812751\n",
      "      Iter       Train Loss   Remaining Time "
     ]
    }
   ],
   "source": [
    "models, weights = train_model(train_data, train_labels.ravel(), train_data_pca, dev_data, dev_labels.ravel(), dev_data_pca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf = LogisticRegression(C=100.0)\n",
    "#clf = GBC(n_estimators=10, max_depth=5,verbose=1)\n",
    "#clf = MultinomialNB()\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=100)\n",
    "#clf = SGDClassifier(fit_intercept=False, shuffle=True, n_jobs=-1,alpha=0.000005,n_iter = 50, loss='log', penalty ='l2')\n",
    "   \n",
    "#clf.fit(train_data, train_labels)\n",
    "#print clf.predict_proba(dev_data).shape\n",
    "# round up the results\n",
    "#probs = np.around(clf.predict_proba(dev_data), decimals=5)\n",
    "#print 'log loss score : {0:.4f} accuracy : {1:.4f}'.format(log_loss(dev_labels, probs), clf.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 2315L) (884262L, 100L)\n"
     ]
    }
   ],
   "source": [
    "# preparate test data\n",
    "test_data = final_test_df[inputFeatures].values\n",
    "#test_data[:,:len(to_be_scaled_features)] = scaler.transform(test_data[:,:len(to_be_scaled_features)])\n",
    "test_data = np.concatenate((test_data,  test_adddress_features.toarray()), axis=1)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "print test_data.shape, test_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n"
     ]
    }
   ],
   "source": [
    "# compute test output\n",
    "### finding the optimum weights    \n",
    "#final_predictions = []\n",
    "final_prob = np.zeros((test_data_pca.shape[0], len(category_encoder.classes_)))\n",
    "for index in range(len(models)):\n",
    "    model = models[index]\n",
    "    #model_mode = model_modes[index]\n",
    "    #if model_mode == 'Full':\n",
    "    #    pass\n",
    "    #    #final_predictions.append(model.predict_proba(test_data))\n",
    "    #if model_mode == 'PCA':\n",
    "    probs = model.predict_proba(test_data_pca)\n",
    "    probs = probs * weights[index]\n",
    "    final_prob = final_prob + probs\n",
    "print final_prob.shape\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# round up the results\n",
    "submit_output = np.around(final_prob, decimals=10)\n",
    "print submit_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 40L)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 40 columns):\n",
      "Id                             884262 non-null int32\n",
      "ARSON                          884262 non-null float64\n",
      "ASSAULT                        884262 non-null float64\n",
      "BAD CHECKS                     884262 non-null float64\n",
      "BRIBERY                        884262 non-null float64\n",
      "BURGLARY                       884262 non-null float64\n",
      "DISORDERLY CONDUCT             884262 non-null float64\n",
      "DRIVING UNDER THE INFLUENCE    884262 non-null float64\n",
      "DRUG/NARCOTIC                  884262 non-null float64\n",
      "DRUNKENNESS                    884262 non-null float64\n",
      "EMBEZZLEMENT                   884262 non-null float64\n",
      "EXTORTION                      884262 non-null float64\n",
      "FAMILY OFFENSES                884262 non-null float64\n",
      "FORGERY/COUNTERFEITING         884262 non-null float64\n",
      "FRAUD                          884262 non-null float64\n",
      "GAMBLING                       884262 non-null float64\n",
      "KIDNAPPING                     884262 non-null float64\n",
      "LARCENY/THEFT                  884262 non-null float64\n",
      "LIQUOR LAWS                    884262 non-null float64\n",
      "LOITERING                      884262 non-null float64\n",
      "MISSING PERSON                 884262 non-null float64\n",
      "NON-CRIMINAL                   884262 non-null float64\n",
      "OTHER OFFENSES                 884262 non-null float64\n",
      "PORNOGRAPHY/OBSCENE MAT        884262 non-null float64\n",
      "PROSTITUTION                   884262 non-null float64\n",
      "RECOVERED VEHICLE              884262 non-null float64\n",
      "ROBBERY                        884262 non-null float64\n",
      "RUNAWAY                        884262 non-null float64\n",
      "SECONDARY CODES                884262 non-null float64\n",
      "SEX OFFENSES FORCIBLE          884262 non-null float64\n",
      "SEX OFFENSES NON FORCIBLE      884262 non-null float64\n",
      "STOLEN PROPERTY                884262 non-null float64\n",
      "SUICIDE                        884262 non-null float64\n",
      "SUSPICIOUS OCC                 884262 non-null float64\n",
      "TREA                           884262 non-null float64\n",
      "TRESPASS                       884262 non-null float64\n",
      "VANDALISM                      884262 non-null float64\n",
      "VEHICLE THEFT                  884262 non-null float64\n",
      "WARRANTS                       884262 non-null float64\n",
      "WEAPON LAWS                    884262 non-null float64\n",
      "dtypes: float64(39), int32(1)\n",
      "memory usage: 273.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = np.c_[final_test_df['Id'].astype(int), submit_output.astype(float)]\n",
    "print result.shape\n",
    "outputColumns =  ['Id'] + list( category_encoder.classes_)\n",
    "df_result = pd.DataFrame(result, columns=outputColumns)\n",
    "df_result['Id'] = df_result['Id'].astype(int)\n",
    "print df_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_result.to_csv('10-SFCrimeMIDSChallengerTeam.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
