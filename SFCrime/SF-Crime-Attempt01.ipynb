{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## San-Franscisco Crime Predition Challenge - Kaggle\n",
    "### Team Member : Shanti Greene, Jing Xu, Abhishek Kumar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Description\n",
    "\n",
    "This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. \n",
    "\n",
    "##### train.csv / test.csv\n",
    "\n",
    "Data fields \n",
    "\n",
    " - Dates - timestamp of the crime incident \n",
    " - Category - category of the crime incident (only in train.csv). This is the target variable you are going to predict. \n",
    " - Descript - detailed description of the crime incident (only in train.csv) \n",
    " - DayOfWeek - the day of the week \n",
    " - PdDistrict - name of the Police Department District \n",
    " - Resolution - how the crime incident was resolved (only in train.csv) \n",
    " - Address - the approximate street address of the crime incident  \n",
    " - X - Longitude \n",
    " - Y - Latitude\n",
    " \n",
    "##### Submission data ( sampleSubmission.csv)\n",
    "\n",
    "You must submit a csv file with the incident id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following:\n",
    "\n",
    "\n",
    "##### evaluation criteria\n",
    "\n",
    "Submissions are evaluated using the multi-class logarithmic loss. Each incident has been labeled with one true class. For each incident, you must submit a set of predicted probabilities (one for every class). The formula is then,\n",
    "\n",
    "logloss=−1/N∑i=1 to N ∑ j=1 to M yijlog(pij),\n",
    "\n",
    "where N is the number of images in the test set, M is the number of class labels, log is the natural logarithm, yij is 1 if observation i is in class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j.\n",
    "\n",
    "The submitted probabilities for a given incident are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with max(min(p,1−10−15),10−15).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import gzip\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read train and test data files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#submission_df = pd.read_csv('sampleSubmission.csv')\n",
    "#street_df = pd.read_csv('Street_Names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>PARK</td>\n",
       "      <td>NONE</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dates        Category                      Descript  \\\n",
       "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
       "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "\n",
       "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
       "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
       "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
       "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.425892  37.774599  \n",
       "1 -122.425892  37.774599  \n",
       "2 -122.424363  37.800414  \n",
       "3 -122.426995  37.800873  \n",
       "4 -122.438738  37.771541  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show head of train_df\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>dist_WARRANTS</th>\n",
       "      <th>dist_OTHER OFFENSES</th>\n",
       "      <th>dist_LARCENY/THEFT</th>\n",
       "      <th>dist_VEHICLE THEFT</th>\n",
       "      <th>dist_VANDALISM</th>\n",
       "      <th>dist_NON-CRIMINAL</th>\n",
       "      <th>dist_ROBBERY</th>\n",
       "      <th>dist_ASSAULT</th>\n",
       "      <th>dist_WEAPON LAWS</th>\n",
       "      <th>...</th>\n",
       "      <th>dist_EMBEZZLEMENT</th>\n",
       "      <th>dist_SUICIDE</th>\n",
       "      <th>dist_LOITERING</th>\n",
       "      <th>dist_SEX OFFENSES NON FORCIBLE</th>\n",
       "      <th>dist_EXTORTION</th>\n",
       "      <th>dist_GAMBLING</th>\n",
       "      <th>dist_BAD CHECKS</th>\n",
       "      <th>dist_TREA</th>\n",
       "      <th>dist_RECOVERED VEHICLE</th>\n",
       "      <th>dist_PORNOGRAPHY/OBSCENE MAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>0.043677</td>\n",
       "      <td>0.116177</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.059746</td>\n",
       "      <td>0.051322</td>\n",
       "      <td>0.097250</td>\n",
       "      <td>0.025072</td>\n",
       "      <td>0.078996</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PARK</td>\n",
       "      <td>0.047006</td>\n",
       "      <td>0.125403</td>\n",
       "      <td>0.185468</td>\n",
       "      <td>0.080364</td>\n",
       "      <td>0.052988</td>\n",
       "      <td>0.120151</td>\n",
       "      <td>0.019407</td>\n",
       "      <td>0.071279</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.167455</td>\n",
       "      <td>0.129824</td>\n",
       "      <td>0.113641</td>\n",
       "      <td>0.068159</td>\n",
       "      <td>0.086917</td>\n",
       "      <td>0.035361</td>\n",
       "      <td>0.108225</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>0.048328</td>\n",
       "      <td>0.190683</td>\n",
       "      <td>0.113149</td>\n",
       "      <td>0.080721</td>\n",
       "      <td>0.059890</td>\n",
       "      <td>0.068198</td>\n",
       "      <td>0.030359</td>\n",
       "      <td>0.110219</td>\n",
       "      <td>0.018416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RICHMOND</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.124577</td>\n",
       "      <td>0.218828</td>\n",
       "      <td>0.091066</td>\n",
       "      <td>0.070340</td>\n",
       "      <td>0.127054</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.070827</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PdDistrict  dist_WARRANTS  dist_OTHER OFFENSES  dist_LARCENY/THEFT  \\\n",
       "0   NORTHERN       0.043677             0.116177            0.271900   \n",
       "1       PARK       0.047006             0.125403            0.185468   \n",
       "2  INGLESIDE       0.032063             0.167455            0.129824   \n",
       "3    BAYVIEW       0.048328             0.190683            0.113149   \n",
       "4   RICHMOND       0.022341             0.124577            0.218828   \n",
       "\n",
       "   dist_VEHICLE THEFT  dist_VANDALISM  dist_NON-CRIMINAL  dist_ROBBERY  \\\n",
       "0            0.059746        0.051322           0.097250      0.025072   \n",
       "1            0.080364        0.052988           0.120151      0.019407   \n",
       "2            0.113641        0.068159           0.086917      0.035361   \n",
       "3            0.080721        0.059890           0.068198      0.030359   \n",
       "4            0.091066        0.070340           0.127054      0.017408   \n",
       "\n",
       "   dist_ASSAULT  dist_WEAPON LAWS              ...               \\\n",
       "0      0.078996          0.007493              ...                \n",
       "1      0.071279          0.007239              ...                \n",
       "2      0.108225          0.014332              ...                \n",
       "3      0.110219          0.018416              ...                \n",
       "4      0.070827          0.007233              ...                \n",
       "\n",
       "   dist_EMBEZZLEMENT  dist_SUICIDE  dist_LOITERING  \\\n",
       "0           0.001244      0.000636        0.001833   \n",
       "1           0.001014      0.000406        0.000466   \n",
       "2           0.000989      0.000824        0.000330   \n",
       "3           0.001118      0.000414        0.000559   \n",
       "4           0.000951      0.000929        0.000177   \n",
       "\n",
       "   dist_SEX OFFENSES NON FORCIBLE  dist_EXTORTION  dist_GAMBLING  \\\n",
       "0                        0.000085        0.000228       0.000095   \n",
       "1                        0.000122        0.000162       0.000020   \n",
       "2                        0.000279        0.000368       0.000203   \n",
       "3                        0.000246        0.000145       0.000324   \n",
       "4                        0.000221        0.000509       0.000088   \n",
       "\n",
       "   dist_BAD CHECKS  dist_TREA  dist_RECOVERED VEHICLE  \\\n",
       "0         0.000513   0.000009                0.002593   \n",
       "1         0.000304   0.000000                0.002373   \n",
       "2         0.000406   0.000000                0.008396   \n",
       "3         0.000380   0.000034                0.008219   \n",
       "4         0.000686   0.000000                0.002765   \n",
       "\n",
       "   dist_PORNOGRAPHY/OBSCENE MAT  \n",
       "0                      0.000047  \n",
       "1                      0.000000  \n",
       "2                      0.000000  \n",
       "3                      0.000022  \n",
       "4                      0.000022  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetPropertionOfCrimeCategoryPerPdDistrcit():\n",
    "    # Propertion of Crime category per district\n",
    "    unique_categories = train_df['Category'].unique()\n",
    "    df_prop_crime_per_pdDisrict = []\n",
    "    for district in train_df['PdDistrict'].unique():   \n",
    "            current_district_crime_row = []\n",
    "            current_district_crime_row.append(district)\n",
    "            total_crime_count = len(train_df[train_df['PdDistrict'] == district])       \n",
    "            for category in unique_categories:\n",
    "                current_category_crime_count =  len(train_df[(train_df['PdDistrict'] == district) & (train_df['Category'] == category)])\n",
    "                proportion_category_crime_count = current_category_crime_count / float(total_crime_count)\n",
    "                current_district_crime_row.append(proportion_category_crime_count)\n",
    "            df_prop_crime_per_pdDisrict.append(current_district_crime_row)\n",
    "    columns = ['PdDistrict'] + list('dist_' + unique_categories)\n",
    "    df_prop_crime_per_pdDisrict =  pd.DataFrame(df_prop_crime_per_pdDisrict, columns=columns)\n",
    "    return df_prop_crime_per_pdDisrict\n",
    "\n",
    "\n",
    "df_prop_crime_per_pdDisrict = GetPropertionOfCrimeCategoryPerPdDistrcit()\n",
    "df_prop_crime_per_pdDisrict.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check its performance. Not working well. Use better technique\n",
    "def GetPropertionOfCrimeCategoryPerAddress():\n",
    "    # Propertion of Crime category per address\n",
    "    df_prop_crime_per_address = []\n",
    "    for address in train_df['Address'].unique():   \n",
    "            current_address_crime_row = []\n",
    "            current_address_crime_row.append(address)\n",
    "            total_crime_count = len(train_df[train_df['Address'] == address])       \n",
    "            for category in unique_categories:\n",
    "                current_category_crime_count =  len(train_df[(train_df['Address'] == address) & (train_df['Category'] == category)])\n",
    "                proportion_category_crime_count = current_category_crime_count / float(total_crime_count)\n",
    "                current_address_crime_row.append(proportion_category_crime_count)\n",
    "            df_prop_crime_per_address.append(current_address_crime_row)\n",
    "    columns = ['Address'] + list('address_' + unique_categories)\n",
    "    df_prop_crime_per_address =  pd.DataFrame(df_prop_crime_per_address, columns=columns)   \n",
    "    return df_prop_crime_per_address\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "train_df['loc'] = train_df['X'].map(lambda x: str(round(x,2))) +   train_df['Y'].map(lambda y: ' {0}'.format(str(round(y,2))))\n",
    "test_df['loc'] = test_df['X'].map(lambda x: str(round(x,2))) +   test_df['Y'].map(lambda y: ' {0}'.format(str(round(y,2))))\n",
    "unique_cordinates =train_df['loc'].unique().tolist()\n",
    "# check how to handle one extra unique cordinates\n",
    "#unique_cordinates = list(set(train_df['loc'].unique().tolist() + test_df['loc'].unique().tolist()))\n",
    "\n",
    "print len(unique_cordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 10 columns):\n",
      "Dates         878049 non-null object\n",
      "Category      878049 non-null object\n",
      "Descript      878049 non-null object\n",
      "DayOfWeek     878049 non-null object\n",
      "PdDistrict    878049 non-null object\n",
      "Resolution    878049 non-null object\n",
      "Address       878049 non-null object\n",
      "X             878049 non-null float64\n",
      "Y             878049 non-null float64\n",
      "loc           878049 non-null object\n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 73.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Dates</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-05-10 23:59:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>2000 Block of THOMAS AV</td>\n",
       "      <td>-122.399588</td>\n",
       "      <td>37.735051</td>\n",
       "      <td>-122.4 37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-10 23:51:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>3RD ST / REVERE AV</td>\n",
       "      <td>-122.391523</td>\n",
       "      <td>37.732432</td>\n",
       "      <td>-122.39 37.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-05-10 23:50:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2000 Block of GOUGH ST</td>\n",
       "      <td>-122.426002</td>\n",
       "      <td>37.792212</td>\n",
       "      <td>-122.43 37.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "      <td>-122.44 37.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "      <td>-122.44 37.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                Dates DayOfWeek PdDistrict                   Address  \\\n",
       "0   0  2015-05-10 23:59:00    Sunday    BAYVIEW   2000 Block of THOMAS AV   \n",
       "1   1  2015-05-10 23:51:00    Sunday    BAYVIEW        3RD ST / REVERE AV   \n",
       "2   2  2015-05-10 23:50:00    Sunday   NORTHERN    2000 Block of GOUGH ST   \n",
       "3   3  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "4   4  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "\n",
       "            X          Y            loc  \n",
       "0 -122.399588  37.735051   -122.4 37.74  \n",
       "1 -122.391523  37.732432  -122.39 37.73  \n",
       "2 -122.426002  37.792212  -122.43 37.79  \n",
       "3 -122.437394  37.721412  -122.44 37.72  \n",
       "4 -122.437394  37.721412  -122.44 37.72  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show head of test_df\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 8 columns):\n",
      "Id            884262 non-null int64\n",
      "Dates         884262 non-null object\n",
      "DayOfWeek     884262 non-null object\n",
      "PdDistrict    884262 non-null object\n",
      "Address       884262 non-null object\n",
      "X             884262 non-null float64\n",
      "Y             884262 non-null float64\n",
      "loc           884262 non-null object\n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 60.7+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show info for submission_df\n",
    "#print submission_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WARRANTS' 'OTHER OFFENSES' 'LARCENY/THEFT' 'VEHICLE THEFT' 'VANDALISM'\n",
      " 'NON-CRIMINAL' 'ROBBERY' 'ASSAULT' 'WEAPON LAWS' 'BURGLARY'\n",
      " 'SUSPICIOUS OCC' 'DRUNKENNESS' 'FORGERY/COUNTERFEITING' 'DRUG/NARCOTIC'\n",
      " 'STOLEN PROPERTY' 'SECONDARY CODES' 'TRESPASS' 'MISSING PERSON' 'FRAUD'\n",
      " 'KIDNAPPING' 'RUNAWAY' 'DRIVING UNDER THE INFLUENCE'\n",
      " 'SEX OFFENSES FORCIBLE' 'PROSTITUTION' 'DISORDERLY CONDUCT' 'ARSON'\n",
      " 'FAMILY OFFENSES' 'LIQUOR LAWS' 'BRIBERY' 'EMBEZZLEMENT' 'SUICIDE'\n",
      " 'LOITERING' 'SEX OFFENSES NON FORCIBLE' 'EXTORTION' 'GAMBLING'\n",
      " 'BAD CHECKS' 'TREA' 'RECOVERED VEHICLE' 'PORNOGRAPHY/OBSCENE MAT']\n",
      "['PdDistrict_NORTHERN', 'PdDistrict_PARK', 'PdDistrict_INGLESIDE', 'PdDistrict_BAYVIEW', 'PdDistrict_RICHMOND', 'PdDistrict_CENTRAL', 'PdDistrict_TARAVAL', 'PdDistrict_TENDERLOIN', 'PdDistrict_MISSION', 'PdDistrict_SOUTHERN']\n",
      "['Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12']\n",
      "['year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015']\n",
      "['season_winter', 'season_spring', 'season_summer']\n",
      "['cord_-122.43 37.77', 'cord_-122.42 37.8', 'cord_-122.43 37.8', 'cord_-122.44 37.77', 'cord_-122.4 37.71', 'cord_-122.42 37.73', 'cord_-122.37 37.73', 'cord_-122.51 37.78', 'cord_-122.42 37.81', 'cord_-122.49 37.74', 'cord_-122.41 37.78', 'cord_-122.43 37.78', 'cord_-122.4 37.73', 'cord_-122.38 37.74', 'cord_-122.42 37.74', 'cord_-122.39 37.74', 'cord_-122.45 37.74', 'cord_-122.42 37.75', 'cord_-122.44 37.8', 'cord_-122.41 37.79', 'cord_-122.43 37.79', 'cord_-122.48 37.74', 'cord_-122.5 37.75', 'cord_-122.44 37.76', 'cord_-122.39 37.78', 'cord_-122.47 37.72', 'cord_-122.43 37.71', 'cord_-122.42 37.77', 'cord_-122.42 37.79', 'cord_-122.47 37.77', 'cord_-122.45 37.77', 'cord_-122.4 37.75', 'cord_-122.4 37.79', 'cord_-122.4 37.78', 'cord_-122.41 37.77', 'cord_-122.47 37.73', 'cord_-122.42 37.78', 'cord_-122.4 37.72', 'cord_-122.41 37.76', 'cord_-122.4 37.8', 'cord_-122.41 37.75', 'cord_-122.44 37.78', 'cord_-122.47 37.74', 'cord_-122.39 37.76', 'cord_-122.45 37.78', 'cord_-122.41 37.8', 'cord_-122.38 37.73', 'cord_-122.42 37.76', 'cord_-122.41 37.74', 'cord_-122.5 37.78', 'cord_-122.43 37.76', 'cord_-122.44 37.72', 'cord_-122.43 37.72', 'cord_-122.4 37.74', 'cord_-122.45 37.76', 'cord_-122.51 37.77', 'cord_-122.4 37.76', 'cord_-122.39 37.73', 'cord_-122.46 37.73', 'cord_-122.46 37.72', 'cord_-122.45 37.8', 'cord_-122.39 37.77', 'cord_-122.45 37.73', 'cord_-122.41 37.81', 'cord_-122.48 37.75', 'cord_-122.44 37.73', 'cord_-122.49 37.78', 'cord_-122.48 37.76', 'cord_-122.48 37.77', 'cord_-122.4 37.77', 'cord_-122.47 37.79', 'cord_-122.42 37.71', 'cord_-122.44 37.74', 'cord_-122.49 37.75', 'cord_-122.48 37.78', 'cord_-122.41 37.73', 'cord_-122.45 37.71', 'cord_-122.47 37.76', 'cord_-122.39 37.79', 'cord_-122.46 37.78', 'cord_-122.46 37.79', 'cord_-122.43 37.73', 'cord_-122.45 37.72', 'cord_-122.51 37.75', 'cord_-122.47 37.78', 'cord_-122.46 37.71', 'cord_-122.44 37.79', 'cord_-122.44 37.71', 'cord_-122.47 37.71', 'cord_-122.5 37.74', 'cord_-122.39 37.75', 'cord_-122.44 37.75', 'cord_-122.45 37.75', 'cord_-122.48 37.73', 'cord_-122.43 37.74', 'cord_-122.43 37.75', 'cord_-122.51 37.74', 'cord_-122.49 37.71', 'cord_-122.46 37.77', 'cord_-122.49 37.73', 'cord_-122.45 37.79', 'cord_-122.5 37.72', 'cord_-122.48 37.72', 'cord_-122.4 37.81', 'cord_-122.46 37.74', 'cord_-122.49 37.76', 'cord_-122.41 37.71', 'cord_-122.46 37.76', 'cord_-122.42 37.72', 'cord_-122.5 37.77', 'cord_-122.39 37.72', 'cord_-122.46 37.75', 'cord_-122.47 37.75', 'cord_-122.43 37.81', 'cord_-122.41 37.72', 'cord_-122.37 37.81', 'cord_-122.5 37.73', 'cord_-122.39 37.8', 'cord_-122.5 37.76', 'cord_-122.49 37.79', 'cord_-122.38 37.72', 'cord_-122.49 37.77', 'cord_-122.39 37.71', 'cord_-122.38 37.76', 'cord_-122.51 37.76', 'cord_-122.49 37.72', 'cord_-122.48 37.79', 'cord_-122.44 37.81', 'cord_-122.5 37.79', 'cord_-122.38 37.75', 'cord_-122.5 37.71', 'cord_-122.37 37.82', 'cord_-122.48 37.71', 'cord_-122.45 37.81', 'cord_-122.51 37.73', 'cord_-122.38 37.71', 'cord_-122.47 37.81', 'cord_-120.5 90.0', 'cord_-122.36 37.81']\n",
      "['ARSON' 'ASSAULT' 'BAD CHECKS' 'BRIBERY' 'BURGLARY' 'DISORDERLY CONDUCT'\n",
      " 'DRIVING UNDER THE INFLUENCE' 'DRUG/NARCOTIC' 'DRUNKENNESS' 'EMBEZZLEMENT'\n",
      " 'EXTORTION' 'FAMILY OFFENSES' 'FORGERY/COUNTERFEITING' 'FRAUD' 'GAMBLING'\n",
      " 'KIDNAPPING' 'LARCENY/THEFT' 'LIQUOR LAWS' 'LOITERING' 'MISSING PERSON'\n",
      " 'NON-CRIMINAL' 'OTHER OFFENSES' 'PORNOGRAPHY/OBSCENE MAT' 'PROSTITUTION'\n",
      " 'RECOVERED VEHICLE' 'ROBBERY' 'RUNAWAY' 'SECONDARY CODES'\n",
      " 'SEX OFFENSES FORCIBLE' 'SEX OFFENSES NON FORCIBLE' 'STOLEN PROPERTY'\n",
      " 'SUICIDE' 'SUSPICIOUS OCC' 'TREA' 'TRESPASS' 'VANDALISM' 'VEHICLE THEFT'\n",
      " 'WARRANTS' 'WEAPON LAWS']\n",
      "['address_0', 'address_1', 'address_2', 'address_3', 'address_4', 'address_5', 'address_6', 'address_7', 'address_8', 'address_9', 'address_10', 'address_11', 'address_12', 'address_13', 'address_14', 'address_15', 'address_16', 'address_17', 'address_18', 'address_19', 'address_20', 'address_21', 'address_22', 'address_23', 'address_24', 'address_25', 'address_26', 'address_27', 'address_28', 'address_29', 'address_30', 'address_31', 'address_32', 'address_33', 'address_34', 'address_35', 'address_36', 'address_37', 'address_38', 'address_39', 'address_40', 'address_41', 'address_42', 'address_43', 'address_44', 'address_45', 'address_46', 'address_47', 'address_48', 'address_49', 'address_50', 'address_51', 'address_52', 'address_53', 'address_54', 'address_55', 'address_56', 'address_57', 'address_58', 'address_59', 'address_60', 'address_61', 'address_62', 'address_63', 'address_64', 'address_65', 'address_66', 'address_67', 'address_68', 'address_69', 'address_70', 'address_71', 'address_72', 'address_73', 'address_74', 'address_75', 'address_76', 'address_77', 'address_78', 'address_79', 'address_80', 'address_81', 'address_82', 'address_83', 'address_84', 'address_85', 'address_86', 'address_87', 'address_88', 'address_89', 'address_90', 'address_91', 'address_92', 'address_93', 'address_94', 'address_95', 'address_96', 'address_97', 'address_98', 'address_99']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = train_df['Category'].unique()\n",
    "print unique_categories\n",
    "\n",
    "# creating features for district and dayofWeek\n",
    "# for discrict\n",
    "Unique_PdDistricts = train_df['PdDistrict'].unique()\n",
    "PdDistrict_features = ['PdDistrict_{0}'.format(district) for district in Unique_PdDistricts]\n",
    "print PdDistrict_features\n",
    "# for days of week\n",
    "Unique_DayOfWeek = train_df['DayOfWeek'].unique()\n",
    "DayOfWeek_features = ['DayOfWeek_{0}'.format(day) for day in Unique_DayOfWeek]\n",
    "# for month\n",
    "month_features = ['Month_{0}'.format(month) for month in range(1,13)]\n",
    "print month_features\n",
    "# for year\n",
    "year_features = ['year_{0}'.format(year) for year in range(2003,2016)]\n",
    "print year_features\n",
    "# for season \n",
    "season_features = ['season_winter','season_spring','season_summer']\n",
    "print season_features\n",
    "#for cordinates\n",
    "cordinate_features = ['cord_{0}'.format(cord) for cord in unique_cordinates]\n",
    "print cordinate_features\n",
    "\n",
    "# for category\n",
    "category_encoder = LabelEncoder()\n",
    "category_encoder.fit(unique_categories) \n",
    "print category_encoder.classes_\n",
    "\n",
    "num_address_features = 100\n",
    "address_vectorizer = HashingVectorizer(decode_error='ignore', n_features=num_address_features,non_negative=True)\n",
    "address_features = ['address_{0}'.format(i) for i in range(num_address_features)]\n",
    "print address_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130\n"
     ]
    }
   ],
   "source": [
    "# Processing Address Feature\n",
    "def removePunctuation(text):\n",
    "    '''\n",
    "    function to remove punctuations\n",
    "    '''\n",
    "    # create a regex for punctuations\n",
    "    punct = re.compile(r'([^A-Za-z0-9 ])')\n",
    "    # replace the punctuation with empty space\n",
    "    return punct.sub(\" \", text)\n",
    "\n",
    "# removing punctuation from address values\n",
    "train_df['Address'] = train_df['Address'].map(lambda x : removePunctuation(x))\n",
    "test_df['Address'] = test_df['Address'].map(lambda x : removePunctuation(x))\n",
    "\n",
    "\n",
    "# count vectorizer for address\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_address_features = vectorizer.fit_transform(train_df['Address'].values)\n",
    "test_adddress_features = vectorizer.transform(test_df['Address'].values)\n",
    "print len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ProcessData(df, datatype):\n",
    "    # set the correct type for dates column\n",
    "    df['Dates'] = df['Dates'].astype('datetime64[ns]')   \n",
    "    # adding Features\n",
    "    df = addFeatures(df)\n",
    "       \n",
    "    # drop columns not needed now\n",
    "    if datatype == 'train':\n",
    "        df = df.drop(['Descript','Resolution','Address', 'Dates','PdDistrict','DayOfWeek'], axis=1)\n",
    "    if datatype == 'test':\n",
    "        df = df.drop(['Address','Dates','PdDistrict','DayOfWeek'], axis=1)\n",
    "    return df\n",
    "    \n",
    "def addFeatures(df):   \n",
    "    #df  = processPdDiscrictCrimePropertion(df)   # not working well.\n",
    "    df = processPdDiscrict(df)  \n",
    "    df = processDayOfWeek(df)\n",
    "    df = processCordinates(df)\n",
    "    #df = processAddress(df) # not working well. use some some other technique to extract features\n",
    "    df = addHourOfCrime(df)\n",
    "    df = addMonthOfCrime(df)\n",
    "    df = addYearOfCrime(df)\n",
    "    df = addSeasonOfCrime(df)\n",
    "    return df\n",
    "    \n",
    "def processPdDiscrictCrimePropertion(df):    \n",
    "    df  = pd.merge(df, df_prop_crime_per_pdDisrict, on='PdDistrict',how='left') \n",
    "    return df\n",
    "\n",
    "\n",
    "def processPdDiscrict(df):   \n",
    "    new_PdDistrict_df = pd.get_dummies(df['PdDistrict'], prefix='PdDistrict')   \n",
    "    new_PdDistrict_df = new_PdDistrict_df[PdDistrict_features]\n",
    "    df  = pd.concat([df, new_PdDistrict_df], axis=1)   \n",
    "    return df\n",
    "\n",
    "\n",
    "def processCordinates(df):   \n",
    "    new_cord_df = pd.get_dummies(df['loc'], prefix='cord')   \n",
    "    new_cord_df = new_cord_df[cordinate_features]\n",
    "    df  = pd.concat([df, new_cord_df], axis=1)   \n",
    "    return df\n",
    "\n",
    "def processDayOfWeek(df):   \n",
    "    new_DayOfWeek_df = pd.get_dummies(df['DayOfWeek'], prefix='DayOfWeek')   \n",
    "    new_DayOfWeek_df = new_DayOfWeek_df[DayOfWeek_features]\n",
    "    df  = pd.concat([df, new_DayOfWeek_df], axis=1)    \n",
    "    return df\n",
    "    \n",
    "def addHourOfCrime(df):      \n",
    "    df['HourOfCrime'] = df['Dates'].map(lambda d: d.hour + d.minute / 60.)  \n",
    "    return df\n",
    "\n",
    "def addMonthOfCrime(df):      \n",
    "    #df['MonthOfCrime'] = df['Dates'].map(lambda d: d.month)  \n",
    "    new_month_df = pd.get_dummies(df['Dates'].map(lambda d: d.month) , prefix='Month')   \n",
    "    new_month_df = new_month_df[month_features]\n",
    "    df  = pd.concat([df, new_month_df], axis=1)    \n",
    "    return df\n",
    "\n",
    "def addYearOfCrime(df):          \n",
    "    new_year_df = pd.get_dummies(df['Dates'].map(lambda d: d.year) , prefix='year')   \n",
    "    new_year_df = new_year_df[year_features]\n",
    "    df  = pd.concat([df, new_year_df], axis=1)    \n",
    "    return df\n",
    "\n",
    "# March - June \"Spring\", July - October \"Summer\", November - February \"Winter\"\n",
    "def addSeasonOfCrime(df):        \n",
    "    new_month_df = pd.get_dummies(df['Dates'].map(lambda d: GetSeason(d.month)) , prefix='season')   \n",
    "    new_month_df = new_month_df[season_features]   \n",
    "    df  = pd.concat([df, new_month_df], axis=1)    \n",
    "    return df\n",
    "def GetSeason(month):\n",
    "    if month == 3 or month == 4 or month == 5 or month == 6:\n",
    "        return 'spring'\n",
    "    if month == 7 or month == 8 or month == 9 or month == 10:\n",
    "        return 'summer'\n",
    "    if month == 11 or month == 12 or month == 1 or month == 2:\n",
    "        return 'winter'\n",
    "\n",
    "def processAddress(df):   \n",
    "    address_hashed = address_vectorizer.fit_transform(df['Address'])\n",
    "    new_address_df = pd.SparseDataFrame([ pd.SparseSeries(address_hashed[i].toarray().ravel()) \n",
    "                                   for i in np.arange(address_hashed.shape[0]) ])\n",
    "    new_address_df.columns = address_features\n",
    "    df  = pd.concat([df, new_address_df], axis=1)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_train_df = ProcessData(train_df, 'train')\n",
    "final_train_df['Category'] = category_encoder.transform(final_train_df['Category'])  \n",
    "final_test_df = ProcessData(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del train_df\n",
    "del test_df\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Columns: 189 entries, Category to season_summer\n",
      "dtypes: float64(187), int64(1), object(1)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Columns: 189 entries, Id to season_summer\n",
      "dtypes: float64(187), int64(1), object(1)\n",
      "memory usage: 1.3+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print final_train_df.info()\n",
    "print final_test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Final Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input features\n",
    "to_be_scaled_features = []\n",
    "#to_be_scaled_features +=  list('dist_' + unique_categories)\n",
    "to_be_scaled_features +=  ['HourOfCrime']\n",
    "no_scale_features = []\n",
    "no_scale_features += PdDistrict_features \n",
    "no_scale_features += DayOfWeek_features \n",
    "no_scale_features += month_features \n",
    "no_scale_features += year_features \n",
    "no_scale_features += cordinate_features\n",
    "no_scale_features += season_features\n",
    "inputFeatures = []\n",
    "inputFeatures = to_be_scaled_features + no_scale_features\n",
    "#output features\n",
    "ouptutFeatures = ['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = final_train_df[inputFeatures].values\n",
    "Y = final_train_df[ouptutFeatures].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del final_train_df\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L, 2315L)\n"
     ]
    }
   ],
   "source": [
    "# append count vectorizer features for address\n",
    "X = np.concatenate((X,  train_address_features.toarray()), axis=1)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  scaling data\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X[:,:len(to_be_scaled_features)] = scaler.fit_transform(X[:,:len(to_be_scaled_features)])\n",
    "#X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del train_address_features\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(X,Y, train_size= 0.8):\n",
    "    shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "    X, Y = X[shuffle], Y[shuffle]\n",
    "    train_index = int(math.floor(X.shape[0] * train_size))    \n",
    "    train_data, train_labels = X[:train_index], Y[:train_index]\n",
    "    dev_data, dev_labels = X[train_index + 1:], Y[train_index + 1:]\n",
    "    return train_data, dev_data, train_labels, dev_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(702439L, 2315L) (702439L, 1L)\n",
      "(175609L, 2315L) (175609L, 1L)\n",
      "39 39\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, train_labels, dev_labels = split_dataset(X,Y, train_size = 0.8)\n",
    "print train_data.shape, train_labels.shape\n",
    "print dev_data.shape, dev_labels.shape\n",
    "print len(np.unique(train_labels)), len(np.unique(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing variables from the memory which are not required further\n",
    "del X\n",
    "del Y\n",
    "gc.collect() # garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch training of random forest\n",
    "def getRows(rows, data, labels):\n",
    "    return data[rows], labels[rows].ravel()\n",
    "\n",
    "def iter_minibatches(chunksize, data, labels):\n",
    "    numtrainingpoints = len(data)     \n",
    "    chunkstartmarker = 0\n",
    "    while chunkstartmarker < numtrainingpoints:\n",
    "        start = chunkstartmarker\n",
    "        if start + chunksize < numtrainingpoints:\n",
    "            end = chunkstartmarker + chunksize\n",
    "        else:\n",
    "            end = numtrainingpoints\n",
    "        chunkrows = range(start,end)       \n",
    "        X_chunk, y_chunk = getRows(chunkrows, data, labels)        \n",
    "        yield X_chunk, y_chunk\n",
    "        chunkstartmarker += chunksize       \n",
    "        \n",
    "def train_model(data, labels):\n",
    "    model = batch_SGDClassifier(data, labels)\n",
    "    #model = MultinomialNBClassifier(data, labels)\n",
    "    #model = batch_RandomForest(data, labels)\n",
    "    return model\n",
    "    \n",
    "def batch_RandomForest(data, labels):\n",
    "    model = RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth=5)\n",
    "    model.fit(data, labels)\n",
    "    return model  \n",
    "    \n",
    "def MultinomialNBClassifier(data, labels):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(data,labels)\n",
    "    return model\n",
    "\n",
    "def batch_SGDClassifier(data,labels):\n",
    "    chunk_size = 1000\n",
    "    batcherator = iter_minibatches(chunk_size, data, labels)\n",
    "    model = SGDClassifier(n_jobs=-1,alpha=0.00005,n_iter = 50, loss='log')\n",
    "    # Train model on each chunk\n",
    "    chunk_count = 1\n",
    "    for X_chunk, y_chunk in batcherator:\n",
    "               \n",
    "        model.partial_fit(X_chunk, y_chunk, classes=np.array(range(len(category_encoder.classes_))))\n",
    "        score = model.score(dev_data, dev_labels)\n",
    "        probs = model.predict_proba(dev_data)\n",
    "        log_loss_value = log_loss(dev_labels, probs)\n",
    "        print 'training using chunk : {0} validation score : {1:.4f} log-loss : {2:.5f}'.format(chunk_count,score,log_loss_value) \n",
    "        chunk_count += 1\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training using chunk : 1 validation score : 0.1422 log-loss : 24.48373\n",
      "training using chunk : 2 validation score : 0.1137 log-loss : 24.30079\n",
      "training using chunk : 3 validation score : 0.1578 log-loss : 20.48776\n",
      "training using chunk : 4 validation score : 0.1046 log-loss : 22.26809\n",
      "training using chunk : 5 validation score : 0.1102 log-loss : 19.66606\n",
      "training using chunk : 6 validation score : 0.1012 log-loss : 17.37252\n",
      "training using chunk : 7 validation score : 0.1349 log-loss : 14.40994\n",
      "training using chunk : 8 validation score : 0.1345 log-loss : 14.03948\n",
      "training using chunk : 9 validation score : 0.1391 log-loss : 13.09857\n",
      "training using chunk : 10 validation score : 0.1073 log-loss : 12.00916\n",
      "training using chunk : 11 validation score : 0.1286 log-loss : 9.13713\n",
      "training using chunk : 12 validation score : 0.1398 log-loss : 10.41530\n",
      "training using chunk : 13 validation score : 0.1293 log-loss : 9.61346\n",
      "training using chunk : 14 validation score : 0.1409 log-loss : 8.19500\n",
      "training using chunk : 15 validation score : 0.0980 log-loss : 9.24534\n",
      "training using chunk : 16 validation score : 0.1335 log-loss : 7.28902\n",
      "training using chunk : 17 validation score : 0.1263 log-loss : 7.87004\n",
      "training using chunk : 18 validation score : 0.1397 log-loss : 7.02132\n",
      "training using chunk : 19 validation score : 0.1481 log-loss : 6.02150\n",
      "training using chunk : 20 validation score : 0.1305 log-loss : 5.95926\n",
      "training using chunk : 21 validation score : 0.1304 log-loss : 6.13270\n",
      "training using chunk : 22 validation score : 0.1758 log-loss : 5.78535\n",
      "training using chunk : 23 validation score : 0.1147 log-loss : 6.14279\n",
      "training using chunk : 24 validation score : 0.1438 log-loss : 5.51605\n",
      "training using chunk : 25 validation score : 0.1463 log-loss : 5.13754\n",
      "training using chunk : 26 validation score : 0.1634 log-loss : 5.14313\n",
      "training using chunk : 27 validation score : 0.1599 log-loss : 4.79633\n",
      "training using chunk : 28 validation score : 0.1586 log-loss : 4.85433\n",
      "training using chunk : 29 validation score : 0.1443 log-loss : 4.98984\n",
      "training using chunk : 30 validation score : 0.1844 log-loss : 4.38649\n",
      "training using chunk : 31 validation score : 0.1463 log-loss : 4.69975\n",
      "training using chunk : 32 validation score : 0.1471 log-loss : 4.68542\n",
      "training using chunk : 33 validation score : 0.1739 log-loss : 4.47098\n",
      "training using chunk : 34 validation score : 0.1311 log-loss : 4.25943\n",
      "training using chunk : 35 validation score : 0.1734 log-loss : 4.00771\n",
      "training using chunk : 36 validation score : 0.1206 log-loss : 4.54366\n",
      "training using chunk : 37 validation score : 0.1375 log-loss : 4.28503\n",
      "training using chunk : 38 validation score : 0.1673 log-loss : 3.71835\n",
      "training using chunk : 39 validation score : 0.1878 log-loss : 3.81142\n",
      "training using chunk : 40 validation score : 0.1751 log-loss : 3.73333\n",
      "training using chunk : 41 validation score : 0.1788 log-loss : 3.59252\n",
      "training using chunk : 42 validation score : 0.1631 log-loss : 3.89647\n",
      "training using chunk : 43 validation score : 0.1491 log-loss : 3.80890\n",
      "training using chunk : 44 validation score : 0.2045 log-loss : 3.54673\n",
      "training using chunk : 45 validation score : 0.1649 log-loss : 3.67021\n",
      "training using chunk : 46 validation score : 0.1732 log-loss : 3.56608\n",
      "training using chunk : 47 validation score : 0.1951 log-loss : 3.61699\n",
      "training using chunk : 48 validation score : 0.1715 log-loss : 3.42306\n",
      "training using chunk : 49 validation score : 0.1985 log-loss : 3.74643\n",
      "training using chunk : 50 validation score : 0.2039 log-loss : 3.52224\n",
      "training using chunk : 51 validation score : 0.1788 log-loss : 3.46635\n",
      "training using chunk : 52 validation score : 0.1350 log-loss : 3.72753\n",
      "training using chunk : 53 validation score : 0.1615 log-loss : 3.46595\n",
      "training using chunk : 54 validation score : 0.1906 log-loss : 3.37595\n",
      "training using chunk : 55 validation score : 0.1800 log-loss : 3.44600\n",
      "training using chunk : 56 validation score : 0.1900 log-loss : 3.24118\n",
      "training using chunk : 57 validation score : 0.1739 log-loss : 3.31443\n",
      "training using chunk : 58 validation score : 0.2026 log-loss : 3.49136\n",
      "training using chunk : 59 validation score : 0.1975 log-loss : 3.16493\n",
      "training using chunk : 60 validation score : 0.1817 log-loss : 3.12546\n",
      "training using chunk : 61 validation score : 0.1663 log-loss : 3.32126\n",
      "training using chunk : 62 validation score : 0.1868 log-loss : 3.15437\n",
      "training using chunk : 63 validation score : 0.1836 log-loss : 3.19763\n",
      "training using chunk : 64 validation score : 0.1609 log-loss : 3.34412\n",
      "training using chunk : 65 validation score : 0.1725 log-loss : 3.20776\n",
      "training using chunk : 66 validation score : 0.1873 log-loss : 3.06529\n",
      "training using chunk : 67 validation score : 0.1600 log-loss : 3.22017\n",
      "training using chunk : 68 validation score : 0.1846 log-loss : 3.03527\n",
      "training using chunk : 69 validation score : 0.1558 log-loss : 3.25272\n",
      "training using chunk : 70 validation score : 0.1546 log-loss : 3.19620\n",
      "training using chunk : 71 validation score : 0.1899 log-loss : 3.20339\n",
      "training using chunk : 72 validation score : 0.1754 log-loss : 3.15265\n",
      "training using chunk : 73 validation score : 0.1819 log-loss : 3.07476\n",
      "training using chunk : 74 validation score : 0.1721 log-loss : 3.10848\n",
      "training using chunk : 75 validation score : 0.2095 log-loss : 3.16965\n",
      "training using chunk : 76 validation score : 0.1835 log-loss : 3.00822\n",
      "training using chunk : 77 validation score : 0.1636 log-loss : 3.12260\n",
      "training using chunk : 78 validation score : 0.1670 log-loss : 3.15215\n",
      "training using chunk : 79 validation score : 0.1854 log-loss : 2.99797\n",
      "training using chunk : 80 validation score : 0.1623 log-loss : 3.04633\n",
      "training using chunk : 81 validation score : 0.1658 log-loss : 3.06412\n",
      "training using chunk : 82 validation score : 0.1973 log-loss : 2.93093\n",
      "training using chunk : 83 validation score : 0.1760 log-loss : 3.01503\n",
      "training using chunk : 84 validation score : 0.1829 log-loss : 3.03841\n",
      "training using chunk : 85 validation score : 0.1575 log-loss : 3.01748\n",
      "training using chunk : 86 validation score : 0.2004 log-loss : 2.99265\n",
      "training using chunk : 87 validation score : 0.1646 log-loss : 3.06193\n",
      "training using chunk : 88 validation score : 0.2232 log-loss : 2.93479\n",
      "training using chunk : 89 validation score : 0.1979 log-loss : 2.86092\n",
      "training using chunk : 90 validation score : 0.2031 log-loss : 2.90947\n",
      "training using chunk : 91 validation score : 0.1988 log-loss : 2.97791\n",
      "training using chunk : 92 validation score : 0.2110 log-loss : 2.84473\n",
      "training using chunk : 93 validation score : 0.1920 log-loss : 2.92556\n",
      "training using chunk : 94 validation score : 0.1559 log-loss : 3.05131\n",
      "training using chunk : 95 validation score : 0.1746 log-loss : 3.00675\n",
      "training using chunk : 96 validation score : 0.1992 log-loss : 2.84681\n",
      "training using chunk : 97 validation score : 0.2148 log-loss : 2.85668\n",
      "training using chunk : 98 validation score : 0.1968 log-loss : 2.86254\n",
      "training using chunk : 99 validation score : 0.2022 log-loss : 2.92042\n",
      "training using chunk : 100 validation score : 0.1847 log-loss : 2.91984\n",
      "training using chunk : 101 validation score : 0.2121 log-loss : 2.96857\n",
      "training using chunk : 102 validation score : 0.2138 log-loss : 2.93908\n",
      "training using chunk : 103 validation score : 0.1895 log-loss : 2.89781\n",
      "training using chunk : 104 validation score : 0.1599 log-loss : 2.97917\n",
      "training using chunk : 105 validation score : 0.1995 log-loss : 2.83605\n",
      "training using chunk : 106 validation score : 0.2015 log-loss : 2.81545\n",
      "training using chunk : 107 validation score : 0.1994 log-loss : 2.85537\n",
      "training using chunk : 108 validation score : 0.2153 log-loss : 2.81599\n",
      "training using chunk : 109 validation score : 0.2088 log-loss : 2.81632\n",
      "training using chunk : 110 validation score : 0.1961 log-loss : 2.79636\n",
      "training using chunk : 111 validation score : 0.2052 log-loss : 2.85905\n",
      "training using chunk : 112 validation score : 0.1963 log-loss : 2.88159\n",
      "training using chunk : 113 validation score : 0.2104 log-loss : 2.79172\n",
      "training using chunk : 114 validation score : 0.2184 log-loss : 2.79732\n",
      "training using chunk : 115 validation score : 0.1788 log-loss : 2.81907\n",
      "training using chunk : 116 validation score : 0.2170 log-loss : 2.72732\n",
      "training using chunk : 117 validation score : 0.2011 log-loss : 2.79036\n",
      "training using chunk : 118 validation score : 0.1574 log-loss : 2.93482\n",
      "training using chunk : 119 validation score : 0.2115 log-loss : 2.80261\n",
      "training using chunk : 120 validation score : 0.2118 log-loss : 2.80024\n",
      "training using chunk : 121 validation score : 0.2185 log-loss : 2.77958\n",
      "training using chunk : 122 validation score : 0.2055 log-loss : 2.78027\n",
      "training using chunk : 123 validation score : 0.1979 log-loss : 2.80803\n",
      "training using chunk : 124 validation score : 0.2214 log-loss : 2.76056\n",
      "training using chunk : 125 validation score : 0.2175 log-loss : 2.77645\n",
      "training using chunk : 126 validation score : 0.1979 log-loss : 2.80442\n",
      "training using chunk : 127 validation score : 0.2269 log-loss : 2.78998\n",
      "training using chunk : 128 validation score : 0.1799 log-loss : 2.84659\n",
      "training using chunk : 129 validation score : 0.2269 log-loss : 2.73335\n",
      "training using chunk : 130 validation score : 0.2284 log-loss : 2.79246\n",
      "training using chunk : 131 validation score : 0.2096 log-loss : 2.77333\n",
      "training using chunk : 132 validation score : 0.2008 log-loss : 2.76733\n",
      "training using chunk : 133 validation score : 0.1938 log-loss : 2.78397\n",
      "training using chunk : 134 validation score : 0.2125 log-loss : 2.73177\n",
      "training using chunk : 135 validation score : 0.2087 log-loss : 2.73625\n",
      "training using chunk : 136 validation score : 0.2062 log-loss : 2.80166\n",
      "training using chunk : 137 validation score : 0.2074 log-loss : 2.74738\n",
      "training using chunk : 138 validation score : 0.2032 log-loss : 2.74817\n",
      "training using chunk : 139 validation score : 0.2324 log-loss : 2.70527\n",
      "training using chunk : 140 validation score : 0.2193 log-loss : 2.74392\n",
      "training using chunk : 141 validation score : 0.2227 log-loss : 2.71015\n",
      "training using chunk : 142 validation score : 0.2238 log-loss : 2.71652\n",
      "training using chunk : 143 validation score : 0.2290 log-loss : 2.81281\n",
      "training using chunk : 144 validation score : 0.2164 log-loss : 2.72923\n",
      "training using chunk : 145 validation score : 0.2039 log-loss : 2.69632\n",
      "training using chunk : 146 validation score : 0.2224 log-loss : 2.73072\n",
      "training using chunk : 147 validation score : 0.2350 log-loss : 2.72871\n",
      "training using chunk : 148 validation score : 0.2316 log-loss : 2.68253\n",
      "training using chunk : 149 validation score : 0.2131 log-loss : 2.71786\n",
      "training using chunk : 150 validation score : 0.1995 log-loss : 2.70947\n",
      "training using chunk : 151 validation score : 0.1979 log-loss : 2.72951\n",
      "training using chunk : 152 validation score : 0.2175 log-loss : 2.70899\n",
      "training using chunk : 153 validation score : 0.2075 log-loss : 2.72861\n",
      "training using chunk : 154 validation score : 0.2332 log-loss : 2.68744\n",
      "training using chunk : 155 validation score : 0.1932 log-loss : 2.74831\n",
      "training using chunk : 156 validation score : 0.2046 log-loss : 2.69292\n",
      "training using chunk : 157 validation score : 0.1982 log-loss : 2.71156\n",
      "training using chunk : 158 validation score : 0.2046 log-loss : 2.71756\n",
      "training using chunk : 159 validation score : 0.2245 log-loss : 2.68797\n",
      "training using chunk : 160 validation score : 0.2254 log-loss : 2.73106\n",
      "training using chunk : 161 validation score : 0.2243 log-loss : 2.70357\n",
      "training using chunk : 162 validation score : 0.2027 log-loss : 2.71261\n",
      "training using chunk : 163 validation score : 0.2178 log-loss : 2.69342\n",
      "training using chunk : 164 validation score : 0.2326 log-loss : 2.67045\n",
      "training using chunk : 165 validation score : 0.2174 log-loss : 2.67827\n",
      "training using chunk : 166 validation score : 0.2133 log-loss : 2.68685\n",
      "training using chunk : 167 validation score : 0.2227 log-loss : 2.67335\n",
      "training using chunk : 168 validation score : 0.2316 log-loss : 2.66609\n",
      "training using chunk : 169 validation score : 0.2232 log-loss : 2.69118\n",
      "training using chunk : 170 validation score : 0.2245 log-loss : 2.68893\n",
      "training using chunk : 171 validation score : 0.2232 log-loss : 2.70620\n",
      "training using chunk : 172 validation score : 0.2125 log-loss : 2.69743\n",
      "training using chunk : 173 validation score : 0.2226 log-loss : 2.68711\n",
      "training using chunk : 174 validation score : 0.2136 log-loss : 2.70160\n",
      "training using chunk : 175 validation score : 0.2176 log-loss : 2.67446\n",
      "training using chunk : 176 validation score : 0.2207 log-loss : 2.68237\n",
      "training using chunk : 177 validation score : 0.2187 log-loss : 2.66736\n",
      "training using chunk : 178 validation score : 0.2352 log-loss : 2.62447\n",
      "training using chunk : 179 validation score : 0.2040 log-loss : 2.68589\n",
      "training using chunk : 180 validation score : 0.2029 log-loss : 2.65811\n",
      "training using chunk : 181 validation score : 0.2132 log-loss : 2.67344\n",
      "training using chunk : 182 validation score : 0.2164 log-loss : 2.66626\n",
      "training using chunk : 183 validation score : 0.2313 log-loss : 2.69593\n",
      "training using chunk : 184 validation score : 0.1963 log-loss : 2.73232\n",
      "training using chunk : 185 validation score : 0.2189 log-loss : 2.69159\n",
      "training using chunk : 186 validation score : 0.2274 log-loss : 2.61849\n",
      "training using chunk : 187 validation score : 0.2213 log-loss : 2.65044\n",
      "training using chunk : 188 validation score : 0.2365 log-loss : 2.62862\n",
      "training using chunk : 189 validation score : 0.2338 log-loss : 2.62118\n",
      "training using chunk : 190 validation score : 0.2189 log-loss : 2.67311\n",
      "training using chunk : 191 validation score : 0.2021 log-loss : 2.67938\n",
      "training using chunk : 192 validation score : 0.2317 log-loss : 2.61744\n",
      "training using chunk : 193 validation score : 0.2181 log-loss : 2.67807\n",
      "training using chunk : 194 validation score : 0.1956 log-loss : 2.69015\n",
      "training using chunk : 195 validation score : 0.2303 log-loss : 2.62909\n",
      "training using chunk : 196 validation score : 0.2185 log-loss : 2.66295\n",
      "training using chunk : 197 validation score : 0.2191 log-loss : 2.65841\n",
      "training using chunk : 198 validation score : 0.2106 log-loss : 2.69496\n",
      "training using chunk : 199 validation score : 0.2237 log-loss : 2.63054\n",
      "training using chunk : 200 validation score : 0.2322 log-loss : 2.61275\n",
      "training using chunk : 201 validation score : 0.2092 log-loss : 2.66413\n",
      "training using chunk : 202 validation score : 0.2201 log-loss : 2.62615\n",
      "training using chunk : 203 validation score : 0.2107 log-loss : 2.65476\n",
      "training using chunk : 204 validation score : 0.2417 log-loss : 2.60814\n",
      "training using chunk : 205 validation score : 0.2300 log-loss : 2.63955\n",
      "training using chunk : 206 validation score : 0.2256 log-loss : 2.61817\n",
      "training using chunk : 207 validation score : 0.1931 log-loss : 2.68382\n",
      "training using chunk : 208 validation score : 0.2282 log-loss : 2.61595\n",
      "training using chunk : 209 validation score : 0.2146 log-loss : 2.64668\n",
      "training using chunk : 210 validation score : 0.2385 log-loss : 2.61279\n",
      "training using chunk : 211 validation score : 0.2314 log-loss : 2.63272\n",
      "training using chunk : 212 validation score : 0.2245 log-loss : 2.61032\n",
      "training using chunk : 213 validation score : 0.2191 log-loss : 2.61421\n",
      "training using chunk : 214 validation score : 0.2210 log-loss : 2.63119\n",
      "training using chunk : 215 validation score : 0.2224 log-loss : 2.63134\n",
      "training using chunk : 216 validation score : 0.2356 log-loss : 2.64340\n",
      "training using chunk : 217 validation score : 0.2077 log-loss : 2.60556\n",
      "training using chunk : 218 validation score : 0.2151 log-loss : 2.63465\n",
      "training using chunk : 219 validation score : 0.2410 log-loss : 2.61416\n",
      "training using chunk : 220 validation score : 0.2453 log-loss : 2.59743\n",
      "training using chunk : 221 validation score : 0.2269 log-loss : 2.64671\n",
      "training using chunk : 222 validation score : 0.2335 log-loss : 2.61658\n",
      "training using chunk : 223 validation score : 0.2304 log-loss : 2.61544\n",
      "training using chunk : 224 validation score : 0.2259 log-loss : 2.61139\n",
      "training using chunk : 225 validation score : 0.2158 log-loss : 2.62623\n",
      "training using chunk : 226 validation score : 0.2365 log-loss : 2.60167\n",
      "training using chunk : 227 validation score : 0.2333 log-loss : 2.58500\n",
      "training using chunk : 228 validation score : 0.2402 log-loss : 2.60239\n",
      "training using chunk : 229 validation score : 0.2297 log-loss : 2.61635\n",
      "training using chunk : 230 validation score : 0.2316 log-loss : 2.60048\n",
      "training using chunk : 231 validation score : 0.2179 log-loss : 2.61068\n",
      "training using chunk : 232 validation score : 0.2340 log-loss : 2.59641\n",
      "training using chunk : 233 validation score : 0.2405 log-loss : 2.63248\n",
      "training using chunk : 234 validation score : 0.2298 log-loss : 2.65401\n",
      "training using chunk : 235 validation score : 0.2357 log-loss : 2.59131\n",
      "training using chunk : 236 validation score : 0.2324 log-loss : 2.60385\n",
      "training using chunk : 237 validation score : 0.2455 log-loss : 2.59491\n",
      "training using chunk : 238 validation score : 0.2126 log-loss : 2.61495\n",
      "training using chunk : 239 validation score : 0.2287 log-loss : 2.58992\n",
      "training using chunk : 240 validation score : 0.2185 log-loss : 2.61359\n",
      "training using chunk : 241 validation score : 0.2390 log-loss : 2.59572\n",
      "training using chunk : 242 validation score : 0.2239 log-loss : 2.64220\n",
      "training using chunk : 243 validation score : 0.2234 log-loss : 2.60197\n",
      "training using chunk : 244 validation score : 0.2316 log-loss : 2.61013\n",
      "training using chunk : 245 validation score : 0.2428 log-loss : 2.60689\n",
      "training using chunk : 246 validation score : 0.2378 log-loss : 2.58641\n",
      "training using chunk : 247 validation score : 0.2393 log-loss : 2.62426\n",
      "training using chunk : 248 validation score : 0.2345 log-loss : 2.60397\n",
      "training using chunk : 249 validation score : 0.2270 log-loss : 2.62059\n",
      "training using chunk : 250 validation score : 0.2327 log-loss : 2.59603\n",
      "training using chunk : 251 validation score : 0.1944 log-loss : 2.64939\n",
      "training using chunk : 252 validation score : 0.2397 log-loss : 2.59053\n",
      "training using chunk : 253 validation score : 0.2290 log-loss : 2.60423\n",
      "training using chunk : 254 validation score : 0.2445 log-loss : 2.58031\n",
      "training using chunk : 255 validation score : 0.2433 log-loss : 2.58003\n",
      "training using chunk : 256 validation score : 0.2355 log-loss : 2.59940\n",
      "training using chunk : 257 validation score : 0.2388 log-loss : 2.57895\n",
      "training using chunk : 258 validation score : 0.2421 log-loss : 2.56648\n",
      "training using chunk : 259 validation score : 0.2161 log-loss : 2.60318\n",
      "training using chunk : 260 validation score : 0.2314 log-loss : 2.61665\n",
      "training using chunk : 261 validation score : 0.2272 log-loss : 2.57294\n",
      "training using chunk : 262 validation score : 0.2368 log-loss : 2.58858\n",
      "training using chunk : 263 validation score : 0.2436 log-loss : 2.58080\n",
      "training using chunk : 264 validation score : 0.2147 log-loss : 2.60344\n",
      "training using chunk : 265 validation score : 0.2072 log-loss : 2.63237\n",
      "training using chunk : 266 validation score : 0.2340 log-loss : 2.58077\n",
      "training using chunk : 267 validation score : 0.2331 log-loss : 2.59994\n",
      "training using chunk : 268 validation score : 0.2294 log-loss : 2.60062\n",
      "training using chunk : 269 validation score : 0.2318 log-loss : 2.58955\n",
      "training using chunk : 270 validation score : 0.2342 log-loss : 2.58560\n",
      "training using chunk : 271 validation score : 0.2417 log-loss : 2.58926\n",
      "training using chunk : 272 validation score : 0.2419 log-loss : 2.57118\n",
      "training using chunk : 273 validation score : 0.2390 log-loss : 2.58504\n",
      "training using chunk : 274 validation score : 0.2406 log-loss : 2.56202\n",
      "training using chunk : 275 validation score : 0.2438 log-loss : 2.59545\n",
      "training using chunk : 276 validation score : 0.2358 log-loss : 2.58502\n",
      "training using chunk : 277 validation score : 0.2363 log-loss : 2.56714\n",
      "training using chunk : 278 validation score : 0.2449 log-loss : 2.55319\n",
      "training using chunk : 279 validation score : 0.2376 log-loss : 2.57997\n",
      "training using chunk : 280 validation score : 0.2332 log-loss : 2.58222\n",
      "training using chunk : 281 validation score : 0.2460 log-loss : 2.57182\n",
      "training using chunk : 282 validation score : 0.2373 log-loss : 2.57467\n",
      "training using chunk : 283 validation score : 0.2092 log-loss : 2.61255\n",
      "training using chunk : 284 validation score : 0.2294 log-loss : 2.56703\n",
      "training using chunk : 285 validation score : 0.2440 log-loss : 2.60670\n",
      "training using chunk : 286 validation score : 0.2481 log-loss : 2.56704\n",
      "training using chunk : 287 validation score : 0.2278 log-loss : 2.56156\n",
      "training using chunk : 288 validation score : 0.2325 log-loss : 2.58182\n",
      "training using chunk : 289 validation score : 0.2227 log-loss : 2.60873\n",
      "training using chunk : 290 validation score : 0.2025 log-loss : 2.63660\n",
      "training using chunk : 291 validation score : 0.2320 log-loss : 2.57309\n",
      "training using chunk : 292 validation score : 0.2424 log-loss : 2.55119\n",
      "training using chunk : 293 validation score : 0.2411 log-loss : 2.57844\n",
      "training using chunk : 294 validation score : 0.2398 log-loss : 2.57679\n",
      "training using chunk : 295 validation score : 0.2289 log-loss : 2.55684\n",
      "training using chunk : 296 validation score : 0.2316 log-loss : 2.59441\n",
      "training using chunk : 297 validation score : 0.2345 log-loss : 2.56642\n",
      "training using chunk : 298 validation score : 0.2351 log-loss : 2.56823\n",
      "training using chunk : 299 validation score : 0.2417 log-loss : 2.55450\n",
      "training using chunk : 300 validation score : 0.2330 log-loss : 2.55154\n",
      "training using chunk : 301 validation score : 0.2460 log-loss : 2.56409\n",
      "training using chunk : 302 validation score : 0.2313 log-loss : 2.55790\n",
      "training using chunk : 303 validation score : 0.2358 log-loss : 2.57137\n",
      "training using chunk : 304 validation score : 0.2348 log-loss : 2.56848\n",
      "training using chunk : 305 validation score : 0.2410 log-loss : 2.55817\n",
      "training using chunk : 306 validation score : 0.2413 log-loss : 2.55727\n",
      "training using chunk : 307 validation score : 0.2413 log-loss : 2.56544\n",
      "training using chunk : 308 validation score : 0.2396 log-loss : 2.53829\n",
      "training using chunk : 309 validation score : 0.2317 log-loss : 2.55918\n",
      "training using chunk : 310 validation score : 0.2175 log-loss : 2.60507\n",
      "training using chunk : 311 validation score : 0.2501 log-loss : 2.54085\n",
      "training using chunk : 312 validation score : 0.2489 log-loss : 2.54577\n",
      "training using chunk : 313 validation score : 0.2416 log-loss : 2.54682\n",
      "training using chunk : 314 validation score : 0.2410 log-loss : 2.56447\n",
      "training using chunk : 315 validation score : 0.2403 log-loss : 2.53431\n",
      "training using chunk : 316 validation score : 0.2286 log-loss : 2.55641\n",
      "training using chunk : 317 validation score : 0.2282 log-loss : 2.56937\n",
      "training using chunk : 318 validation score : 0.2300 log-loss : 2.57385\n",
      "training using chunk : 319 validation score : 0.2387 log-loss : 2.55171\n",
      "training using chunk : 320 validation score : 0.2386 log-loss : 2.55088\n",
      "training using chunk : 321 validation score : 0.2222 log-loss : 2.56437\n",
      "training using chunk : 322 validation score : 0.2317 log-loss : 2.59005\n",
      "training using chunk : 323 validation score : 0.2340 log-loss : 2.60885\n",
      "training using chunk : 324 validation score : 0.2509 log-loss : 2.56080\n",
      "training using chunk : 325 validation score : 0.2402 log-loss : 2.56007\n",
      "training using chunk : 326 validation score : 0.2421 log-loss : 2.54983\n",
      "training using chunk : 327 validation score : 0.2417 log-loss : 2.57331\n",
      "training using chunk : 328 validation score : 0.2294 log-loss : 2.55456\n",
      "training using chunk : 329 validation score : 0.2417 log-loss : 2.57719\n",
      "training using chunk : 330 validation score : 0.2475 log-loss : 2.55643\n",
      "training using chunk : 331 validation score : 0.2518 log-loss : 2.56033\n",
      "training using chunk : 332 validation score : 0.2460 log-loss : 2.56530\n",
      "training using chunk : 333 validation score : 0.2314 log-loss : 2.56595\n",
      "training using chunk : 334 validation score : 0.2516 log-loss : 2.55639\n",
      "training using chunk : 335 validation score : 0.2444 log-loss : 2.55516\n",
      "training using chunk : 336 validation score : 0.2126 log-loss : 2.59998\n",
      "training using chunk : 337 validation score : 0.2434 log-loss : 2.55115\n",
      "training using chunk : 338 validation score : 0.2329 log-loss : 2.54126\n",
      "training using chunk : 339 validation score : 0.2380 log-loss : 2.56138\n",
      "training using chunk : 340 validation score : 0.2421 log-loss : 2.52282\n",
      "training using chunk : 341 validation score : 0.2357 log-loss : 2.55265\n",
      "training using chunk : 342 validation score : 0.2332 log-loss : 2.54273\n",
      "training using chunk : 343 validation score : 0.2429 log-loss : 2.54728\n",
      "training using chunk : 344 validation score : 0.2296 log-loss : 2.54452\n",
      "training using chunk : 345 validation score : 0.2370 log-loss : 2.54619\n",
      "training using chunk : 346 validation score : 0.2188 log-loss : 2.57325\n",
      "training using chunk : 347 validation score : 0.2402 log-loss : 2.53513\n",
      "training using chunk : 348 validation score : 0.2494 log-loss : 2.54782\n",
      "training using chunk : 349 validation score : 0.2131 log-loss : 2.57014\n",
      "training using chunk : 350 validation score : 0.2393 log-loss : 2.53482\n",
      "training using chunk : 351 validation score : 0.2483 log-loss : 2.55195\n",
      "training using chunk : 352 validation score : 0.2450 log-loss : 2.53564\n",
      "training using chunk : 353 validation score : 0.2354 log-loss : 2.53200\n",
      "training using chunk : 354 validation score : 0.2355 log-loss : 2.54973\n",
      "training using chunk : 355 validation score : 0.2469 log-loss : 2.52667\n",
      "training using chunk : 356 validation score : 0.2379 log-loss : 2.54127\n",
      "training using chunk : 357 validation score : 0.2458 log-loss : 2.53090\n",
      "training using chunk : 358 validation score : 0.2504 log-loss : 2.56425\n",
      "training using chunk : 359 validation score : 0.2497 log-loss : 2.53126\n",
      "training using chunk : 360 validation score : 0.2296 log-loss : 2.54846\n",
      "training using chunk : 361 validation score : 0.2447 log-loss : 2.54482\n",
      "training using chunk : 362 validation score : 0.2316 log-loss : 2.55465\n",
      "training using chunk : 363 validation score : 0.2433 log-loss : 2.55117\n",
      "training using chunk : 364 validation score : 0.2538 log-loss : 2.53144\n",
      "training using chunk : 365 validation score : 0.2048 log-loss : 2.60099\n",
      "training using chunk : 366 validation score : 0.2392 log-loss : 2.53274\n",
      "training using chunk : 367 validation score : 0.2457 log-loss : 2.53750\n",
      "training using chunk : 368 validation score : 0.2456 log-loss : 2.55199\n",
      "training using chunk : 369 validation score : 0.2400 log-loss : 2.53885\n",
      "training using chunk : 370 validation score : 0.2233 log-loss : 2.55968\n",
      "training using chunk : 371 validation score : 0.2440 log-loss : 2.52124\n",
      "training using chunk : 372 validation score : 0.2504 log-loss : 2.52822\n",
      "training using chunk : 373 validation score : 0.2529 log-loss : 2.55458\n",
      "training using chunk : 374 validation score : 0.2422 log-loss : 2.53836\n",
      "training using chunk : 375 validation score : 0.2434 log-loss : 2.52672\n",
      "training using chunk : 376 validation score : 0.2252 log-loss : 2.55738\n",
      "training using chunk : 377 validation score : 0.2489 log-loss : 2.53388\n",
      "training using chunk : 378 validation score : 0.2370 log-loss : 2.52803\n",
      "training using chunk : 379 validation score : 0.2506 log-loss : 2.52978\n",
      "training using chunk : 380 validation score : 0.2328 log-loss : 2.55629\n",
      "training using chunk : 381 validation score : 0.2390 log-loss : 2.55923\n",
      "training using chunk : 382 validation score : 0.2497 log-loss : 2.51865\n",
      "training using chunk : 383 validation score : 0.2446 log-loss : 2.54263\n",
      "training using chunk : 384 validation score : 0.2481 log-loss : 2.53211\n",
      "training using chunk : 385 validation score : 0.2472 log-loss : 2.51309\n",
      "training using chunk : 386 validation score : 0.2285 log-loss : 2.54388\n",
      "training using chunk : 387 validation score : 0.2449 log-loss : 2.53445\n",
      "training using chunk : 388 validation score : 0.2412 log-loss : 2.53827\n",
      "training using chunk : 389 validation score : 0.2408 log-loss : 2.53664\n",
      "training using chunk : 390 validation score : 0.2492 log-loss : 2.54677\n",
      "training using chunk : 391 validation score : 0.2460 log-loss : 2.53512\n",
      "training using chunk : 392 validation score : 0.2442 log-loss : 2.54407\n",
      "training using chunk : 393 validation score : 0.2469 log-loss : 2.52629\n",
      "training using chunk : 394 validation score : 0.2415 log-loss : 2.53387\n",
      "training using chunk : 395 validation score : 0.2422 log-loss : 2.52069\n",
      "training using chunk : 396 validation score : 0.2385 log-loss : 2.52081\n",
      "training using chunk : 397 validation score : 0.2496 log-loss : 2.52340\n",
      "training using chunk : 398 validation score : 0.2424 log-loss : 2.52431\n",
      "training using chunk : 399 validation score : 0.2503 log-loss : 2.52379\n",
      "training using chunk : 400 validation score : 0.2489 log-loss : 2.54497\n",
      "training using chunk : 401 validation score : 0.2469 log-loss : 2.53023\n",
      "training using chunk : 402 validation score : 0.2499 log-loss : 2.53859\n",
      "training using chunk : 403 validation score : 0.2384 log-loss : 2.52908\n",
      "training using chunk : 404 validation score : 0.2450 log-loss : 2.52382\n",
      "training using chunk : 405 validation score : 0.2506 log-loss : 2.50450\n",
      "training using chunk : 406 validation score : 0.2486 log-loss : 2.51372\n",
      "training using chunk : 407 validation score : 0.2452 log-loss : 2.51558\n",
      "training using chunk : 408 validation score : 0.2480 log-loss : 2.52797\n",
      "training using chunk : 409 validation score : 0.2502 log-loss : 2.53504\n",
      "training using chunk : 410 validation score : 0.2431 log-loss : 2.52283\n",
      "training using chunk : 411 validation score : 0.2531 log-loss : 2.51307\n",
      "training using chunk : 412 validation score : 0.2473 log-loss : 2.53316\n",
      "training using chunk : 413 validation score : 0.2391 log-loss : 2.52149\n",
      "training using chunk : 414 validation score : 0.2409 log-loss : 2.52153\n",
      "training using chunk : 415 validation score : 0.2422 log-loss : 2.52885\n",
      "training using chunk : 416 validation score : 0.2559 log-loss : 2.51888\n",
      "training using chunk : 417 validation score : 0.2481 log-loss : 2.55111\n",
      "training using chunk : 418 validation score : 0.2494 log-loss : 2.52970\n",
      "training using chunk : 419 validation score : 0.2355 log-loss : 2.52275\n",
      "training using chunk : 420 validation score : 0.2461 log-loss : 2.52051\n",
      "training using chunk : 421 validation score : 0.2472 log-loss : 2.53724\n",
      "training using chunk : 422 validation score : 0.2508 log-loss : 2.52424\n",
      "training using chunk : 423 validation score : 0.2430 log-loss : 2.52391\n",
      "training using chunk : 424 validation score : 0.2407 log-loss : 2.53015\n",
      "training using chunk : 425 validation score : 0.2441 log-loss : 2.52654\n",
      "training using chunk : 426 validation score : 0.2401 log-loss : 2.53067\n",
      "training using chunk : 427 validation score : 0.2500 log-loss : 2.53288\n",
      "training using chunk : 428 validation score : 0.2445 log-loss : 2.52086\n",
      "training using chunk : 429 validation score : 0.2294 log-loss : 2.54031\n",
      "training using chunk : 430 validation score : 0.2508 log-loss : 2.49806\n",
      "training using chunk : 431 validation score : 0.2394 log-loss : 2.53627\n",
      "training using chunk : 432 validation score : 0.2501 log-loss : 2.51003\n",
      "training using chunk : 433 validation score : 0.2310 log-loss : 2.53689\n",
      "training using chunk : 434 validation score : 0.2508 log-loss : 2.50812\n",
      "training using chunk : 435 validation score : 0.2481 log-loss : 2.50668\n",
      "training using chunk : 436 validation score : 0.2426 log-loss : 2.51911\n",
      "training using chunk : 437 validation score : 0.2133 log-loss : 2.54974\n",
      "training using chunk : 438 validation score : 0.2491 log-loss : 2.51514\n",
      "training using chunk : 439 validation score : 0.2362 log-loss : 2.52891\n",
      "training using chunk : 440 validation score : 0.2456 log-loss : 2.51762\n",
      "training using chunk : 441 validation score : 0.2510 log-loss : 2.53657\n",
      "training using chunk : 442 validation score : 0.2538 log-loss : 2.52297\n",
      "training using chunk : 443 validation score : 0.2464 log-loss : 2.51721\n",
      "training using chunk : 444 validation score : 0.2463 log-loss : 2.51151\n",
      "training using chunk : 445 validation score : 0.2401 log-loss : 2.54087\n",
      "training using chunk : 446 validation score : 0.2508 log-loss : 2.50635\n",
      "training using chunk : 447 validation score : 0.2286 log-loss : 2.54731\n",
      "training using chunk : 448 validation score : 0.2400 log-loss : 2.52921\n",
      "training using chunk : 449 validation score : 0.2506 log-loss : 2.50721\n",
      "training using chunk : 450 validation score : 0.2451 log-loss : 2.52527\n",
      "training using chunk : 451 validation score : 0.2415 log-loss : 2.51932\n",
      "training using chunk : 452 validation score : 0.2524 log-loss : 2.51812\n",
      "training using chunk : 453 validation score : 0.2483 log-loss : 2.50819\n",
      "training using chunk : 454 validation score : 0.2198 log-loss : 2.54441\n",
      "training using chunk : 455 validation score : 0.2451 log-loss : 2.52248\n",
      "training using chunk : 456 validation score : 0.2464 log-loss : 2.53201\n",
      "training using chunk : 457 validation score : 0.2505 log-loss : 2.52506\n",
      "training using chunk : 458 validation score : 0.2424 log-loss : 2.52150\n",
      "training using chunk : 459 validation score : 0.2520 log-loss : 2.50273\n",
      "training using chunk : 460 validation score : 0.2292 log-loss : 2.53538\n",
      "training using chunk : 461 validation score : 0.2434 log-loss : 2.50656\n",
      "training using chunk : 462 validation score : 0.2439 log-loss : 2.51202\n",
      "training using chunk : 463 validation score : 0.2462 log-loss : 2.55707\n",
      "training using chunk : 464 validation score : 0.2477 log-loss : 2.51107\n",
      "training using chunk : 465 validation score : 0.2353 log-loss : 2.51409\n",
      "training using chunk : 466 validation score : 0.2436 log-loss : 2.50183\n",
      "training using chunk : 467 validation score : 0.2536 log-loss : 2.50926\n",
      "training using chunk : 468 validation score : 0.2512 log-loss : 2.50771\n",
      "training using chunk : 469 validation score : 0.2531 log-loss : 2.51099\n",
      "training using chunk : 470 validation score : 0.2413 log-loss : 2.50948\n",
      "training using chunk : 471 validation score : 0.2348 log-loss : 2.52408\n",
      "training using chunk : 472 validation score : 0.2397 log-loss : 2.50501\n",
      "training using chunk : 473 validation score : 0.2478 log-loss : 2.51465\n",
      "training using chunk : 474 validation score : 0.2360 log-loss : 2.51125\n",
      "training using chunk : 475 validation score : 0.2493 log-loss : 2.50204\n",
      "training using chunk : 476 validation score : 0.2497 log-loss : 2.53898\n",
      "training using chunk : 477 validation score : 0.2453 log-loss : 2.50109\n",
      "training using chunk : 478 validation score : 0.2539 log-loss : 2.50839\n",
      "training using chunk : 479 validation score : 0.2457 log-loss : 2.50067\n",
      "training using chunk : 480 validation score : 0.2498 log-loss : 2.51868\n",
      "training using chunk : 481 validation score : 0.2404 log-loss : 2.51380\n",
      "training using chunk : 482 validation score : 0.2451 log-loss : 2.51356\n",
      "training using chunk : 483 validation score : 0.2382 log-loss : 2.51218\n",
      "training using chunk : 484 validation score : 0.2470 log-loss : 2.51355\n",
      "training using chunk : 485 validation score : 0.2359 log-loss : 2.52562\n",
      "training using chunk : 486 validation score : 0.2525 log-loss : 2.51049\n",
      "training using chunk : 487 validation score : 0.2405 log-loss : 2.51611\n",
      "training using chunk : 488 validation score : 0.2408 log-loss : 2.54788\n",
      "training using chunk : 489 validation score : 0.2480 log-loss : 2.52837\n",
      "training using chunk : 490 validation score : 0.2505 log-loss : 2.52190\n",
      "training using chunk : 491 validation score : 0.2389 log-loss : 2.52330\n",
      "training using chunk : 492 validation score : 0.2450 log-loss : 2.51089\n",
      "training using chunk : 493 validation score : 0.2524 log-loss : 2.50470\n",
      "training using chunk : 494 validation score : 0.2475 log-loss : 2.52065\n",
      "training using chunk : 495 validation score : 0.2520 log-loss : 2.50958\n",
      "training using chunk : 496 validation score : 0.2529 log-loss : 2.50208\n",
      "training using chunk : 497 validation score : 0.2449 log-loss : 2.52309\n",
      "training using chunk : 498 validation score : 0.2439 log-loss : 2.50491\n",
      "training using chunk : 499 validation score : 0.2377 log-loss : 2.50859\n",
      "training using chunk : 500 validation score : 0.2539 log-loss : 2.49943\n",
      "training using chunk : 501 validation score : 0.2490 log-loss : 2.50875\n",
      "training using chunk : 502 validation score : 0.2380 log-loss : 2.51741\n",
      "training using chunk : 503 validation score : 0.2334 log-loss : 2.51605\n",
      "training using chunk : 504 validation score : 0.2560 log-loss : 2.51006\n",
      "training using chunk : 505 validation score : 0.2545 log-loss : 2.50001\n",
      "training using chunk : 506 validation score : 0.2349 log-loss : 2.51383\n",
      "training using chunk : 507 validation score : 0.2512 log-loss : 2.49490\n",
      "training using chunk : 508 validation score : 0.2525 log-loss : 2.51402\n",
      "training using chunk : 509 validation score : 0.2472 log-loss : 2.50395\n",
      "training using chunk : 510 validation score : 0.2382 log-loss : 2.51162\n",
      "training using chunk : 511 validation score : 0.2517 log-loss : 2.50645\n",
      "training using chunk : 512 validation score : 0.2511 log-loss : 2.51999\n",
      "training using chunk : 513 validation score : 0.2523 log-loss : 2.51059\n",
      "training using chunk : 514 validation score : 0.2497 log-loss : 2.51570\n",
      "training using chunk : 515 validation score : 0.2552 log-loss : 2.49072\n",
      "training using chunk : 516 validation score : 0.2500 log-loss : 2.50608\n",
      "training using chunk : 517 validation score : 0.2533 log-loss : 2.48932\n",
      "training using chunk : 518 validation score : 0.2455 log-loss : 2.49577\n",
      "training using chunk : 519 validation score : 0.2492 log-loss : 2.52373\n",
      "training using chunk : 520 validation score : 0.2573 log-loss : 2.48852\n",
      "training using chunk : 521 validation score : 0.2417 log-loss : 2.51300\n",
      "training using chunk : 522 validation score : 0.2563 log-loss : 2.49682\n",
      "training using chunk : 523 validation score : 0.2509 log-loss : 2.50632\n",
      "training using chunk : 524 validation score : 0.2532 log-loss : 2.49219\n",
      "training using chunk : 525 validation score : 0.2532 log-loss : 2.50198\n",
      "training using chunk : 526 validation score : 0.2435 log-loss : 2.50491\n",
      "training using chunk : 527 validation score : 0.2497 log-loss : 2.50038\n",
      "training using chunk : 528 validation score : 0.2441 log-loss : 2.50654\n",
      "training using chunk : 529 validation score : 0.2509 log-loss : 2.51074\n",
      "training using chunk : 530 validation score : 0.2522 log-loss : 2.50992\n",
      "training using chunk : 531 validation score : 0.2489 log-loss : 2.50391\n",
      "training using chunk : 532 validation score : 0.2495 log-loss : 2.50637\n",
      "training using chunk : 533 validation score : 0.2446 log-loss : 2.50638\n",
      "training using chunk : 534 validation score : 0.2532 log-loss : 2.50065\n",
      "training using chunk : 535 validation score : 0.2396 log-loss : 2.51794\n",
      "training using chunk : 536 validation score : 0.2554 log-loss : 2.49896\n",
      "training using chunk : 537 validation score : 0.2524 log-loss : 2.50743\n",
      "training using chunk : 538 validation score : 0.2505 log-loss : 2.49991\n",
      "training using chunk : 539 validation score : 0.2512 log-loss : 2.50896\n",
      "training using chunk : 540 validation score : 0.2531 log-loss : 2.50546\n",
      "training using chunk : 541 validation score : 0.2504 log-loss : 2.51039\n",
      "training using chunk : 542 validation score : 0.2560 log-loss : 2.50529\n",
      "training using chunk : 543 validation score : 0.2449 log-loss : 2.50602\n",
      "training using chunk : 544 validation score : 0.2444 log-loss : 2.49687\n",
      "training using chunk : 545 validation score : 0.2267 log-loss : 2.52720\n",
      "training using chunk : 546 validation score : 0.2567 log-loss : 2.49830\n",
      "training using chunk : 547 validation score : 0.2509 log-loss : 2.49367\n",
      "training using chunk : 548 validation score : 0.2461 log-loss : 2.49532\n",
      "training using chunk : 549 validation score : 0.2554 log-loss : 2.50441\n",
      "training using chunk : 550 validation score : 0.2520 log-loss : 2.49200\n",
      "training using chunk : 551 validation score : 0.2531 log-loss : 2.50149\n",
      "training using chunk : 552 validation score : 0.2516 log-loss : 2.49586\n",
      "training using chunk : 553 validation score : 0.2495 log-loss : 2.49964\n",
      "training using chunk : 554 validation score : 0.2457 log-loss : 2.51104\n",
      "training using chunk : 555 validation score : 0.2539 log-loss : 2.49776\n",
      "training using chunk : 556 validation score : 0.2381 log-loss : 2.52390\n",
      "training using chunk : 557 validation score : 0.2390 log-loss : 2.51031\n",
      "training using chunk : 558 validation score : 0.2545 log-loss : 2.51055\n",
      "training using chunk : 559 validation score : 0.2476 log-loss : 2.50359\n",
      "training using chunk : 560 validation score : 0.2488 log-loss : 2.49219\n",
      "training using chunk : 561 validation score : 0.2426 log-loss : 2.51180\n",
      "training using chunk : 562 validation score : 0.2540 log-loss : 2.49735\n",
      "training using chunk : 563 validation score : 0.2545 log-loss : 2.49921\n",
      "training using chunk : 564 validation score : 0.2375 log-loss : 2.51442\n",
      "training using chunk : 565 validation score : 0.2551 log-loss : 2.50535\n",
      "training using chunk : 566 validation score : 0.2527 log-loss : 2.49219\n",
      "training using chunk : 567 validation score : 0.2462 log-loss : 2.49478\n",
      "training using chunk : 568 validation score : 0.2404 log-loss : 2.52866\n",
      "training using chunk : 569 validation score : 0.2454 log-loss : 2.51241\n",
      "training using chunk : 570 validation score : 0.2501 log-loss : 2.51037\n",
      "training using chunk : 571 validation score : 0.2478 log-loss : 2.52654\n",
      "training using chunk : 572 validation score : 0.2525 log-loss : 2.49820\n",
      "training using chunk : 573 validation score : 0.2564 log-loss : 2.48773\n",
      "training using chunk : 574 validation score : 0.2548 log-loss : 2.49424\n",
      "training using chunk : 575 validation score : 0.2512 log-loss : 2.52052\n",
      "training using chunk : 576 validation score : 0.2392 log-loss : 2.50567\n",
      "training using chunk : 577 validation score : 0.2552 log-loss : 2.49321\n",
      "training using chunk : 578 validation score : 0.2481 log-loss : 2.49431\n",
      "training using chunk : 579 validation score : 0.2482 log-loss : 2.49294\n",
      "training using chunk : 580 validation score : 0.2579 log-loss : 2.49014\n",
      "training using chunk : 581 validation score : 0.2406 log-loss : 2.50278\n",
      "training using chunk : 582 validation score : 0.2584 log-loss : 2.50132\n",
      "training using chunk : 583 validation score : 0.2599 log-loss : 2.48550\n",
      "training using chunk : 584 validation score : 0.2478 log-loss : 2.48766\n",
      "training using chunk : 585 validation score : 0.2505 log-loss : 2.50451\n",
      "training using chunk : 586 validation score : 0.2468 log-loss : 2.51227\n",
      "training using chunk : 587 validation score : 0.2496 log-loss : 2.51137\n",
      "training using chunk : 588 validation score : 0.2478 log-loss : 2.49175\n",
      "training using chunk : 589 validation score : 0.2520 log-loss : 2.49791\n",
      "training using chunk : 590 validation score : 0.2572 log-loss : 2.48414\n",
      "training using chunk : 591 validation score : 0.2498 log-loss : 2.49076\n",
      "training using chunk : 592 validation score : 0.2534 log-loss : 2.49502\n",
      "training using chunk : 593 validation score : 0.2522 log-loss : 2.50963\n",
      "training using chunk : 594 validation score : 0.2509 log-loss : 2.49288\n",
      "training using chunk : 595 validation score : 0.2535 log-loss : 2.49338\n",
      "training using chunk : 596 validation score : 0.2504 log-loss : 2.50603\n",
      "training using chunk : 597 validation score : 0.2556 log-loss : 2.50021\n",
      "training using chunk : 598 validation score : 0.2532 log-loss : 2.49662\n",
      "training using chunk : 599 validation score : 0.2603 log-loss : 2.48415\n",
      "training using chunk : 600 validation score : 0.2536 log-loss : 2.51806\n",
      "training using chunk : 601 validation score : 0.2557 log-loss : 2.49218\n",
      "training using chunk : 602 validation score : 0.2578 log-loss : 2.50383\n",
      "training using chunk : 603 validation score : 0.2570 log-loss : 2.49452\n",
      "training using chunk : 604 validation score : 0.2557 log-loss : 2.48684\n",
      "training using chunk : 605 validation score : 0.2433 log-loss : 2.50847\n",
      "training using chunk : 606 validation score : 0.2508 log-loss : 2.50334\n",
      "training using chunk : 607 validation score : 0.2566 log-loss : 2.48642\n",
      "training using chunk : 608 validation score : 0.2567 log-loss : 2.48979\n",
      "training using chunk : 609 validation score : 0.2473 log-loss : 2.50908\n",
      "training using chunk : 610 validation score : 0.2563 log-loss : 2.48330\n",
      "training using chunk : 611 validation score : 0.2571 log-loss : 2.49235\n",
      "training using chunk : 612 validation score : 0.2527 log-loss : 2.48680\n",
      "training using chunk : 613 validation score : 0.2337 log-loss : 2.51095\n",
      "training using chunk : 614 validation score : 0.2537 log-loss : 2.48550\n",
      "training using chunk : 615 validation score : 0.2561 log-loss : 2.48987\n",
      "training using chunk : 616 validation score : 0.2518 log-loss : 2.49515\n",
      "training using chunk : 617 validation score : 0.2610 log-loss : 2.49137\n",
      "training using chunk : 618 validation score : 0.2546 log-loss : 2.48474\n",
      "training using chunk : 619 validation score : 0.2597 log-loss : 2.49251\n",
      "training using chunk : 620 validation score : 0.2598 log-loss : 2.48925\n",
      "training using chunk : 621 validation score : 0.2542 log-loss : 2.48535\n",
      "training using chunk : 622 validation score : 0.2535 log-loss : 2.48088\n",
      "training using chunk : 623 validation score : 0.2523 log-loss : 2.48865\n",
      "training using chunk : 624 validation score : 0.2511 log-loss : 2.50836\n",
      "training using chunk : 625 validation score : 0.2558 log-loss : 2.50458\n",
      "training using chunk : 626 validation score : 0.2538 log-loss : 2.48934\n",
      "training using chunk : 627 validation score : 0.2574 log-loss : 2.48553\n",
      "training using chunk : 628 validation score : 0.2536 log-loss : 2.49177\n",
      "training using chunk : 629 validation score : 0.2582 log-loss : 2.48134\n",
      "training using chunk : 630 validation score : 0.2488 log-loss : 2.50050\n",
      "training using chunk : 631 validation score : 0.2586 log-loss : 2.49074\n",
      "training using chunk : 632 validation score : 0.2580 log-loss : 2.49726\n",
      "training using chunk : 633 validation score : 0.2428 log-loss : 2.50469\n",
      "training using chunk : 634 validation score : 0.2490 log-loss : 2.49079\n",
      "training using chunk : 635 validation score : 0.2558 log-loss : 2.48049\n",
      "training using chunk : 636 validation score : 0.2516 log-loss : 2.50267\n",
      "training using chunk : 637 validation score : 0.2536 log-loss : 2.49453\n",
      "training using chunk : 638 validation score : 0.2590 log-loss : 2.49616\n",
      "training using chunk : 639 validation score : 0.2501 log-loss : 2.49156\n",
      "training using chunk : 640 validation score : 0.2514 log-loss : 2.50002\n",
      "training using chunk : 641 validation score : 0.2590 log-loss : 2.48859\n",
      "training using chunk : 642 validation score : 0.2539 log-loss : 2.49479\n",
      "training using chunk : 643 validation score : 0.2526 log-loss : 2.48745\n",
      "training using chunk : 644 validation score : 0.2570 log-loss : 2.48350\n",
      "training using chunk : 645 validation score : 0.2382 log-loss : 2.50849\n",
      "training using chunk : 646 validation score : 0.2449 log-loss : 2.50328\n",
      "training using chunk : 647 validation score : 0.2482 log-loss : 2.49311\n",
      "training using chunk : 648 validation score : 0.2584 log-loss : 2.49894\n",
      "training using chunk : 649 validation score : 0.2504 log-loss : 2.48520\n",
      "training using chunk : 650 validation score : 0.2504 log-loss : 2.48683\n",
      "training using chunk : 651 validation score : 0.2528 log-loss : 2.47991\n",
      "training using chunk : 652 validation score : 0.2557 log-loss : 2.48616\n",
      "training using chunk : 653 validation score : 0.2575 log-loss : 2.48644\n",
      "training using chunk : 654 validation score : 0.2362 log-loss : 2.50954\n",
      "training using chunk : 655 validation score : 0.2581 log-loss : 2.48572\n",
      "training using chunk : 656 validation score : 0.2567 log-loss : 2.48319\n",
      "training using chunk : 657 validation score : 0.2390 log-loss : 2.52474\n",
      "training using chunk : 658 validation score : 0.2480 log-loss : 2.49670\n",
      "training using chunk : 659 validation score : 0.2582 log-loss : 2.48158\n",
      "training using chunk : 660 validation score : 0.2574 log-loss : 2.48501\n",
      "training using chunk : 661 validation score : 0.2534 log-loss : 2.49247\n",
      "training using chunk : 662 validation score : 0.2548 log-loss : 2.48133\n",
      "training using chunk : 663 validation score : 0.2595 log-loss : 2.48132\n",
      "training using chunk : 664 validation score : 0.2555 log-loss : 2.48319\n",
      "training using chunk : 665 validation score : 0.2573 log-loss : 2.49002\n",
      "training using chunk : 666 validation score : 0.2387 log-loss : 2.50623\n",
      "training using chunk : 667 validation score : 0.2495 log-loss : 2.49494\n",
      "training using chunk : 668 validation score : 0.2527 log-loss : 2.48201\n",
      "training using chunk : 669 validation score : 0.2564 log-loss : 2.48552\n",
      "training using chunk : 670 validation score : 0.2590 log-loss : 2.48116\n",
      "training using chunk : 671 validation score : 0.2415 log-loss : 2.49184\n",
      "training using chunk : 672 validation score : 0.2577 log-loss : 2.48078\n",
      "training using chunk : 673 validation score : 0.2448 log-loss : 2.49042\n",
      "training using chunk : 674 validation score : 0.2473 log-loss : 2.50522\n",
      "training using chunk : 675 validation score : 0.2559 log-loss : 2.48328\n",
      "training using chunk : 676 validation score : 0.2524 log-loss : 2.48705\n",
      "training using chunk : 677 validation score : 0.2432 log-loss : 2.49911\n",
      "training using chunk : 678 validation score : 0.2528 log-loss : 2.48126\n",
      "training using chunk : 679 validation score : 0.2561 log-loss : 2.49239\n",
      "training using chunk : 680 validation score : 0.2507 log-loss : 2.48675\n",
      "training using chunk : 681 validation score : 0.2500 log-loss : 2.48087\n",
      "training using chunk : 682 validation score : 0.2565 log-loss : 2.48106\n",
      "training using chunk : 683 validation score : 0.2554 log-loss : 2.48061\n",
      "training using chunk : 684 validation score : 0.2540 log-loss : 2.49437\n",
      "training using chunk : 685 validation score : 0.2563 log-loss : 2.48480\n",
      "training using chunk : 686 validation score : 0.2567 log-loss : 2.49439\n",
      "training using chunk : 687 validation score : 0.2450 log-loss : 2.49547\n",
      "training using chunk : 688 validation score : 0.2458 log-loss : 2.49693\n",
      "training using chunk : 689 validation score : 0.2511 log-loss : 2.48619\n",
      "training using chunk : 690 validation score : 0.2563 log-loss : 2.47316\n",
      "training using chunk : 691 validation score : 0.2443 log-loss : 2.49010\n",
      "training using chunk : 692 validation score : 0.2575 log-loss : 2.47790\n",
      "training using chunk : 693 validation score : 0.2553 log-loss : 2.48798\n",
      "training using chunk : 694 validation score : 0.2548 log-loss : 2.47737\n",
      "training using chunk : 695 validation score : 0.2552 log-loss : 2.47712\n",
      "training using chunk : 696 validation score : 0.2526 log-loss : 2.48616\n",
      "training using chunk : 697 validation score : 0.2604 log-loss : 2.47589\n",
      "training using chunk : 698 validation score : 0.2537 log-loss : 2.48608\n",
      "training using chunk : 699 validation score : 0.2562 log-loss : 2.48783\n",
      "training using chunk : 700 validation score : 0.2436 log-loss : 2.49287\n",
      "training using chunk : 701 validation score : 0.2538 log-loss : 2.48269\n",
      "training using chunk : 702 validation score : 0.2586 log-loss : 2.47819\n",
      "training using chunk : 703 validation score : 0.2588 log-loss : 2.48457\n",
      "Final log loss score : 2.4846 accuracy : 0.2588\n"
     ]
    }
   ],
   "source": [
    "model = train_model(train_data, train_labels.ravel())\n",
    "\n",
    "# round up the results\n",
    "probs = np.around(model.predict_proba(dev_data), decimals=5)\n",
    "print 'Final log loss score : {0:.4f} accuracy : {1:.4f}'.format(log_loss(dev_labels, probs), model.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf = LogisticRegression(C=100.0)\n",
    "#clf = GBC(n_estimators=10, max_depth=5,verbose=1)\n",
    "#clf = MultinomialNB()\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=100)\n",
    "#clf = SGDClassifier(fit_intercept=False, shuffle=True, n_jobs=-1,alpha=0.000005,n_iter = 50, loss='log', penalty ='l2')\n",
    "   \n",
    "#clf.fit(train_data, train_labels)\n",
    "#print clf.predict_proba(dev_data).shape\n",
    "# round up the results\n",
    "#probs = np.around(clf.predict_proba(dev_data), decimals=5)\n",
    "#print 'log loss score : {0:.4f} accuracy : {1:.4f}'.format(log_loss(dev_labels, probs), clf.score(dev_data, dev_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_data\n",
    "del train_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training using chunk : 1 validation score : 0.1130 log-loss : 26.15099\n",
      "training using chunk : 2 validation score : 0.1533 log-loss : 21.82010\n",
      "training using chunk : 3 validation score : 0.1379 log-loss : 23.44579\n",
      "training using chunk : 4 validation score : 0.0882 log-loss : 21.63701\n",
      "training using chunk : 5 validation score : 0.1496 log-loss : 20.35951\n",
      "training using chunk : 6 validation score : 0.0995 log-loss : 18.50244\n",
      "training using chunk : 7 validation score : 0.1725 log-loss : 13.00564\n",
      "training using chunk : 8 validation score : 0.1548 log-loss : 12.63484\n",
      "training using chunk : 9 validation score : 0.1388 log-loss : 12.17220\n",
      "training using chunk : 10 validation score : 0.1139 log-loss : 11.70642\n",
      "training using chunk : 11 validation score : 0.1586 log-loss : 9.29018\n",
      "training using chunk : 12 validation score : 0.1550 log-loss : 10.14502\n",
      "training using chunk : 13 validation score : 0.1116 log-loss : 9.31017\n",
      "training using chunk : 14 validation score : 0.1448 log-loss : 8.29448\n",
      "training using chunk : 15 validation score : 0.1772 log-loss : 7.54661\n",
      "training using chunk : 16 validation score : 0.1707 log-loss : 7.45229\n",
      "training using chunk : 17 validation score : 0.1747 log-loss : 6.89690\n",
      "training using chunk : 18 validation score : 0.1727 log-loss : 6.41037\n",
      "training using chunk : 19 validation score : 0.1282 log-loss : 7.01568\n",
      "training using chunk : 20 validation score : 0.1267 log-loss : 6.93514\n",
      "training using chunk : 21 validation score : 0.1747 log-loss : 5.68608\n",
      "training using chunk : 22 validation score : 0.1459 log-loss : 5.81850\n",
      "training using chunk : 23 validation score : 0.1274 log-loss : 5.97007\n",
      "training using chunk : 24 validation score : 0.1603 log-loss : 5.48232\n",
      "training using chunk : 25 validation score : 0.1829 log-loss : 4.85373\n",
      "training using chunk : 26 validation score : 0.1461 log-loss : 4.79488\n",
      "training using chunk : 27 validation score : 0.1995 log-loss : 4.77270\n",
      "training using chunk : 28 validation score : 0.1315 log-loss : 5.33019\n",
      "training using chunk : 29 validation score : 0.1748 log-loss : 4.51908\n",
      "training using chunk : 30 validation score : 0.1519 log-loss : 4.71535\n",
      "training using chunk : 31 validation score : 0.1743 log-loss : 5.01627\n",
      "training using chunk : 32 validation score : 0.1825 log-loss : 4.34047\n",
      "training using chunk : 33 validation score : 0.1481 log-loss : 4.53996\n",
      "training using chunk : 34 validation score : 0.1504 log-loss : 4.25948\n",
      "training using chunk : 35 validation score : 0.1392 log-loss : 4.35992\n",
      "training using chunk : 36 validation score : 0.1864 log-loss : 3.90133\n",
      "training using chunk : 37 validation score : 0.1719 log-loss : 3.99734\n",
      "training using chunk : 38 validation score : 0.1659 log-loss : 3.95237\n",
      "training using chunk : 39 validation score : 0.1708 log-loss : 3.75913\n",
      "training using chunk : 40 validation score : 0.1925 log-loss : 3.66539\n",
      "training using chunk : 41 validation score : 0.1104 log-loss : 4.12788\n",
      "training using chunk : 42 validation score : 0.1141 log-loss : 4.14606\n",
      "training using chunk : 43 validation score : 0.1507 log-loss : 3.85443\n",
      "training using chunk : 44 validation score : 0.1602 log-loss : 3.82935\n",
      "training using chunk : 45 validation score : 0.2131 log-loss : 3.84206\n",
      "training using chunk : 46 validation score : 0.1344 log-loss : 3.81052\n",
      "training using chunk : 47 validation score : 0.1888 log-loss : 3.49579\n",
      "training using chunk : 48 validation score : 0.2005 log-loss : 3.48967\n",
      "training using chunk : 49 validation score : 0.1705 log-loss : 3.52776\n",
      "training using chunk : 50 validation score : 0.2194 log-loss : 3.41920\n",
      "training using chunk : 51 validation score : 0.1548 log-loss : 3.47165\n",
      "training using chunk : 52 validation score : 0.1933 log-loss : 3.52786\n",
      "training using chunk : 53 validation score : 0.1846 log-loss : 3.53909\n",
      "training using chunk : 54 validation score : 0.1857 log-loss : 3.62539\n",
      "training using chunk : 55 validation score : 0.1574 log-loss : 3.44870\n",
      "training using chunk : 56 validation score : 0.1722 log-loss : 3.34125\n",
      "training using chunk : 57 validation score : 0.2048 log-loss : 3.34386\n",
      "training using chunk : 58 validation score : 0.1948 log-loss : 3.27214\n",
      "training using chunk : 59 validation score : 0.1884 log-loss : 3.25846\n",
      "training using chunk : 60 validation score : 0.1906 log-loss : 3.11964\n",
      "training using chunk : 61 validation score : 0.1899 log-loss : 3.23040\n",
      "training using chunk : 62 validation score : 0.1800 log-loss : 3.28117\n",
      "training using chunk : 63 validation score : 0.2084 log-loss : 3.13384\n",
      "training using chunk : 64 validation score : 0.1484 log-loss : 3.30295\n",
      "training using chunk : 65 validation score : 0.1581 log-loss : 3.16339\n",
      "training using chunk : 66 validation score : 0.2021 log-loss : 3.04317\n",
      "training using chunk : 67 validation score : 0.1867 log-loss : 3.09254\n",
      "training using chunk : 68 validation score : 0.2048 log-loss : 3.05443\n",
      "training using chunk : 69 validation score : 0.1991 log-loss : 3.07091\n",
      "training using chunk : 70 validation score : 0.1677 log-loss : 3.14415\n",
      "training using chunk : 71 validation score : 0.1443 log-loss : 3.24645\n",
      "training using chunk : 72 validation score : 0.1773 log-loss : 3.13886\n",
      "training using chunk : 73 validation score : 0.1981 log-loss : 3.16297\n",
      "training using chunk : 74 validation score : 0.1560 log-loss : 3.09347\n",
      "training using chunk : 75 validation score : 0.1988 log-loss : 2.94543\n",
      "training using chunk : 76 validation score : 0.1771 log-loss : 3.12306\n",
      "training using chunk : 77 validation score : 0.1885 log-loss : 3.07182\n",
      "training using chunk : 78 validation score : 0.2095 log-loss : 3.06876\n",
      "training using chunk : 79 validation score : 0.1509 log-loss : 3.17966\n",
      "training using chunk : 80 validation score : 0.1339 log-loss : 3.22918\n",
      "training using chunk : 81 validation score : 0.1763 log-loss : 3.05372\n",
      "training using chunk : 82 validation score : 0.1992 log-loss : 2.97753\n",
      "training using chunk : 83 validation score : 0.2116 log-loss : 2.90812\n",
      "training using chunk : 84 validation score : 0.1788 log-loss : 3.12068\n",
      "training using chunk : 85 validation score : 0.1906 log-loss : 2.87277\n",
      "training using chunk : 86 validation score : 0.1771 log-loss : 2.97622\n",
      "training using chunk : 87 validation score : 0.1676 log-loss : 3.00600\n",
      "training using chunk : 88 validation score : 0.2135 log-loss : 2.90911\n",
      "training using chunk : 89 validation score : 0.2207 log-loss : 2.91334\n",
      "training using chunk : 90 validation score : 0.1896 log-loss : 2.93713\n",
      "training using chunk : 91 validation score : 0.2139 log-loss : 2.90628\n",
      "training using chunk : 92 validation score : 0.1762 log-loss : 2.91003\n",
      "training using chunk : 93 validation score : 0.1965 log-loss : 2.88399\n",
      "training using chunk : 94 validation score : 0.1738 log-loss : 2.92325\n",
      "training using chunk : 95 validation score : 0.2135 log-loss : 2.85635\n",
      "training using chunk : 96 validation score : 0.1781 log-loss : 2.93739\n",
      "training using chunk : 97 validation score : 0.1495 log-loss : 3.04590\n",
      "training using chunk : 98 validation score : 0.2098 log-loss : 2.92877\n",
      "training using chunk : 99 validation score : 0.1867 log-loss : 2.82298\n",
      "training using chunk : 100 validation score : 0.2044 log-loss : 2.87380\n",
      "training using chunk : 101 validation score : 0.2124 log-loss : 2.88350\n",
      "training using chunk : 102 validation score : 0.1977 log-loss : 2.87618\n",
      "training using chunk : 103 validation score : 0.1629 log-loss : 2.90749\n",
      "training using chunk : 104 validation score : 0.1922 log-loss : 2.84509\n",
      "training using chunk : 105 validation score : 0.1952 log-loss : 2.83214\n",
      "training using chunk : 106 validation score : 0.2044 log-loss : 2.87166\n",
      "training using chunk : 107 validation score : 0.1942 log-loss : 2.81568\n",
      "training using chunk : 108 validation score : 0.2122 log-loss : 2.77419\n",
      "training using chunk : 109 validation score : 0.2092 log-loss : 2.80800\n",
      "training using chunk : 110 validation score : 0.2135 log-loss : 2.85368\n",
      "training using chunk : 111 validation score : 0.2135 log-loss : 2.86851\n",
      "training using chunk : 112 validation score : 0.2010 log-loss : 2.86104\n",
      "training using chunk : 113 validation score : 0.2039 log-loss : 2.78178\n",
      "training using chunk : 114 validation score : 0.2184 log-loss : 2.77938\n",
      "training using chunk : 115 validation score : 0.2235 log-loss : 2.80129\n",
      "training using chunk : 116 validation score : 0.1991 log-loss : 2.83040\n",
      "training using chunk : 117 validation score : 0.1848 log-loss : 2.85963\n",
      "training using chunk : 118 validation score : 0.2160 log-loss : 2.77545\n",
      "training using chunk : 119 validation score : 0.1983 log-loss : 2.84540\n",
      "training using chunk : 120 validation score : 0.1871 log-loss : 2.80919\n",
      "training using chunk : 121 validation score : 0.2101 log-loss : 2.79742\n",
      "training using chunk : 122 validation score : 0.2050 log-loss : 2.76772\n",
      "training using chunk : 123 validation score : 0.2144 log-loss : 2.72794\n",
      "training using chunk : 124 validation score : 0.2036 log-loss : 2.73410\n",
      "training using chunk : 125 validation score : 0.1997 log-loss : 2.82654\n",
      "training using chunk : 126 validation score : 0.2099 log-loss : 2.74149\n",
      "training using chunk : 127 validation score : 0.1468 log-loss : 2.89954\n",
      "training using chunk : 128 validation score : 0.1866 log-loss : 2.78391\n",
      "training using chunk : 129 validation score : 0.2145 log-loss : 2.73499\n",
      "training using chunk : 130 validation score : 0.2163 log-loss : 2.76312\n",
      "training using chunk : 131 validation score : 0.2115 log-loss : 2.70283\n",
      "training using chunk : 132 validation score : 0.2122 log-loss : 2.80953\n",
      "training using chunk : 133 validation score : 0.1966 log-loss : 2.74398\n",
      "training using chunk : 134 validation score : 0.2102 log-loss : 2.73406\n",
      "training using chunk : 135 validation score : 0.2324 log-loss : 2.74658\n",
      "training using chunk : 136 validation score : 0.2237 log-loss : 2.70569\n",
      "training using chunk : 137 validation score : 0.2108 log-loss : 2.75398\n",
      "training using chunk : 138 validation score : 0.1873 log-loss : 2.78332\n",
      "training using chunk : 139 validation score : 0.2253 log-loss : 2.70609\n",
      "training using chunk : 140 validation score : 0.2222 log-loss : 2.75309\n",
      "training using chunk : 141 validation score : 0.2215 log-loss : 2.69092\n",
      "training using chunk : 142 validation score : 0.2138 log-loss : 2.72116\n",
      "training using chunk : 143 validation score : 0.2024 log-loss : 2.73672\n",
      "training using chunk : 144 validation score : 0.2240 log-loss : 2.69475\n",
      "training using chunk : 145 validation score : 0.1960 log-loss : 2.80552\n",
      "training using chunk : 146 validation score : 0.2114 log-loss : 2.72750\n",
      "training using chunk : 147 validation score : 0.1928 log-loss : 2.74860\n",
      "training using chunk : 148 validation score : 0.2087 log-loss : 2.69192\n",
      "training using chunk : 149 validation score : 0.2173 log-loss : 2.67600\n",
      "training using chunk : 150 validation score : 0.2004 log-loss : 2.72536\n",
      "training using chunk : 151 validation score : 0.2242 log-loss : 2.70087\n",
      "training using chunk : 152 validation score : 0.2285 log-loss : 2.71001\n",
      "training using chunk : 153 validation score : 0.2329 log-loss : 2.64961\n",
      "training using chunk : 154 validation score : 0.2202 log-loss : 2.68512\n",
      "training using chunk : 155 validation score : 0.2030 log-loss : 2.72807\n",
      "training using chunk : 156 validation score : 0.1983 log-loss : 2.71356\n",
      "training using chunk : 157 validation score : 0.2149 log-loss : 2.72307\n",
      "training using chunk : 158 validation score : 0.1936 log-loss : 2.78219\n",
      "training using chunk : 159 validation score : 0.2348 log-loss : 2.72337\n",
      "training using chunk : 160 validation score : 0.2185 log-loss : 2.67180\n",
      "training using chunk : 161 validation score : 0.2341 log-loss : 2.64762\n",
      "training using chunk : 162 validation score : 0.2302 log-loss : 2.67231\n",
      "training using chunk : 163 validation score : 0.2073 log-loss : 2.70104\n",
      "training using chunk : 164 validation score : 0.2404 log-loss : 2.67633\n",
      "training using chunk : 165 validation score : 0.2391 log-loss : 2.63583\n",
      "training using chunk : 166 validation score : 0.2366 log-loss : 2.65519\n",
      "training using chunk : 167 validation score : 0.2429 log-loss : 2.67999\n",
      "training using chunk : 168 validation score : 0.2190 log-loss : 2.68177\n",
      "training using chunk : 169 validation score : 0.2315 log-loss : 2.67112\n",
      "training using chunk : 170 validation score : 0.2229 log-loss : 2.63500\n",
      "training using chunk : 171 validation score : 0.2195 log-loss : 2.64620\n",
      "training using chunk : 172 validation score : 0.2248 log-loss : 2.64806\n",
      "training using chunk : 173 validation score : 0.2346 log-loss : 2.63714\n",
      "training using chunk : 174 validation score : 0.2034 log-loss : 2.67424\n",
      "training using chunk : 175 validation score : 0.2204 log-loss : 2.64172\n",
      "training using chunk : 176 validation score : 0.2004 log-loss : 2.67973\n",
      "training using chunk : 177 validation score : 0.2296 log-loss : 2.73266\n",
      "training using chunk : 178 validation score : 0.2273 log-loss : 2.64119\n",
      "training using chunk : 179 validation score : 0.2416 log-loss : 2.64869\n",
      "training using chunk : 180 validation score : 0.2092 log-loss : 2.70482\n",
      "training using chunk : 181 validation score : 0.2248 log-loss : 2.68609\n",
      "training using chunk : 182 validation score : 0.2089 log-loss : 2.67195\n",
      "training using chunk : 183 validation score : 0.2424 log-loss : 2.65531\n",
      "training using chunk : 184 validation score : 0.2347 log-loss : 2.62437\n",
      "training using chunk : 185 validation score : 0.2153 log-loss : 2.66613\n",
      "training using chunk : 186 validation score : 0.2127 log-loss : 2.65421\n",
      "training using chunk : 187 validation score : 0.2279 log-loss : 2.63941\n",
      "training using chunk : 188 validation score : 0.2147 log-loss : 2.66626\n",
      "training using chunk : 189 validation score : 0.2395 log-loss : 2.60081\n",
      "training using chunk : 190 validation score : 0.2240 log-loss : 2.64091\n",
      "training using chunk : 191 validation score : 0.2189 log-loss : 2.65714\n",
      "training using chunk : 192 validation score : 0.2246 log-loss : 2.63678\n",
      "training using chunk : 193 validation score : 0.2268 log-loss : 2.62732\n",
      "training using chunk : 194 validation score : 0.2147 log-loss : 2.62544\n",
      "training using chunk : 195 validation score : 0.2296 log-loss : 2.60546\n",
      "training using chunk : 196 validation score : 0.2253 log-loss : 2.60485\n",
      "training using chunk : 197 validation score : 0.2286 log-loss : 2.62618\n",
      "training using chunk : 198 validation score : 0.2392 log-loss : 2.60648\n",
      "training using chunk : 199 validation score : 0.2140 log-loss : 2.68513\n",
      "training using chunk : 200 validation score : 0.2193 log-loss : 2.60920\n",
      "training using chunk : 201 validation score : 0.2349 log-loss : 2.63201\n",
      "training using chunk : 202 validation score : 0.2325 log-loss : 2.61576\n",
      "training using chunk : 203 validation score : 0.2231 log-loss : 2.63704\n",
      "training using chunk : 204 validation score : 0.1988 log-loss : 2.67639\n",
      "training using chunk : 205 validation score : 0.1965 log-loss : 2.65805\n",
      "training using chunk : 206 validation score : 0.2337 log-loss : 2.63281\n",
      "training using chunk : 207 validation score : 0.2259 log-loss : 2.61435\n",
      "training using chunk : 208 validation score : 0.2200 log-loss : 2.64499\n",
      "training using chunk : 209 validation score : 0.2312 log-loss : 2.61745\n",
      "training using chunk : 210 validation score : 0.2322 log-loss : 2.64306\n",
      "training using chunk : 211 validation score : 0.2159 log-loss : 2.62332\n",
      "training using chunk : 212 validation score : 0.2353 log-loss : 2.57831\n",
      "training using chunk : 213 validation score : 0.2046 log-loss : 2.67630\n",
      "training using chunk : 214 validation score : 0.2168 log-loss : 2.67078\n",
      "training using chunk : 215 validation score : 0.2233 log-loss : 2.60125\n",
      "training using chunk : 216 validation score : 0.2263 log-loss : 2.63318\n",
      "training using chunk : 217 validation score : 0.2254 log-loss : 2.59631\n",
      "training using chunk : 218 validation score : 0.2346 log-loss : 2.59901\n",
      "training using chunk : 219 validation score : 0.2009 log-loss : 2.63576\n",
      "training using chunk : 220 validation score : 0.2241 log-loss : 2.60125\n",
      "training using chunk : 221 validation score : 0.2436 log-loss : 2.62335\n",
      "training using chunk : 222 validation score : 0.2208 log-loss : 2.61644\n",
      "training using chunk : 223 validation score : 0.2370 log-loss : 2.65148\n",
      "training using chunk : 224 validation score : 0.2041 log-loss : 2.63320\n",
      "training using chunk : 225 validation score : 0.2398 log-loss : 2.58174\n",
      "training using chunk : 226 validation score : 0.2356 log-loss : 2.59159\n",
      "training using chunk : 227 validation score : 0.2393 log-loss : 2.58474\n",
      "training using chunk : 228 validation score : 0.2299 log-loss : 2.58713\n",
      "training using chunk : 229 validation score : 0.2228 log-loss : 2.61760\n",
      "training using chunk : 230 validation score : 0.2401 log-loss : 2.58477\n",
      "training using chunk : 231 validation score : 0.2193 log-loss : 2.60842\n",
      "training using chunk : 232 validation score : 0.2169 log-loss : 2.61013\n",
      "training using chunk : 233 validation score : 0.2311 log-loss : 2.62023\n",
      "training using chunk : 234 validation score : 0.2203 log-loss : 2.59407\n",
      "training using chunk : 235 validation score : 0.2227 log-loss : 2.61953\n",
      "training using chunk : 236 validation score : 0.2001 log-loss : 2.65155\n",
      "training using chunk : 237 validation score : 0.2305 log-loss : 2.57986\n",
      "training using chunk : 238 validation score : 0.2237 log-loss : 2.64338\n",
      "training using chunk : 239 validation score : 0.2344 log-loss : 2.59870\n",
      "training using chunk : 240 validation score : 0.2384 log-loss : 2.60160\n",
      "training using chunk : 241 validation score : 0.2273 log-loss : 2.59837\n",
      "training using chunk : 242 validation score : 0.2282 log-loss : 2.60713\n",
      "training using chunk : 243 validation score : 0.2430 log-loss : 2.59424\n",
      "training using chunk : 244 validation score : 0.2323 log-loss : 2.59152\n",
      "training using chunk : 245 validation score : 0.2481 log-loss : 2.58538\n",
      "training using chunk : 246 validation score : 0.2402 log-loss : 2.58165\n",
      "training using chunk : 247 validation score : 0.2313 log-loss : 2.59311\n",
      "training using chunk : 248 validation score : 0.2387 log-loss : 2.57621\n",
      "training using chunk : 249 validation score : 0.2197 log-loss : 2.61613\n",
      "training using chunk : 250 validation score : 0.2355 log-loss : 2.58560\n",
      "training using chunk : 251 validation score : 0.2228 log-loss : 2.61612\n",
      "training using chunk : 252 validation score : 0.2025 log-loss : 2.62759\n",
      "training using chunk : 253 validation score : 0.2224 log-loss : 2.59756\n",
      "training using chunk : 254 validation score : 0.2393 log-loss : 2.57110\n",
      "training using chunk : 255 validation score : 0.2406 log-loss : 2.58622\n",
      "training using chunk : 256 validation score : 0.2058 log-loss : 2.62363\n",
      "training using chunk : 257 validation score : 0.2290 log-loss : 2.57464\n",
      "training using chunk : 258 validation score : 0.2394 log-loss : 2.58049\n",
      "training using chunk : 259 validation score : 0.2356 log-loss : 2.59440\n",
      "training using chunk : 260 validation score : 0.2421 log-loss : 2.55469\n",
      "training using chunk : 261 validation score : 0.2218 log-loss : 2.58492\n",
      "training using chunk : 262 validation score : 0.2437 log-loss : 2.57976\n",
      "training using chunk : 263 validation score : 0.2345 log-loss : 2.56811\n",
      "training using chunk : 264 validation score : 0.2380 log-loss : 2.59277\n",
      "training using chunk : 265 validation score : 0.2438 log-loss : 2.56534\n",
      "training using chunk : 266 validation score : 0.2394 log-loss : 2.59118\n",
      "training using chunk : 267 validation score : 0.2082 log-loss : 2.60271\n",
      "training using chunk : 268 validation score : 0.2364 log-loss : 2.59862\n",
      "training using chunk : 269 validation score : 0.2326 log-loss : 2.58375\n",
      "training using chunk : 270 validation score : 0.2227 log-loss : 2.58597\n",
      "training using chunk : 271 validation score : 0.2390 log-loss : 2.59364\n",
      "training using chunk : 272 validation score : 0.2291 log-loss : 2.57461\n",
      "training using chunk : 273 validation score : 0.2251 log-loss : 2.59593\n",
      "training using chunk : 274 validation score : 0.2365 log-loss : 2.56256\n",
      "training using chunk : 275 validation score : 0.2349 log-loss : 2.56041\n",
      "training using chunk : 276 validation score : 0.2331 log-loss : 2.57345\n",
      "training using chunk : 277 validation score : 0.2382 log-loss : 2.57281\n",
      "training using chunk : 278 validation score : 0.2089 log-loss : 2.62167\n",
      "training using chunk : 279 validation score : 0.2513 log-loss : 2.54799\n",
      "training using chunk : 280 validation score : 0.2380 log-loss : 2.55345\n",
      "training using chunk : 281 validation score : 0.2433 log-loss : 2.55962\n",
      "training using chunk : 282 validation score : 0.2472 log-loss : 2.55595\n",
      "training using chunk : 283 validation score : 0.2366 log-loss : 2.55752\n",
      "training using chunk : 284 validation score : 0.2330 log-loss : 2.55541\n",
      "training using chunk : 285 validation score : 0.2427 log-loss : 2.56597\n",
      "training using chunk : 286 validation score : 0.2214 log-loss : 2.58087\n",
      "training using chunk : 287 validation score : 0.2244 log-loss : 2.56284\n",
      "training using chunk : 288 validation score : 0.2425 log-loss : 2.56694\n",
      "training using chunk : 289 validation score : 0.2440 log-loss : 2.54088\n",
      "training using chunk : 290 validation score : 0.2392 log-loss : 2.55964\n",
      "training using chunk : 291 validation score : 0.2300 log-loss : 2.55051\n",
      "training using chunk : 292 validation score : 0.2289 log-loss : 2.56716\n",
      "training using chunk : 293 validation score : 0.2076 log-loss : 2.59241\n",
      "training using chunk : 294 validation score : 0.2175 log-loss : 2.60552\n",
      "training using chunk : 295 validation score : 0.2264 log-loss : 2.56759\n",
      "training using chunk : 296 validation score : 0.2511 log-loss : 2.55549\n",
      "training using chunk : 297 validation score : 0.2385 log-loss : 2.55220\n",
      "training using chunk : 298 validation score : 0.2404 log-loss : 2.57803\n",
      "training using chunk : 299 validation score : 0.2367 log-loss : 2.54655\n",
      "training using chunk : 300 validation score : 0.2372 log-loss : 2.54492\n",
      "training using chunk : 301 validation score : 0.2457 log-loss : 2.55633\n",
      "training using chunk : 302 validation score : 0.2398 log-loss : 2.56844\n",
      "training using chunk : 303 validation score : 0.2473 log-loss : 2.56155\n",
      "training using chunk : 304 validation score : 0.2255 log-loss : 2.55046\n",
      "training using chunk : 305 validation score : 0.2496 log-loss : 2.52044\n",
      "training using chunk : 306 validation score : 0.2438 log-loss : 2.55896\n",
      "training using chunk : 307 validation score : 0.2284 log-loss : 2.54876\n",
      "training using chunk : 308 validation score : 0.2418 log-loss : 2.55953\n",
      "training using chunk : 309 validation score : 0.2235 log-loss : 2.57613\n",
      "training using chunk : 310 validation score : 0.2411 log-loss : 2.57086\n",
      "training using chunk : 311 validation score : 0.2414 log-loss : 2.53914\n",
      "training using chunk : 312 validation score : 0.2319 log-loss : 2.55070\n",
      "training using chunk : 313 validation score : 0.2318 log-loss : 2.57100\n",
      "training using chunk : 314 validation score : 0.2445 log-loss : 2.53531\n",
      "training using chunk : 315 validation score : 0.2446 log-loss : 2.55308\n",
      "training using chunk : 316 validation score : 0.2448 log-loss : 2.55041\n",
      "training using chunk : 317 validation score : 0.2328 log-loss : 2.55718\n",
      "training using chunk : 318 validation score : 0.2384 log-loss : 2.55875\n",
      "training using chunk : 319 validation score : 0.2366 log-loss : 2.55807\n",
      "training using chunk : 320 validation score : 0.2390 log-loss : 2.53861\n",
      "training using chunk : 321 validation score : 0.2303 log-loss : 2.57411\n",
      "training using chunk : 322 validation score : 0.2444 log-loss : 2.53478\n",
      "training using chunk : 323 validation score : 0.2417 log-loss : 2.54566\n",
      "training using chunk : 324 validation score : 0.2368 log-loss : 2.53020\n",
      "training using chunk : 325 validation score : 0.2465 log-loss : 2.53378\n",
      "training using chunk : 326 validation score : 0.2242 log-loss : 2.56759\n",
      "training using chunk : 327 validation score : 0.2494 log-loss : 2.55602\n",
      "training using chunk : 328 validation score : 0.2384 log-loss : 2.55880\n",
      "training using chunk : 329 validation score : 0.2394 log-loss : 2.55074\n",
      "training using chunk : 330 validation score : 0.2382 log-loss : 2.54074\n",
      "training using chunk : 331 validation score : 0.2404 log-loss : 2.53400\n",
      "training using chunk : 332 validation score : 0.2452 log-loss : 2.51887\n",
      "training using chunk : 333 validation score : 0.2493 log-loss : 2.54798\n",
      "training using chunk : 334 validation score : 0.2431 log-loss : 2.53217\n",
      "training using chunk : 335 validation score : 0.2353 log-loss : 2.55495\n",
      "training using chunk : 336 validation score : 0.2156 log-loss : 2.58041\n",
      "training using chunk : 337 validation score : 0.2341 log-loss : 2.54904\n",
      "training using chunk : 338 validation score : 0.2476 log-loss : 2.53065\n",
      "training using chunk : 339 validation score : 0.2349 log-loss : 2.53411\n",
      "training using chunk : 340 validation score : 0.2251 log-loss : 2.55394\n",
      "training using chunk : 341 validation score : 0.2388 log-loss : 2.55028\n",
      "training using chunk : 342 validation score : 0.2428 log-loss : 2.52863\n",
      "training using chunk : 343 validation score : 0.2519 log-loss : 2.53930\n",
      "training using chunk : 344 validation score : 0.2523 log-loss : 2.51659\n",
      "training using chunk : 345 validation score : 0.2511 log-loss : 2.51846\n",
      "training using chunk : 346 validation score : 0.2353 log-loss : 2.52930\n",
      "training using chunk : 347 validation score : 0.2408 log-loss : 2.54412\n",
      "training using chunk : 348 validation score : 0.2526 log-loss : 2.53428\n",
      "training using chunk : 349 validation score : 0.2468 log-loss : 2.53728\n",
      "training using chunk : 350 validation score : 0.2486 log-loss : 2.53764\n",
      "training using chunk : 351 validation score : 0.2476 log-loss : 2.54183\n",
      "training using chunk : 352 validation score : 0.2265 log-loss : 2.54111\n",
      "training using chunk : 353 validation score : 0.2258 log-loss : 2.55367\n",
      "training using chunk : 354 validation score : 0.2300 log-loss : 2.55097\n",
      "training using chunk : 355 validation score : 0.2428 log-loss : 2.52848\n",
      "training using chunk : 356 validation score : 0.2268 log-loss : 2.55678\n",
      "training using chunk : 357 validation score : 0.2434 log-loss : 2.52420\n",
      "training using chunk : 358 validation score : 0.2479 log-loss : 2.53181\n",
      "training using chunk : 359 validation score : 0.2510 log-loss : 2.53466\n",
      "training using chunk : 360 validation score : 0.2515 log-loss : 2.52787\n",
      "training using chunk : 361 validation score : 0.2401 log-loss : 2.52599\n",
      "training using chunk : 362 validation score : 0.2409 log-loss : 2.52530\n",
      "training using chunk : 363 validation score : 0.2440 log-loss : 2.52111\n",
      "training using chunk : 364 validation score : 0.2437 log-loss : 2.53486\n",
      "training using chunk : 365 validation score : 0.2451 log-loss : 2.53459\n",
      "training using chunk : 366 validation score : 0.2295 log-loss : 2.54829\n",
      "training using chunk : 367 validation score : 0.2484 log-loss : 2.51642\n",
      "training using chunk : 368 validation score : 0.2527 log-loss : 2.50921\n",
      "training using chunk : 369 validation score : 0.2497 log-loss : 2.55501\n",
      "training using chunk : 370 validation score : 0.2416 log-loss : 2.54150\n",
      "training using chunk : 371 validation score : 0.2346 log-loss : 2.54269\n",
      "training using chunk : 372 validation score : 0.2405 log-loss : 2.53284\n",
      "training using chunk : 373 validation score : 0.2392 log-loss : 2.54172\n",
      "training using chunk : 374 validation score : 0.2494 log-loss : 2.52741\n",
      "training using chunk : 375 validation score : 0.2405 log-loss : 2.53865\n",
      "training using chunk : 376 validation score : 0.2434 log-loss : 2.53118\n",
      "training using chunk : 377 validation score : 0.2300 log-loss : 2.53565\n",
      "training using chunk : 378 validation score : 0.2375 log-loss : 2.52496\n",
      "training using chunk : 379 validation score : 0.2439 log-loss : 2.52351\n",
      "training using chunk : 380 validation score : 0.2317 log-loss : 2.54554\n",
      "training using chunk : 381 validation score : 0.2216 log-loss : 2.54570\n",
      "training using chunk : 382 validation score : 0.2438 log-loss : 2.52584\n",
      "training using chunk : 383 validation score : 0.2413 log-loss : 2.54345\n",
      "training using chunk : 384 validation score : 0.2243 log-loss : 2.53330\n",
      "training using chunk : 385 validation score : 0.2413 log-loss : 2.52857\n",
      "training using chunk : 386 validation score : 0.2383 log-loss : 2.52369\n",
      "training using chunk : 387 validation score : 0.2434 log-loss : 2.53011\n",
      "training using chunk : 388 validation score : 0.2426 log-loss : 2.53900\n",
      "training using chunk : 389 validation score : 0.2472 log-loss : 2.50636\n",
      "training using chunk : 390 validation score : 0.2457 log-loss : 2.53194\n",
      "training using chunk : 391 validation score : 0.2334 log-loss : 2.54620\n",
      "training using chunk : 392 validation score : 0.2299 log-loss : 2.53190\n",
      "training using chunk : 393 validation score : 0.2440 log-loss : 2.52246\n",
      "training using chunk : 394 validation score : 0.2511 log-loss : 2.52197\n",
      "training using chunk : 395 validation score : 0.2246 log-loss : 2.52697\n",
      "training using chunk : 396 validation score : 0.2535 log-loss : 2.51284\n",
      "training using chunk : 397 validation score : 0.2543 log-loss : 2.51277\n",
      "training using chunk : 398 validation score : 0.2297 log-loss : 2.52034\n",
      "training using chunk : 399 validation score : 0.2480 log-loss : 2.51879\n",
      "training using chunk : 400 validation score : 0.2341 log-loss : 2.52519\n",
      "training using chunk : 401 validation score : 0.2522 log-loss : 2.53456\n",
      "training using chunk : 402 validation score : 0.2265 log-loss : 2.55360\n",
      "training using chunk : 403 validation score : 0.2349 log-loss : 2.54081\n",
      "training using chunk : 404 validation score : 0.2488 log-loss : 2.51639\n",
      "training using chunk : 405 validation score : 0.2519 log-loss : 2.52686\n",
      "training using chunk : 406 validation score : 0.2515 log-loss : 2.52942\n",
      "training using chunk : 407 validation score : 0.2543 log-loss : 2.52152\n",
      "training using chunk : 408 validation score : 0.2534 log-loss : 2.50453\n",
      "training using chunk : 409 validation score : 0.2365 log-loss : 2.52392\n",
      "training using chunk : 410 validation score : 0.2417 log-loss : 2.51210\n",
      "training using chunk : 411 validation score : 0.2420 log-loss : 2.54220\n",
      "training using chunk : 412 validation score : 0.2475 log-loss : 2.51679\n",
      "training using chunk : 413 validation score : 0.2440 log-loss : 2.51504\n",
      "training using chunk : 414 validation score : 0.2506 log-loss : 2.52602\n",
      "training using chunk : 415 validation score : 0.2479 log-loss : 2.51800\n",
      "training using chunk : 416 validation score : 0.2514 log-loss : 2.51234\n",
      "training using chunk : 417 validation score : 0.2505 log-loss : 2.51345\n",
      "training using chunk : 418 validation score : 0.2380 log-loss : 2.51194\n",
      "training using chunk : 419 validation score : 0.2451 log-loss : 2.51174\n",
      "training using chunk : 420 validation score : 0.2478 log-loss : 2.51612\n",
      "training using chunk : 421 validation score : 0.2379 log-loss : 2.51744\n",
      "training using chunk : 422 validation score : 0.2487 log-loss : 2.51396\n",
      "training using chunk : 423 validation score : 0.2528 log-loss : 2.52082\n",
      "training using chunk : 424 validation score : 0.2474 log-loss : 2.51407\n",
      "training using chunk : 425 validation score : 0.2466 log-loss : 2.50202\n",
      "training using chunk : 426 validation score : 0.2520 log-loss : 2.50107\n",
      "training using chunk : 427 validation score : 0.2500 log-loss : 2.50072\n",
      "training using chunk : 428 validation score : 0.2459 log-loss : 2.51127\n",
      "training using chunk : 429 validation score : 0.2544 log-loss : 2.51595\n",
      "training using chunk : 430 validation score : 0.2479 log-loss : 2.51056\n",
      "training using chunk : 431 validation score : 0.2443 log-loss : 2.50922\n",
      "training using chunk : 432 validation score : 0.2414 log-loss : 2.51274\n",
      "training using chunk : 433 validation score : 0.2505 log-loss : 2.51799\n",
      "training using chunk : 434 validation score : 0.2308 log-loss : 2.52722\n",
      "training using chunk : 435 validation score : 0.2382 log-loss : 2.51464\n",
      "training using chunk : 436 validation score : 0.2506 log-loss : 2.49951\n",
      "training using chunk : 437 validation score : 0.2505 log-loss : 2.52305\n",
      "training using chunk : 438 validation score : 0.2525 log-loss : 2.51385\n",
      "training using chunk : 439 validation score : 0.2485 log-loss : 2.51763\n",
      "training using chunk : 440 validation score : 0.2537 log-loss : 2.49740\n",
      "training using chunk : 441 validation score : 0.2506 log-loss : 2.51270\n",
      "training using chunk : 442 validation score : 0.2368 log-loss : 2.52267\n",
      "training using chunk : 443 validation score : 0.2536 log-loss : 2.50690\n",
      "training using chunk : 444 validation score : 0.2535 log-loss : 2.51056\n",
      "training using chunk : 445 validation score : 0.2481 log-loss : 2.50508\n",
      "training using chunk : 446 validation score : 0.2467 log-loss : 2.50905\n",
      "training using chunk : 447 validation score : 0.2564 log-loss : 2.48709\n",
      "training using chunk : 448 validation score : 0.2557 log-loss : 2.50261\n",
      "training using chunk : 449 validation score : 0.2571 log-loss : 2.50157\n",
      "training using chunk : 450 validation score : 0.2476 log-loss : 2.49883\n",
      "training using chunk : 451 validation score : 0.2329 log-loss : 2.50991\n",
      "training using chunk : 452 validation score : 0.2414 log-loss : 2.51732\n",
      "training using chunk : 453 validation score : 0.2369 log-loss : 2.50831\n",
      "training using chunk : 454 validation score : 0.2474 log-loss : 2.49947\n",
      "training using chunk : 455 validation score : 0.2494 log-loss : 2.50958\n",
      "training using chunk : 456 validation score : 0.2476 log-loss : 2.52558\n",
      "training using chunk : 457 validation score : 0.2410 log-loss : 2.50644\n",
      "training using chunk : 458 validation score : 0.2384 log-loss : 2.50721\n",
      "training using chunk : 459 validation score : 0.2463 log-loss : 2.51502\n",
      "training using chunk : 460 validation score : 0.2504 log-loss : 2.50626\n",
      "training using chunk : 461 validation score : 0.2473 log-loss : 2.51312\n",
      "training using chunk : 462 validation score : 0.2457 log-loss : 2.50624\n",
      "training using chunk : 463 validation score : 0.2525 log-loss : 2.51095\n",
      "training using chunk : 464 validation score : 0.2419 log-loss : 2.53623\n",
      "training using chunk : 465 validation score : 0.2508 log-loss : 2.50278\n",
      "training using chunk : 466 validation score : 0.2338 log-loss : 2.52924\n",
      "training using chunk : 467 validation score : 0.2454 log-loss : 2.51780\n",
      "training using chunk : 468 validation score : 0.2482 log-loss : 2.51764\n",
      "training using chunk : 469 validation score : 0.2538 log-loss : 2.50457\n",
      "training using chunk : 470 validation score : 0.2435 log-loss : 2.49781\n",
      "training using chunk : 471 validation score : 0.2513 log-loss : 2.48932\n",
      "training using chunk : 472 validation score : 0.2482 log-loss : 2.50360\n",
      "training using chunk : 473 validation score : 0.2518 log-loss : 2.49705\n",
      "training using chunk : 474 validation score : 0.2551 log-loss : 2.49674\n",
      "training using chunk : 475 validation score : 0.2508 log-loss : 2.49648\n",
      "training using chunk : 476 validation score : 0.2380 log-loss : 2.52076\n",
      "training using chunk : 477 validation score : 0.2520 log-loss : 2.51841\n",
      "training using chunk : 478 validation score : 0.2423 log-loss : 2.49931\n",
      "training using chunk : 479 validation score : 0.2449 log-loss : 2.49853\n",
      "training using chunk : 480 validation score : 0.2551 log-loss : 2.49442\n",
      "training using chunk : 481 validation score : 0.2522 log-loss : 2.50330\n",
      "training using chunk : 482 validation score : 0.2280 log-loss : 2.53145\n",
      "training using chunk : 483 validation score : 0.2499 log-loss : 2.49797\n",
      "training using chunk : 484 validation score : 0.2532 log-loss : 2.49154\n",
      "training using chunk : 485 validation score : 0.2550 log-loss : 2.48797\n",
      "training using chunk : 486 validation score : 0.2505 log-loss : 2.49474\n",
      "training using chunk : 487 validation score : 0.2541 log-loss : 2.50931\n",
      "training using chunk : 488 validation score : 0.2517 log-loss : 2.50266\n",
      "training using chunk : 489 validation score : 0.2524 log-loss : 2.50054\n",
      "training using chunk : 490 validation score : 0.2484 log-loss : 2.49751\n",
      "training using chunk : 491 validation score : 0.2460 log-loss : 2.49844\n",
      "training using chunk : 492 validation score : 0.2396 log-loss : 2.52161\n",
      "training using chunk : 493 validation score : 0.2567 log-loss : 2.49835\n",
      "training using chunk : 494 validation score : 0.2490 log-loss : 2.49825\n",
      "training using chunk : 495 validation score : 0.2461 log-loss : 2.51869\n",
      "training using chunk : 496 validation score : 0.2450 log-loss : 2.51824\n",
      "training using chunk : 497 validation score : 0.2520 log-loss : 2.50430\n",
      "training using chunk : 498 validation score : 0.2559 log-loss : 2.49043\n",
      "training using chunk : 499 validation score : 0.2493 log-loss : 2.51334\n",
      "training using chunk : 500 validation score : 0.2484 log-loss : 2.51049\n",
      "training using chunk : 501 validation score : 0.2493 log-loss : 2.50323\n",
      "training using chunk : 502 validation score : 0.2409 log-loss : 2.50186\n",
      "training using chunk : 503 validation score : 0.2441 log-loss : 2.50551\n",
      "training using chunk : 504 validation score : 0.2561 log-loss : 2.50379\n",
      "training using chunk : 505 validation score : 0.2472 log-loss : 2.52557\n",
      "training using chunk : 506 validation score : 0.2543 log-loss : 2.51912\n",
      "training using chunk : 507 validation score : 0.2543 log-loss : 2.49870\n",
      "training using chunk : 508 validation score : 0.2508 log-loss : 2.50455\n",
      "training using chunk : 509 validation score : 0.2468 log-loss : 2.51126\n",
      "training using chunk : 510 validation score : 0.2557 log-loss : 2.49700\n",
      "training using chunk : 511 validation score : 0.2570 log-loss : 2.48114\n",
      "training using chunk : 512 validation score : 0.2564 log-loss : 2.49603\n",
      "training using chunk : 513 validation score : 0.2359 log-loss : 2.53557\n",
      "training using chunk : 514 validation score : 0.2541 log-loss : 2.49604\n",
      "training using chunk : 515 validation score : 0.2420 log-loss : 2.51560\n",
      "training using chunk : 516 validation score : 0.2506 log-loss : 2.49465\n",
      "training using chunk : 517 validation score : 0.2490 log-loss : 2.49113\n",
      "training using chunk : 518 validation score : 0.2520 log-loss : 2.49942\n",
      "training using chunk : 519 validation score : 0.2493 log-loss : 2.50601\n",
      "training using chunk : 520 validation score : 0.2562 log-loss : 2.49369\n",
      "training using chunk : 521 validation score : 0.2286 log-loss : 2.53172\n",
      "training using chunk : 522 validation score : 0.2493 log-loss : 2.48667\n",
      "training using chunk : 523 validation score : 0.2475 log-loss : 2.49962\n",
      "training using chunk : 524 validation score : 0.2522 log-loss : 2.50906\n",
      "training using chunk : 525 validation score : 0.2575 log-loss : 2.49132\n",
      "training using chunk : 526 validation score : 0.2545 log-loss : 2.49224\n",
      "training using chunk : 527 validation score : 0.2557 log-loss : 2.49232\n",
      "training using chunk : 528 validation score : 0.2574 log-loss : 2.48262\n",
      "training using chunk : 529 validation score : 0.2540 log-loss : 2.50572\n",
      "training using chunk : 530 validation score : 0.2523 log-loss : 2.49764\n",
      "training using chunk : 531 validation score : 0.2564 log-loss : 2.49044\n",
      "training using chunk : 532 validation score : 0.2553 log-loss : 2.47841\n",
      "training using chunk : 533 validation score : 0.2291 log-loss : 2.50848\n",
      "training using chunk : 534 validation score : 0.2519 log-loss : 2.48723\n",
      "training using chunk : 535 validation score : 0.2409 log-loss : 2.49612\n",
      "training using chunk : 536 validation score : 0.2406 log-loss : 2.51901\n",
      "training using chunk : 537 validation score : 0.2551 log-loss : 2.49277\n",
      "training using chunk : 538 validation score : 0.2560 log-loss : 2.48790\n",
      "training using chunk : 539 validation score : 0.2426 log-loss : 2.51241\n",
      "training using chunk : 540 validation score : 0.2604 log-loss : 2.48477\n",
      "training using chunk : 541 validation score : 0.2587 log-loss : 2.48015\n",
      "training using chunk : 542 validation score : 0.2504 log-loss : 2.49122\n",
      "training using chunk : 543 validation score : 0.2469 log-loss : 2.49934\n",
      "training using chunk : 544 validation score : 0.2523 log-loss : 2.49026\n",
      "training using chunk : 545 validation score : 0.2433 log-loss : 2.48866\n",
      "training using chunk : 546 validation score : 0.2537 log-loss : 2.48541\n",
      "training using chunk : 547 validation score : 0.2506 log-loss : 2.49210\n",
      "training using chunk : 548 validation score : 0.2441 log-loss : 2.48944\n",
      "training using chunk : 549 validation score : 0.2554 log-loss : 2.49494\n",
      "training using chunk : 550 validation score : 0.2520 log-loss : 2.48330\n",
      "training using chunk : 551 validation score : 0.2507 log-loss : 2.49921\n",
      "training using chunk : 552 validation score : 0.2445 log-loss : 2.50424\n",
      "training using chunk : 553 validation score : 0.2343 log-loss : 2.50358\n",
      "training using chunk : 554 validation score : 0.2485 log-loss : 2.48466\n",
      "training using chunk : 555 validation score : 0.2408 log-loss : 2.49195\n",
      "training using chunk : 556 validation score : 0.2558 log-loss : 2.48414\n",
      "training using chunk : 557 validation score : 0.2507 log-loss : 2.49910\n",
      "training using chunk : 558 validation score : 0.2505 log-loss : 2.48808\n",
      "training using chunk : 559 validation score : 0.2542 log-loss : 2.48996\n",
      "training using chunk : 560 validation score : 0.2512 log-loss : 2.50580\n",
      "training using chunk : 561 validation score : 0.2349 log-loss : 2.52472\n",
      "training using chunk : 562 validation score : 0.2549 log-loss : 2.49499\n",
      "training using chunk : 563 validation score : 0.2601 log-loss : 2.47987\n",
      "training using chunk : 564 validation score : 0.2554 log-loss : 2.49778\n",
      "training using chunk : 565 validation score : 0.2556 log-loss : 2.49181\n",
      "training using chunk : 566 validation score : 0.2408 log-loss : 2.48979\n",
      "training using chunk : 567 validation score : 0.2496 log-loss : 2.48696\n",
      "training using chunk : 568 validation score : 0.2544 log-loss : 2.47734\n",
      "training using chunk : 569 validation score : 0.2577 log-loss : 2.48913\n",
      "training using chunk : 570 validation score : 0.2454 log-loss : 2.49205\n",
      "training using chunk : 571 validation score : 0.2549 log-loss : 2.49152\n",
      "training using chunk : 572 validation score : 0.2548 log-loss : 2.48442\n",
      "training using chunk : 573 validation score : 0.2449 log-loss : 2.49878\n",
      "training using chunk : 574 validation score : 0.2580 log-loss : 2.49056\n",
      "training using chunk : 575 validation score : 0.2572 log-loss : 2.47927\n",
      "training using chunk : 576 validation score : 0.2540 log-loss : 2.49324\n",
      "training using chunk : 577 validation score : 0.2524 log-loss : 2.48246\n",
      "training using chunk : 578 validation score : 0.2601 log-loss : 2.49393\n",
      "training using chunk : 579 validation score : 0.2496 log-loss : 2.49150\n",
      "training using chunk : 580 validation score : 0.2513 log-loss : 2.49236\n",
      "training using chunk : 581 validation score : 0.2472 log-loss : 2.49600\n",
      "training using chunk : 582 validation score : 0.2594 log-loss : 2.47812\n",
      "training using chunk : 583 validation score : 0.2291 log-loss : 2.50775\n",
      "training using chunk : 584 validation score : 0.2402 log-loss : 2.49509\n",
      "training using chunk : 585 validation score : 0.2496 log-loss : 2.49663\n",
      "training using chunk : 586 validation score : 0.2497 log-loss : 2.49371\n",
      "training using chunk : 587 validation score : 0.2536 log-loss : 2.47521\n",
      "training using chunk : 588 validation score : 0.2563 log-loss : 2.46689\n",
      "training using chunk : 589 validation score : 0.2587 log-loss : 2.47469\n",
      "training using chunk : 590 validation score : 0.2534 log-loss : 2.47778\n",
      "training using chunk : 591 validation score : 0.2543 log-loss : 2.47672\n",
      "training using chunk : 592 validation score : 0.2569 log-loss : 2.47697\n",
      "training using chunk : 593 validation score : 0.2501 log-loss : 2.48213\n",
      "training using chunk : 594 validation score : 0.2545 log-loss : 2.47392\n",
      "training using chunk : 595 validation score : 0.2587 log-loss : 2.47836\n",
      "training using chunk : 596 validation score : 0.2457 log-loss : 2.48544\n",
      "training using chunk : 597 validation score : 0.2515 log-loss : 2.47908\n",
      "training using chunk : 598 validation score : 0.2404 log-loss : 2.49450\n",
      "training using chunk : 599 validation score : 0.2542 log-loss : 2.48241\n",
      "training using chunk : 600 validation score : 0.2543 log-loss : 2.48529\n",
      "training using chunk : 601 validation score : 0.2426 log-loss : 2.51066\n",
      "training using chunk : 602 validation score : 0.2557 log-loss : 2.48408\n",
      "training using chunk : 603 validation score : 0.2555 log-loss : 2.47838\n",
      "training using chunk : 604 validation score : 0.2572 log-loss : 2.47415\n",
      "training using chunk : 605 validation score : 0.2551 log-loss : 2.47527\n",
      "training using chunk : 606 validation score : 0.2529 log-loss : 2.48459\n",
      "training using chunk : 607 validation score : 0.2566 log-loss : 2.49391\n",
      "training using chunk : 608 validation score : 0.2510 log-loss : 2.48854\n",
      "training using chunk : 609 validation score : 0.2566 log-loss : 2.47626\n",
      "training using chunk : 610 validation score : 0.2505 log-loss : 2.48096\n",
      "training using chunk : 611 validation score : 0.2560 log-loss : 2.48940\n",
      "training using chunk : 612 validation score : 0.2556 log-loss : 2.47458\n",
      "training using chunk : 613 validation score : 0.2503 log-loss : 2.49162\n",
      "training using chunk : 614 validation score : 0.2528 log-loss : 2.48986\n",
      "training using chunk : 615 validation score : 0.2576 log-loss : 2.48833\n",
      "training using chunk : 616 validation score : 0.2564 log-loss : 2.48829\n",
      "training using chunk : 617 validation score : 0.2420 log-loss : 2.49395\n",
      "training using chunk : 618 validation score : 0.2561 log-loss : 2.48349\n",
      "training using chunk : 619 validation score : 0.2560 log-loss : 2.48089\n",
      "training using chunk : 620 validation score : 0.2586 log-loss : 2.49866\n",
      "training using chunk : 621 validation score : 0.2562 log-loss : 2.47969\n",
      "training using chunk : 622 validation score : 0.2553 log-loss : 2.46902\n",
      "training using chunk : 623 validation score : 0.2566 log-loss : 2.48133\n",
      "training using chunk : 624 validation score : 0.2483 log-loss : 2.48082\n",
      "training using chunk : 625 validation score : 0.2490 log-loss : 2.48360\n",
      "training using chunk : 626 validation score : 0.2576 log-loss : 2.48056\n",
      "training using chunk : 627 validation score : 0.2396 log-loss : 2.48822\n",
      "training using chunk : 628 validation score : 0.2465 log-loss : 2.49182\n",
      "training using chunk : 629 validation score : 0.2591 log-loss : 2.47734\n",
      "training using chunk : 630 validation score : 0.2504 log-loss : 2.49973\n",
      "training using chunk : 631 validation score : 0.2611 log-loss : 2.47764\n",
      "training using chunk : 632 validation score : 0.2573 log-loss : 2.47178\n",
      "training using chunk : 633 validation score : 0.2580 log-loss : 2.46836\n",
      "training using chunk : 634 validation score : 0.2510 log-loss : 2.48571\n",
      "training using chunk : 635 validation score : 0.2553 log-loss : 2.48012\n",
      "training using chunk : 636 validation score : 0.2547 log-loss : 2.47844\n",
      "training using chunk : 637 validation score : 0.2525 log-loss : 2.48535\n",
      "training using chunk : 638 validation score : 0.2528 log-loss : 2.47589\n",
      "training using chunk : 639 validation score : 0.2602 log-loss : 2.47234\n",
      "training using chunk : 640 validation score : 0.2527 log-loss : 2.48423\n",
      "training using chunk : 641 validation score : 0.2546 log-loss : 2.50250\n",
      "training using chunk : 642 validation score : 0.2399 log-loss : 2.48858\n",
      "training using chunk : 643 validation score : 0.2560 log-loss : 2.47856\n",
      "training using chunk : 644 validation score : 0.2465 log-loss : 2.50253\n",
      "training using chunk : 645 validation score : 0.2580 log-loss : 2.47833\n",
      "training using chunk : 646 validation score : 0.2564 log-loss : 2.47486\n",
      "training using chunk : 647 validation score : 0.2524 log-loss : 2.46867\n",
      "training using chunk : 648 validation score : 0.2503 log-loss : 2.47290\n",
      "training using chunk : 649 validation score : 0.2570 log-loss : 2.48024\n",
      "training using chunk : 650 validation score : 0.2565 log-loss : 2.47657\n",
      "training using chunk : 651 validation score : 0.2537 log-loss : 2.47470\n",
      "training using chunk : 652 validation score : 0.2594 log-loss : 2.47052\n",
      "training using chunk : 653 validation score : 0.2513 log-loss : 2.47536\n",
      "training using chunk : 654 validation score : 0.2533 log-loss : 2.47558\n",
      "training using chunk : 655 validation score : 0.2498 log-loss : 2.47505\n",
      "training using chunk : 656 validation score : 0.2548 log-loss : 2.48122\n",
      "training using chunk : 657 validation score : 0.2492 log-loss : 2.49396\n",
      "training using chunk : 658 validation score : 0.2545 log-loss : 2.48337\n",
      "training using chunk : 659 validation score : 0.2576 log-loss : 2.48442\n",
      "training using chunk : 660 validation score : 0.2583 log-loss : 2.48105\n",
      "training using chunk : 661 validation score : 0.2564 log-loss : 2.48286\n",
      "training using chunk : 662 validation score : 0.2548 log-loss : 2.49531\n",
      "training using chunk : 663 validation score : 0.2546 log-loss : 2.48533\n",
      "training using chunk : 664 validation score : 0.2537 log-loss : 2.49990\n",
      "training using chunk : 665 validation score : 0.2541 log-loss : 2.49531\n",
      "training using chunk : 666 validation score : 0.2545 log-loss : 2.47757\n",
      "training using chunk : 667 validation score : 0.2571 log-loss : 2.48291\n",
      "training using chunk : 668 validation score : 0.2620 log-loss : 2.46919\n",
      "training using chunk : 669 validation score : 0.2464 log-loss : 2.48714\n",
      "training using chunk : 670 validation score : 0.2561 log-loss : 2.47777\n",
      "training using chunk : 671 validation score : 0.2555 log-loss : 2.48064\n",
      "training using chunk : 672 validation score : 0.2563 log-loss : 2.46959\n",
      "training using chunk : 673 validation score : 0.2575 log-loss : 2.48665\n",
      "training using chunk : 674 validation score : 0.2538 log-loss : 2.47963\n",
      "training using chunk : 675 validation score : 0.2585 log-loss : 2.48396\n",
      "training using chunk : 676 validation score : 0.2558 log-loss : 2.48216\n",
      "training using chunk : 677 validation score : 0.2548 log-loss : 2.47790\n",
      "training using chunk : 678 validation score : 0.2590 log-loss : 2.47870\n",
      "training using chunk : 679 validation score : 0.2572 log-loss : 2.47143\n",
      "training using chunk : 680 validation score : 0.2602 log-loss : 2.47245\n",
      "training using chunk : 681 validation score : 0.2564 log-loss : 2.47294\n",
      "training using chunk : 682 validation score : 0.2582 log-loss : 2.48606\n",
      "training using chunk : 683 validation score : 0.2600 log-loss : 2.47745\n",
      "training using chunk : 684 validation score : 0.2524 log-loss : 2.47284\n",
      "training using chunk : 685 validation score : 0.2580 log-loss : 2.48711\n",
      "training using chunk : 686 validation score : 0.2516 log-loss : 2.47309\n",
      "training using chunk : 687 validation score : 0.2608 log-loss : 2.46857\n",
      "training using chunk : 688 validation score : 0.2586 log-loss : 2.47776\n",
      "training using chunk : 689 validation score : 0.2559 log-loss : 2.48652\n",
      "training using chunk : 690 validation score : 0.2586 log-loss : 2.47720\n",
      "training using chunk : 691 validation score : 0.2549 log-loss : 2.48652\n",
      "training using chunk : 692 validation score : 0.2593 log-loss : 2.47529\n",
      "training using chunk : 693 validation score : 0.2493 log-loss : 2.47832\n",
      "training using chunk : 694 validation score : 0.2511 log-loss : 2.47730\n",
      "training using chunk : 695 validation score : 0.2487 log-loss : 2.47806\n",
      "training using chunk : 696 validation score : 0.2617 log-loss : 2.47356\n",
      "training using chunk : 697 validation score : 0.2579 log-loss : 2.49171\n",
      "training using chunk : 698 validation score : 0.2521 log-loss : 2.48363\n",
      "training using chunk : 699 validation score : 0.2567 log-loss : 2.48368\n",
      "training using chunk : 700 validation score : 0.2562 log-loss : 2.46615\n",
      "training using chunk : 701 validation score : 0.2595 log-loss : 2.46927\n",
      "training using chunk : 702 validation score : 0.2609 log-loss : 2.47305\n",
      "training using chunk : 703 validation score : 0.2601 log-loss : 2.47345\n",
      "training using chunk : 704 validation score : 0.2581 log-loss : 2.48280\n",
      "training using chunk : 705 validation score : 0.2585 log-loss : 2.48038\n",
      "training using chunk : 706 validation score : 0.2621 log-loss : 2.46512\n",
      "training using chunk : 707 validation score : 0.2423 log-loss : 2.48805\n",
      "training using chunk : 708 validation score : 0.2591 log-loss : 2.47113\n",
      "training using chunk : 709 validation score : 0.2490 log-loss : 2.47821\n",
      "training using chunk : 710 validation score : 0.2601 log-loss : 2.47502\n",
      "training using chunk : 711 validation score : 0.2473 log-loss : 2.48188\n",
      "training using chunk : 712 validation score : 0.2416 log-loss : 2.48564\n",
      "training using chunk : 713 validation score : 0.2575 log-loss : 2.47305\n",
      "training using chunk : 714 validation score : 0.2554 log-loss : 2.48065\n",
      "training using chunk : 715 validation score : 0.2525 log-loss : 2.47516\n",
      "training using chunk : 716 validation score : 0.2589 log-loss : 2.46881\n",
      "training using chunk : 717 validation score : 0.2590 log-loss : 2.47096\n",
      "training using chunk : 718 validation score : 0.2578 log-loss : 2.46553\n",
      "training using chunk : 719 validation score : 0.2556 log-loss : 2.47717\n",
      "training using chunk : 720 validation score : 0.2570 log-loss : 2.46636\n",
      "training using chunk : 721 validation score : 0.2576 log-loss : 2.47917\n",
      "training using chunk : 722 validation score : 0.2567 log-loss : 2.46921\n",
      "training using chunk : 723 validation score : 0.2586 log-loss : 2.46852\n",
      "training using chunk : 724 validation score : 0.2616 log-loss : 2.46602\n",
      "training using chunk : 725 validation score : 0.2620 log-loss : 2.46710\n",
      "training using chunk : 726 validation score : 0.2557 log-loss : 2.47236\n",
      "training using chunk : 727 validation score : 0.2529 log-loss : 2.46626\n",
      "training using chunk : 728 validation score : 0.2588 log-loss : 2.46364\n",
      "training using chunk : 729 validation score : 0.2511 log-loss : 2.47388\n",
      "training using chunk : 730 validation score : 0.2529 log-loss : 2.47338\n",
      "training using chunk : 731 validation score : 0.2560 log-loss : 2.47179\n",
      "training using chunk : 732 validation score : 0.2567 log-loss : 2.46468\n",
      "training using chunk : 733 validation score : 0.2540 log-loss : 2.46568\n",
      "training using chunk : 734 validation score : 0.2572 log-loss : 2.46856\n",
      "training using chunk : 735 validation score : 0.2576 log-loss : 2.47637\n",
      "training using chunk : 736 validation score : 0.2578 log-loss : 2.46397\n",
      "training using chunk : 737 validation score : 0.2492 log-loss : 2.47325\n",
      "training using chunk : 738 validation score : 0.2533 log-loss : 2.48091\n",
      "training using chunk : 739 validation score : 0.2597 log-loss : 2.47257\n",
      "training using chunk : 740 validation score : 0.2594 log-loss : 2.46648\n",
      "training using chunk : 741 validation score : 0.2600 log-loss : 2.46379\n",
      "training using chunk : 742 validation score : 0.2545 log-loss : 2.47624\n",
      "training using chunk : 743 validation score : 0.2543 log-loss : 2.50245\n",
      "training using chunk : 744 validation score : 0.2592 log-loss : 2.47391\n",
      "training using chunk : 745 validation score : 0.2524 log-loss : 2.47472\n",
      "training using chunk : 746 validation score : 0.2556 log-loss : 2.47586\n",
      "training using chunk : 747 validation score : 0.2590 log-loss : 2.46751\n",
      "training using chunk : 748 validation score : 0.2588 log-loss : 2.46854\n",
      "training using chunk : 749 validation score : 0.2488 log-loss : 2.47700\n",
      "training using chunk : 750 validation score : 0.2561 log-loss : 2.47394\n",
      "training using chunk : 751 validation score : 0.2577 log-loss : 2.47235\n",
      "training using chunk : 752 validation score : 0.2589 log-loss : 2.47887\n",
      "training using chunk : 753 validation score : 0.2586 log-loss : 2.46402\n",
      "training using chunk : 754 validation score : 0.2601 log-loss : 2.46622\n",
      "training using chunk : 755 validation score : 0.2504 log-loss : 2.47288\n",
      "training using chunk : 756 validation score : 0.2601 log-loss : 2.47523\n",
      "training using chunk : 757 validation score : 0.2544 log-loss : 2.47784\n",
      "training using chunk : 758 validation score : 0.2504 log-loss : 2.48144\n",
      "training using chunk : 759 validation score : 0.2553 log-loss : 2.47299\n",
      "training using chunk : 760 validation score : 0.2529 log-loss : 2.48787\n",
      "training using chunk : 761 validation score : 0.2516 log-loss : 2.47019\n",
      "training using chunk : 762 validation score : 0.2571 log-loss : 2.47782\n",
      "training using chunk : 763 validation score : 0.2502 log-loss : 2.47373\n",
      "training using chunk : 764 validation score : 0.2518 log-loss : 2.47253\n",
      "training using chunk : 765 validation score : 0.2478 log-loss : 2.47608\n",
      "training using chunk : 766 validation score : 0.2484 log-loss : 2.47472\n",
      "training using chunk : 767 validation score : 0.2609 log-loss : 2.46011\n",
      "training using chunk : 768 validation score : 0.2615 log-loss : 2.46225\n",
      "training using chunk : 769 validation score : 0.2494 log-loss : 2.47925\n",
      "training using chunk : 770 validation score : 0.2595 log-loss : 2.47873\n",
      "training using chunk : 771 validation score : 0.2514 log-loss : 2.47045\n",
      "training using chunk : 772 validation score : 0.2416 log-loss : 2.48258\n",
      "training using chunk : 773 validation score : 0.2551 log-loss : 2.46768\n",
      "training using chunk : 774 validation score : 0.2567 log-loss : 2.47126\n",
      "training using chunk : 775 validation score : 0.2597 log-loss : 2.46147\n",
      "training using chunk : 776 validation score : 0.2621 log-loss : 2.46614\n",
      "training using chunk : 777 validation score : 0.2561 log-loss : 2.47393\n",
      "training using chunk : 778 validation score : 0.2600 log-loss : 2.46794\n",
      "training using chunk : 779 validation score : 0.2558 log-loss : 2.46506\n",
      "training using chunk : 780 validation score : 0.2536 log-loss : 2.47124\n",
      "training using chunk : 781 validation score : 0.2610 log-loss : 2.46438\n",
      "training using chunk : 782 validation score : 0.2606 log-loss : 2.46360\n",
      "training using chunk : 783 validation score : 0.2635 log-loss : 2.46066\n",
      "training using chunk : 784 validation score : 0.2584 log-loss : 2.45690\n",
      "training using chunk : 785 validation score : 0.2520 log-loss : 2.46936\n",
      "training using chunk : 786 validation score : 0.2605 log-loss : 2.46473\n",
      "training using chunk : 787 validation score : 0.2576 log-loss : 2.46693\n",
      "training using chunk : 788 validation score : 0.2588 log-loss : 2.47386\n",
      "training using chunk : 789 validation score : 0.2630 log-loss : 2.46213\n",
      "training using chunk : 790 validation score : 0.2544 log-loss : 2.47113\n",
      "training using chunk : 791 validation score : 0.2589 log-loss : 2.47114\n",
      "training using chunk : 792 validation score : 0.2593 log-loss : 2.46753\n",
      "training using chunk : 793 validation score : 0.2596 log-loss : 2.47434\n",
      "training using chunk : 794 validation score : 0.2622 log-loss : 2.45716\n",
      "training using chunk : 795 validation score : 0.2624 log-loss : 2.47082\n",
      "training using chunk : 796 validation score : 0.2588 log-loss : 2.48233\n",
      "training using chunk : 797 validation score : 0.2563 log-loss : 2.46659\n",
      "training using chunk : 798 validation score : 0.2563 log-loss : 2.46686\n",
      "training using chunk : 799 validation score : 0.2566 log-loss : 2.46471\n",
      "training using chunk : 800 validation score : 0.2567 log-loss : 2.47742\n",
      "training using chunk : 801 validation score : 0.2488 log-loss : 2.47575\n",
      "training using chunk : 802 validation score : 0.2554 log-loss : 2.46653\n",
      "training using chunk : 803 validation score : 0.2561 log-loss : 2.47138\n",
      "training using chunk : 804 validation score : 0.2571 log-loss : 2.46444\n",
      "training using chunk : 805 validation score : 0.2630 log-loss : 2.46266\n",
      "training using chunk : 806 validation score : 0.2595 log-loss : 2.46043\n",
      "training using chunk : 807 validation score : 0.2560 log-loss : 2.47046\n",
      "training using chunk : 808 validation score : 0.2605 log-loss : 2.46852\n",
      "training using chunk : 809 validation score : 0.2604 log-loss : 2.46267\n",
      "training using chunk : 810 validation score : 0.2578 log-loss : 2.47161\n",
      "training using chunk : 811 validation score : 0.2579 log-loss : 2.47307\n",
      "training using chunk : 812 validation score : 0.2567 log-loss : 2.47895\n",
      "training using chunk : 813 validation score : 0.2610 log-loss : 2.46084\n",
      "training using chunk : 814 validation score : 0.2615 log-loss : 2.46100\n",
      "training using chunk : 815 validation score : 0.2548 log-loss : 2.47013\n",
      "training using chunk : 816 validation score : 0.2539 log-loss : 2.46860\n",
      "training using chunk : 817 validation score : 0.2604 log-loss : 2.47081\n",
      "training using chunk : 818 validation score : 0.2509 log-loss : 2.47217\n",
      "training using chunk : 819 validation score : 0.2604 log-loss : 2.45555\n",
      "training using chunk : 820 validation score : 0.2638 log-loss : 2.45875\n",
      "training using chunk : 821 validation score : 0.2603 log-loss : 2.46109\n",
      "training using chunk : 822 validation score : 0.2641 log-loss : 2.45748\n",
      "training using chunk : 823 validation score : 0.2574 log-loss : 2.47559\n",
      "training using chunk : 824 validation score : 0.2583 log-loss : 2.46864\n",
      "training using chunk : 825 validation score : 0.2585 log-loss : 2.46080\n",
      "training using chunk : 826 validation score : 0.2619 log-loss : 2.46462\n",
      "training using chunk : 827 validation score : 0.2603 log-loss : 2.46642\n",
      "training using chunk : 828 validation score : 0.2618 log-loss : 2.46074\n",
      "training using chunk : 829 validation score : 0.2598 log-loss : 2.46091\n",
      "training using chunk : 830 validation score : 0.2575 log-loss : 2.47022\n",
      "training using chunk : 831 validation score : 0.2571 log-loss : 2.47584\n",
      "training using chunk : 832 validation score : 0.2566 log-loss : 2.46666\n",
      "training using chunk : 833 validation score : 0.2618 log-loss : 2.45638\n",
      "training using chunk : 834 validation score : 0.2588 log-loss : 2.46139\n",
      "training using chunk : 835 validation score : 0.2454 log-loss : 2.47234\n",
      "training using chunk : 836 validation score : 0.2614 log-loss : 2.46561\n",
      "training using chunk : 837 validation score : 0.2601 log-loss : 2.46024\n",
      "training using chunk : 838 validation score : 0.2595 log-loss : 2.46915\n",
      "training using chunk : 839 validation score : 0.2598 log-loss : 2.45722\n",
      "training using chunk : 840 validation score : 0.2570 log-loss : 2.46113\n",
      "training using chunk : 841 validation score : 0.2608 log-loss : 2.45950\n",
      "training using chunk : 842 validation score : 0.2489 log-loss : 2.47955\n",
      "training using chunk : 843 validation score : 0.2491 log-loss : 2.47998\n",
      "training using chunk : 844 validation score : 0.2605 log-loss : 2.46265\n",
      "training using chunk : 845 validation score : 0.2559 log-loss : 2.46627\n",
      "training using chunk : 846 validation score : 0.2592 log-loss : 2.46053\n",
      "training using chunk : 847 validation score : 0.2629 log-loss : 2.45897\n",
      "training using chunk : 848 validation score : 0.2532 log-loss : 2.46964\n",
      "training using chunk : 849 validation score : 0.2588 log-loss : 2.47286\n",
      "training using chunk : 850 validation score : 0.2610 log-loss : 2.45902\n",
      "training using chunk : 851 validation score : 0.2615 log-loss : 2.46834\n",
      "training using chunk : 852 validation score : 0.2556 log-loss : 2.46080\n",
      "training using chunk : 853 validation score : 0.2598 log-loss : 2.46583\n",
      "training using chunk : 854 validation score : 0.2533 log-loss : 2.46479\n",
      "training using chunk : 855 validation score : 0.2567 log-loss : 2.47139\n",
      "training using chunk : 856 validation score : 0.2518 log-loss : 2.47534\n",
      "training using chunk : 857 validation score : 0.2579 log-loss : 2.45930\n",
      "training using chunk : 858 validation score : 0.2575 log-loss : 2.46076\n",
      "training using chunk : 859 validation score : 0.2580 log-loss : 2.46646\n",
      "training using chunk : 860 validation score : 0.2527 log-loss : 2.47942\n",
      "training using chunk : 861 validation score : 0.2620 log-loss : 2.45578\n",
      "training using chunk : 862 validation score : 0.2604 log-loss : 2.45928\n",
      "training using chunk : 863 validation score : 0.2586 log-loss : 2.47342\n",
      "training using chunk : 864 validation score : 0.2516 log-loss : 2.46438\n",
      "training using chunk : 865 validation score : 0.2579 log-loss : 2.46137\n",
      "training using chunk : 866 validation score : 0.2521 log-loss : 2.46894\n",
      "training using chunk : 867 validation score : 0.2615 log-loss : 2.47012\n",
      "training using chunk : 868 validation score : 0.2586 log-loss : 2.46090\n",
      "training using chunk : 869 validation score : 0.2611 log-loss : 2.45202\n",
      "training using chunk : 870 validation score : 0.2560 log-loss : 2.46327\n",
      "training using chunk : 871 validation score : 0.2569 log-loss : 2.46696\n",
      "training using chunk : 872 validation score : 0.2536 log-loss : 2.47306\n",
      "training using chunk : 873 validation score : 0.2623 log-loss : 2.46891\n",
      "training using chunk : 874 validation score : 0.2602 log-loss : 2.46060\n",
      "training using chunk : 875 validation score : 0.2618 log-loss : 2.46605\n",
      "training using chunk : 876 validation score : 0.2611 log-loss : 2.45669\n",
      "training using chunk : 877 validation score : 0.2621 log-loss : 2.46257\n",
      "training using chunk : 878 validation score : 0.2620 log-loss : 2.46744\n",
      "training using chunk : 879 validation score : 0.2620 log-loss : 2.46197\n"
     ]
    }
   ],
   "source": [
    "# train on complete data\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "model = train_model(X,Y.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1535"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del X\n",
    "#del Y\n",
    "#del final_train_df\n",
    "#del dev_data\n",
    "#del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 2315L)\n"
     ]
    }
   ],
   "source": [
    "# preparate test data\n",
    "test_data = final_test_df[inputFeatures].values\n",
    "test_data[:,:len(to_be_scaled_features)] = scaler.transform(test_data[:,:len(to_be_scaled_features)])\n",
    "test_data = np.concatenate((test_data,  test_adddress_features.toarray()), axis=1)\n",
    "print test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n"
     ]
    }
   ],
   "source": [
    "#evaluate final probability\n",
    "submit_output = model.predict_proba(test_data)\n",
    "# round up the results\n",
    "submit_output = np.around(submit_output, decimals=10)\n",
    "print submit_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 40L)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 40 columns):\n",
      "Id                             884262 non-null int32\n",
      "ARSON                          884262 non-null float64\n",
      "ASSAULT                        884262 non-null float64\n",
      "BAD CHECKS                     884262 non-null float64\n",
      "BRIBERY                        884262 non-null float64\n",
      "BURGLARY                       884262 non-null float64\n",
      "DISORDERLY CONDUCT             884262 non-null float64\n",
      "DRIVING UNDER THE INFLUENCE    884262 non-null float64\n",
      "DRUG/NARCOTIC                  884262 non-null float64\n",
      "DRUNKENNESS                    884262 non-null float64\n",
      "EMBEZZLEMENT                   884262 non-null float64\n",
      "EXTORTION                      884262 non-null float64\n",
      "FAMILY OFFENSES                884262 non-null float64\n",
      "FORGERY/COUNTERFEITING         884262 non-null float64\n",
      "FRAUD                          884262 non-null float64\n",
      "GAMBLING                       884262 non-null float64\n",
      "KIDNAPPING                     884262 non-null float64\n",
      "LARCENY/THEFT                  884262 non-null float64\n",
      "LIQUOR LAWS                    884262 non-null float64\n",
      "LOITERING                      884262 non-null float64\n",
      "MISSING PERSON                 884262 non-null float64\n",
      "NON-CRIMINAL                   884262 non-null float64\n",
      "OTHER OFFENSES                 884262 non-null float64\n",
      "PORNOGRAPHY/OBSCENE MAT        884262 non-null float64\n",
      "PROSTITUTION                   884262 non-null float64\n",
      "RECOVERED VEHICLE              884262 non-null float64\n",
      "ROBBERY                        884262 non-null float64\n",
      "RUNAWAY                        884262 non-null float64\n",
      "SECONDARY CODES                884262 non-null float64\n",
      "SEX OFFENSES FORCIBLE          884262 non-null float64\n",
      "SEX OFFENSES NON FORCIBLE      884262 non-null float64\n",
      "STOLEN PROPERTY                884262 non-null float64\n",
      "SUICIDE                        884262 non-null float64\n",
      "SUSPICIOUS OCC                 884262 non-null float64\n",
      "TREA                           884262 non-null float64\n",
      "TRESPASS                       884262 non-null float64\n",
      "VANDALISM                      884262 non-null float64\n",
      "VEHICLE THEFT                  884262 non-null float64\n",
      "WARRANTS                       884262 non-null float64\n",
      "WEAPON LAWS                    884262 non-null float64\n",
      "dtypes: float64(39), int32(1)\n",
      "memory usage: 273.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = np.c_[final_test_df['Id'].astype(int), submit_output.astype(float)]\n",
    "print result.shape\n",
    "outputColumns =  ['Id'] + list( category_encoder.classes_)\n",
    "df_result = pd.DataFrame(result, columns=outputColumns)\n",
    "df_result['Id'] = df_result['Id'].astype(int)\n",
    "print df_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_result.to_csv('08-SFCrimeMIDSChallengerTeam.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
